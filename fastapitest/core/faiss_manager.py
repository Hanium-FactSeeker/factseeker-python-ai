import os
import json
import hashlib
import logging
import boto3
import faiss
import pickle
import time
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

CHUNK_CACHE_DIR = "article_faiss_cache"
S3_BUCKET_NAME = "factseeker-faiss-db"
S3_PREFIX = "article_faiss_cache/"
os.makedirs(CHUNK_CACHE_DIR, exist_ok=True)

s3 = boto3.client("s3")

def sha256_of(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def download_from_s3_if_exists(s3_key: str, local_path: str) -> bool:
    try:
        s3.download_file(S3_BUCKET_NAME, s3_key, local_path)
        logging.info(f"ğŸ”½ S3ì—ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {s3_key}")
        return True
    except Exception as e:
        logging.warning(f"âš ï¸ S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {s3_key} â†’ {e}")
        return False

def upload_to_s3(local_path: str, s3_key: str):
    try:
        s3.upload_file(local_path, S3_BUCKET_NAME, s3_key)
        logging.info(f"ğŸ†™ S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_key}")
    except Exception as e:
        logging.error(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key} â†’ {e}")

def get_or_build_faiss(url: str, article_text: str, embed_model) -> FAISS:
    hashed = sha256_of(url)
    folder_path = os.path.join(CHUNK_CACHE_DIR, hashed)
    os.makedirs(folder_path, exist_ok=True)
    
    faiss_path = os.path.join(folder_path, "index.faiss")
    pkl_path = os.path.join(folder_path, "index.pkl")

    s3_faiss_key = f"{S3_PREFIX}{hashed}/index.faiss"
    s3_pkl_key = f"{S3_PREFIX}{hashed}/index.pkl"

    # âœ… ë¡œì»¬ì— ì—†ìœ¼ë©´ S3ì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„
    if not os.path.exists(faiss_path) or not os.path.exists(pkl_path):
        logging.info("ğŸ“¦ ë¡œì»¬ ìºì‹œ ì—†ìŒ â†’ S3ì—ì„œ ë¡œë”© ì‹œë„")
        start = time.time()
        download_from_s3_if_exists(s3_faiss_key, faiss_path)
        download_from_s3_if_exists(s3_pkl_key, pkl_path)
        elapsed = time.time() - start
        logging.info(f"â±ï¸ [S3 ë‹¤ìš´ë¡œë“œ] ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")

    # âœ… ë‹¤ìš´ë¡œë“œë˜ì—ˆê±°ë‚˜ ì›ë˜ë¶€í„° ë¡œì»¬ì— ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(faiss_path) and os.path.exists(pkl_path):
        logging.info("âœ… FAISS ìºì‹œ ë¡œë“œ ì™„ë£Œ")
        with open(pkl_path, "rb") as f:
            stored_texts = pickle.load(f)
        return FAISS.load_local(folder_path, embed_model, stored_texts)

    # âŒ ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    logging.info("âš™ï¸ FAISS ì¸ë±ìŠ¤ ìƒˆë¡œ ìƒì„± ì¤‘...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
    chunks = splitter.split_text(article_text)
    docs = [Document(page_content=chunk, metadata={"url": url}) for chunk in chunks]
    db = FAISS.from_documents(docs, embed_model)
    db.save_local(folder_path)
    with open(pkl_path, "wb") as f:
        pickle.dump(docs, f)

    # ì—…ë¡œë“œ
    upload_to_s3(faiss_path, s3_faiss_key)
    upload_to_s3(pkl_path, s3_pkl_key)

    return db
