import os
import hashlib
import logging
import boto3
import pickle
import time
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# lambdasì—ì„œ get_article_text í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì˜ì¡´ì„± ì£¼ì… ë“±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
from core.lambdas import get_article_text

CHUNK_CACHE_DIR = "article_faiss_cache"
S3_BUCKET_NAME = "factseeker-faiss-db"
S3_PREFIX = "article_faiss_cache/"
os.makedirs(CHUNK_CACHE_DIR, exist_ok=True)

s3 = boto3.client("s3")

def sha256_of(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def download_from_s3_if_exists(s3_key, local_path):
    """S3ì—ì„œ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        s3.head_object(Bucket=S3_BUCKET_NAME, Key=s3_key)
        logging.info(f"ğŸ“¦ S3ì—ì„œ ìºì‹œ íŒŒì¼ í™•ì¸: {s3_key}")
        s3.download_file(S3_BUCKET_NAME, s3_key, local_path)
        if os.path.exists(local_path):
            logging.info(f"âœ… S3 ìºì‹œ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {local_path} ({os.path.getsize(local_path)} bytes)")
            return True
    except Exception:
        logging.info(f"ğŸ’¨ S3ì— ìºì‹œ íŒŒì¼ ì—†ìŒ: {s3_key}")
        return False
    return False

def upload_to_s3(local_path: str, s3_key: str):
    """íŒŒì¼ì„ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        s3.upload_file(local_path, S3_BUCKET_NAME, s3_key)
        logging.info(f"ğŸ†™ S3 ì—…ë¡œë“œ ì™„ë£Œ: {s3_key}")
    except Exception as e:
        logging.error(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key} -> {e}")

async def get_documents_with_caching(url: str, embed_model) -> list[Document]:
    """
    URLì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ìºì‹œ(ë¡œì»¬/S3)ë¥¼ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ë¬¸ì„œë¥¼ í¬ë¡¤ë§, ì²˜ë¦¬ í›„ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    hashed = sha256_of(url)
    folder_path = os.path.join(CHUNK_CACHE_DIR, hashed)
    os.makedirs(folder_path, exist_ok=True)

    faiss_path = os.path.join(folder_path, "index.faiss")
    pkl_path = os.path.join(folder_path, "index.pkl")

    # 1. ë¡œì»¬ ìºì‹œ í™•ì¸
    if os.path.exists(faiss_path) and os.path.exists(pkl_path):
        logging.info(f"âœ… ë¡œì»¬ ìºì‹œì—ì„œ ë¬¸ì„œ ë¡œë“œ: {url}")
        with open(pkl_path, "rb") as f:
            return pickle.load(f)

    # 2. S3 ìºì‹œ í™•ì¸
    s3_faiss_key = f"{S3_PREFIX}{hashed}/index.faiss"
    s3_pkl_key = f"{S3_PREFIX}{hashed}/index.pkl"

    if download_from_s3_if_exists(s3_pkl_key, pkl_path) and download_from_s3_if_exists(s3_faiss_key, faiss_path):
        logging.info(f"âœ… S3 ìºì‹œì—ì„œ ë¬¸ì„œ ë¡œë“œ: {url}")
        with open(pkl_path, "rb") as f:
            return pickle.load(f)

    # 3. ìºì‹œê°€ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ë° ì‹ ê·œ ìƒì„±
    logging.info(f"âš™ï¸ ìºì‹œ ì—†ìŒ. ê¸°ì‚¬ í¬ë¡¤ë§ ë° FAISS ì¸ë±ìŠ¤ ì‹ ê·œ ìƒì„± ì‹œì‘: {url}")
    
    article_text = await get_article_text(url)
    if not article_text or len(article_text) < 200:
        logging.warning(f"í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤: {url}")
        return []

    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Document ìƒì„±
    splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
    chunks = splitter.split_text(article_text)
    docs = [Document(page_content=chunk, metadata={"url": url, "original_title": ""}) for chunk in chunks]
    
    if not docs:
        logging.warning(f"ë¬¸ì„œ ì²­í¬ ìƒì„± ì‹¤íŒ¨: {url}")
        return []

    # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
    try:
        db = FAISS.from_documents(docs, embed_model)
        db.save_local(folder_path)
        with open(pkl_path, "wb") as f:
            pickle.dump(docs, f)
        logging.info(f"ğŸ’¾ ë¡œì»¬ì— ìºì‹œ ì €ì¥ ì™„ë£Œ: {folder_path}")

        # S3ì— ì—…ë¡œë“œ
        upload_to_s3(faiss_path, s3_faiss_key)
        upload_to_s3(pkl_path, s3_pkl_key)

        return docs
    except Exception as e:
        logging.error(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì €ì¥ ì‹¤íŒ¨: {url}, ì—ëŸ¬: {e}")
        return []