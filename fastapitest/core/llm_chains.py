import os
import re
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate


def get_chat_llm():
    """LangChain용 ChatOpenAI 모델을 초기화합니다."""
    # OpenAI API 키는 .env에서 로드됩니다.
    return ChatOpenAI(model="gpt-4o", temperature=0, request_timeout=60, max_retries=5)  

def build_claim_extractor():
    prompt = PromptTemplate.from_template("""
[역할]
당신은 정치 유튜브 영상 또는 발화문에서 추출된 문장이 **팩트체크 가능한 주장인지**를 판단하고, 
검증 가능한 경우 **사실 검증이 가능한 정제 문장으로 재구성**하는 전문가입니다.

[판단 목적]
- 문장이 참인지 거짓인지를 따지는 것이 아니라, **객관적 사실 여부를 검증할 수 있는 주장인지**를 선별하는 것이 목표입니다.
- 팩트체크 가능한 경우에는, **정확하고 간결한 문장 형태로 정제하여 출력**해야 합니다.

[판단 기준]

**팩트체크 가능 (검증 가능한 구체적 사실이 포함된 경우)**:
- 수치, 통계, 금액, 연도 등 **객관적 데이터**가 명시된 경우
- 인물의 **발언, 행동, 결정, 경력 등**이 드러난 경우
- **정책, 제도, 조직 구조, 법안 등**의 설명이 포함된 경우
- **보도·공식문서로 확인 가능한 의혹 제기** 등

**팩트체크 불가능 (모호하거나 주관적 판단 중심)**:
- 감정, 해석, 비난, 주관적 평가 중심인 경우
- 은유, 과장, 추측, 음모론 등으로 **사실 판단 기준이 불명확**한 경우
- 당위적 주장, 가치 판단, 예측 중심의 문장 (예: ~해야 한다, ~일 것이다)
- 주체, 행위, 시점 등이 불분명하거나 불특정한 경우
- **정치적 맥락 부적합**: 위에 명시된 '정치적 맥락 적합성' 기준에 부합하지 않아, 정치적 팩트체크의 의미가 없는 주장 (예시: '윤석열 대통령의 생일은 1960년 12월 18일이다.', '김건희 여사가 이번 주말에 미술 전시회에 방문했다.')
- !!!중요!!!어떤 제품이나 서비스에 대한 **광고, 홍보, 마케팅 문구** (예: '이 제품은 최고의 품질을 자랑합니다.', '이 서비스는 고객 만족을 최우선으로 합니다.', '이 제품은 1+1입니다.', '이 제품은 ~ 원단을 사용합니다.')는 팩트체크 불가능으로 간주합니다.

[정제 지시]

※ 아래와 같은 경우엔 **착각 또는 인식 오류** 문장으로 정제:
- 발언자가 수치, 정책, 제도 등에 대해 명백한 오해나 오류를 보이는 경우:
    → “김민석은 국가채무 비율을 20~30%로 착각하고 있다.”
    → “김민석은 대한민국 예산에 대해 잘못된 인식을 드러냈다.”

※ 반드시 **발언자명을 포함**한 단일 주장 문장으로 정제할 것
※ **자막에 등장하지 않은 정보나 생성된 문장**은 포함하지 마세요. 자막에 명시된 범위 내에서만 판단해야 합니다.

※ '지시했다', '요청했다', '비판했다' 등 **행위성 발언**은 반드시 **그 대상(무엇을 지시했는지, 비판했는지 등)을 포함**하도록 문장을 보완해 주세요.  
※ 대상이 모호할 경우, 자막 맥락을 분석하여 **가능한 한 구체적인 표현으로 보완**해 주세요.  

# ──────────────────────────────────────────────
# !!!새로 추가되거나 수정된 내용!!!
# ──────────────────────────────────────────────
# 발언자가 명시되지 않은 채 모호하게 '사실이 드러났다' 등으로 표현된 경우:
# - 원본 자막에 구체적인 주어가 없는 경우라도, 해당 주장의 핵심 내용이 '어떤 인물' 또는 '어떤 주체'의 '인지 오류/특정 행위'에 대한 것이라면,
#   팩트체크를 위해 필요시 '특정 인물은' 또는 '발언자는' 등의 일반화된 주어를 사용하여 정제된 문장을 구성해야 합니다.
#   이 경우, '자막에 없는 정보를 생성하지 말라'는 규칙보다 '팩트체크 가능한 주장으로 정제하라'는 규칙이 우선합니다.
#   예: "대한민국 국가 경제의 기본 수치조차 제대로 인지하지 못하고 있다는 사실이 드러난 겁니다."
#       → (정제) "발언자는 대한민국 국가 경제의 기본 수치를 제대로 인지하지 못하고 있다."
#   (이 경우, 발언자가 누구인지 자막에서 명확히 식별 가능하다면 그 이름을 직접 사용합니다.)

[추가 규칙]

1. !!!!!!하나의 문장에 **여러 주장이 있다면**, 각각 **단일 문장**으로 나누어 개별 판단!!!!!!
    -ex) -> "트럼프 대통령은 브라질의 룰라 정부가 보소나루 전 대통령에 대한 정치적 탄압을 벌이고 있다고 주장하며 관세를 10%에서 50%까지 올리는 조치를 단행했다."
    => "트럼프 대통령은 브라징의 룰라 정부가 보소나루 전 대통령에 대한 정치적 탄압을 벌이고 있다고 주장했다.", "트럼프 대통령은 브라질에 관세를 10%에서 50%까지 올리는 조치를 단행했다."
2. “의혹을 받고 있다”는 표현은 **실제 보도 근거가 있는 경우에 한해** 구체화하여 판단
3. 발언자가 명시되지 않은 경우에는 **“(발언자 미상)”**으로 표기
4. **지나치게 포괄적/모호한 표현**은 ‘부분 가능’ 또는 ‘불가능’으로 분류
5. **모든 출력 문장은 하나의 주제/주장만 포함**해야 함

[주어 오판 방지 규칙]

- 특정 인물(예: 김상욱)의 이름이 언급되었더라도, 해당 문장에서 **그 인물이 실제로 발언한 내용이 명확하지 않다면 주어로 지정하지 마세요.**
- 유튜버가 다른 인물의 말을 인용하거나 언급하는 경우라도, **해당 인물이 정확히 어떤 발언을 했는지가 자막에 명시되지 않으면 주장으로 구성하지 마세요.**
- 자막에 “김상욱도 똑같이 얘기했어요”, “김상욱이 그렇게 말했다고 합니다” 등 **간접 언급만 존재할 경우**,
그 문장은 유튜버의 주장일 뿐이며, **김상욱을 주어로 한 fact claim을 생성하지 마세요.**

[출력 형식]

각 주장을 다음 3가지 유형 중 하나로 분류하고, 다음 llm chain이 확인할 수 있도록 해당 문장 옆에 간단히 사유를 명시:

- 팩트체크 가능
- (부분 가능): 사실 판단 가능한 핵심 포함, 일부 표현은 수사적
- 팩트체크 불가능 (사유 간단히 명시)

[예시 출력]

- [1] 김민석은 국가 채무 비율을 20~30%로 착각하고 있다. → (부분 가능): 발언 여부는 검증 가능하나 표현은 수사적임
- [2] 김민석은 국가 예산이 673조 원이라는 사실을 모르고 있다. → (부분 가능): 핵심은 사실이나 표현은 과장
- [3] 김민석은 출판 기념회 돈봉투 의혹이 제기된 인물이다. → 팩트체크 가능
- [4] 김민석은 부정부패 의혹 등 수많은 논란을 받고 있다. → (부분 가능): 표현은 포괄적이나 실제 의혹 존재
- [5] 김민석은 정치적으로 죽었다. → 팩트체크 불가능 (주관적 표현 + 검증 불가)
- [6] 김상욱은 민주당 내부 갈등이 있다고 말했다. →❌ 팩트체크 불가능 (해당 발언 자막에 없음, 유튜버 해석일 가능성 높음)
- **[7] 대한민국 국가 경제의 기본 수치조차 제대로 인지하지 못하고 있다는 사실이 드러난 겁니다. → 팩트체크 가능 (정제: 특정 인물은 대한민국 국가 경제의 기본 수치를 제대로 인지하지 못하고 있다.)**
- [8] 오늘 날짜가 2025년 4월 4일이다. → 팩트체크 불가능 (정치적 맥락 부적합)
- [9] 오늘 오전에 윤석열 대통령이 파면이 됐다. → 팩트체크 가능 (윤석열 대통령의 파면 여부)

입력:
{transcript}

출력:
""")
    return prompt | get_chat_llm()


def build_claim_summarizer():
    """정치적 주장을 뉴스 검색에 최적화된 핵심 키워드 구문으로 요약합니다."""
    prompt = PromptTemplate.from_template("""
다음 정치적 주장을 **뉴스 검색에 효과적인 핵심 키워드 구문**으로 요약하세요.

[규칙]
1.  **문장 형태는 절대 사용하지 마세요.** → 핵심 명사 중심의 구문으로 출력
2.  **주어(인물, 기관 등) + 핵심 주제(사건, 대상 등) 조합으로 구성**
3.  다음은 **절대 포함하지 마세요**:
    - 말하는 행위 (예: 발언, 주장, 언급, 밝혔다, 말했다)
    - **숫자, 비율, 퍼센트(%), 날짜, 연도 등 구체적인 수치는 절대 포함하지 마세요.** (예: 673조 원, 20%, 48.4%, 2022년)
    - **인물의 지식 상태나 행동 방식, 부정적 평가를 나타내는 단어 (예: 모른다, 착각, 회피, 얼버무림, 엉뚱한, 실언, 알고 있지 않다, 대응, 애매모호, 정확히 말하지 않음, 노력하지 않음, 관심 없음, 배신, 자폭, 이중대, 기회주의자, 아이콘, 발판 등 모든 부정적/주관적/행동 양식/평가 관련 단어).** 이러한 단어들이 주장에 포함되어 있더라도 검색어에서는 반드시 제거해야 합니다.
4.  **고유명사(인물, 기관, 지역 등)는 반드시 포함하세요.**
5.  **'회차', '시점'과 같이 시간의 흐름을 나타내는 일반적인 단어는 필요 시 포함하세요.**
6.  실제 기사 제목에서 자연스럽게 나올 법한 표현으로 구성하세요.

[출력 형식]
- 단 하나의 구문 (예: 김민석 의원 국가 채무 OECD 평균)

[예시]
입력: "김민석은 대한민국 국가 예산이 673조 원이라는 사실을 모른다."
출력: 김민석 대한민국 국가 예산

입력: "김민석은 국가 채무 비율을 20에서 30%로 착각하고 있다."
출력: 김민석 국가 채무 비율

입력: "김민석은 국가 채무 비율을 회피하려고 했다."
출력: 김민석 국가 채무 비율

입력: "김민석은 국가 채무 비율을 OECD 평균보다 높다고도 하고 낮다고도 한다는 식으로 얼버무렸다."
출력: 김민석 국가 채무 비율 OECD 평균

입력: "김민석은 국가 예산에 대해 엉뚱한 수치를 언급한 적이 있다."
출력: 김민석 국가 예산

입력: "대한민국 국가 채무 비율은 48.4%이다."
출력: 대한민국 국가 채무 비율

입력: "국가 채무 비율이 49.0%까지 갈 수 있다는 전망이 있다."
출력: 국가 채무 비율 전망

입력:
{claim}

출력:
""")
    return prompt | get_chat_llm()

def build_factcheck_chain():
    prompt = PromptTemplate.from_template("""
[당신의 역할]
당신은 특정 뉴스 문장이 **제시된 주장의 핵심 내용을 정확히 뒷받침하거나 반박하는 구체적인 사실 관계를 포함하는지**를 판단하는 엄격한 팩트체크 전문가입니다. 단순한 키워드 일치가 아닌, **주장의 주요 구성 요소(누가, 무엇을, 어떻게, 했는지 등)가 뉴스 스니펫에 직접적으로 설명**되어야만 합니다.

[판단 기준]

1.  **관련성 판단:**
    * **주어(인물/기관/단체)의 정확한 일치:** 뉴스 문장이 주장에 언급된 **주어를 정확히 지칭하고 그들의 행동, 발언, 정책 변화 등을 직접적으로 다룰 때만 "관련 있음"**으로 판단합니다.
        * 예: 주장이 '이재명 정부'에 대한 것인데, 뉴스는 '윤석열 정부'를 다룬다면 '관련성: 아니오'.
        * 예: 주장이 '정은경'에 대한 것인데, 뉴스는 '다른 인물'을 다룬다면 '관련성: 아니오'.
    * **핵심 동사/행위의 직접적인 설명:** 뉴스 문장이 주장의 **핵심 동사 또는 행위(예: '탈락했다', '임명되었다', '수익을 냈다', '싸움으로 바뀌었다' 등)에 대한 구체적인 사실을 직접적으로 설명**해야 "관련 있음"입니다.
        * 주장이 'A가 B를 했다'인데, 뉴스는 'A가 B를 할 예정'이거나 'A와 B가 관련 논란이 있다'는 식의 간접적인 내용만 있다면 '관련성: 아니오'.
    * **표현은 달라도 동일 사실:** 표현 방식은 달라도, 실제로 **동일한 사실 관계(주어+핵심 행위)를 전달**한다면 "관련 있음"입니다.
        * 예: "김민석은 국가채무를 20~30%로 안다고 말했다" ≈ "김민석은 채무 비율을 착각했다" → 관련 있음
        * 예: "A 장관이 사퇴 압력을 받았다" ≈ "여당에서 A 장관에 대한 경질론이 제기되었다" → 관련 있음
        * 예: "국정원 취업 청탁" ≈ "국정원 전 간부가 자녀 채용을 청탁받았다" → 관련 있음

2.  **사실 설명 여부 판단:**
    * 뉴스 문장이 **주장의 사실 여부를 입증하거나 반박하는 구체적인 팩트 자체를 전달**하는 목적으로 작성되었다면 "사실 설명: 예"입니다.
    * 뉴스가 비판하거나 감정을 표현하더라도 **주장의 사실 관계에 대한 구체적인 설명이 포함**되어 있으면 "예"입니다.
    * **주장에 특정 수치(예: 673조 원, 20%, 2022년)가 언급되었다면, 뉴스 스니펫에 해당 수치 또는 그에 대한 직접적인 사실 관계가 명확하게 언급되어야만 "예"로 판단합니다.** 불분명하거나 간접적인 언급은 "아니오"입니다.
    * **[추가된 핵심 규칙] 만약 주장이 어떤 사건에 대한 '특정 해석', '의도', '성격 규정'(예: '표적 수사', '정치적 탄압', '음모', '부당하다' 등 주관적인 판단이나 낙인)을 포함한다면, 뉴스 스니펫은 그 '해석/의도/성격 규정' 자체가 **사실임을 직접적으로 명시하거나 명백히 증명**해야만 "사실 설명: 예"입니다. 단순히 해당 사건의 '객관적인 진행 상황'이나 '관련 논란'만을 다루고 있다면, 주장의 '해석' 부분에 대한 '사실 설명'은 "아니오"로 판단하세요.**
    * 뉴스가 **팩트 없이 의견, 주장, 인용, 미래 예측, 가정, 추측, 소문만 전달**하는 경우는 "아니오"입니다.

[출력 형식]
- 관련성: 예 / 아니오
- 사실 설명 여부: 예 / 아니오
- 간단한 설명: (뉴스가 주장과 관련이 있는지, 어떤 사실을 설명하는지, 또는 부족한 이유를 1~2줄로 요약)
- **핵심 근거 문장: 뉴스 본문에서 관련성 판단의 근거가 되는 한 문장(직접 인용, 원문 그대로 한 줄 추출)**

[주의 사항 - 매우 중요]

* 뉴스에 인물 이름이 등장한다고 해서 자동으로 관련이 있는 것은 아닙니다. 반드시 **그 인물이 주장과 관련된 '무엇을 했는지' 또는 '어떤 상태인지'에 대한 직접적인 사실 관계**가 뉴스에 설명되어야만 합니다.
* **증거가 주장의 핵심 내용을 직접적으로 다루지 않는다면**, 아무리 유사한 키워드가 있더라도 **"관련성: 아니오"**로 판단하세요.
* 주관적 표현, 감정적 비판, 해석만 있고 사실적 근거가 없는 경우, "사실 설명: 아니오"입니다.
* **[강조] 주장이 특정 사건에 대한 '표적 수사', '탄압', '음모' 등과 같은 '성격 규정'이나 '의도 부여'를 담고 있다면, 뉴스 스니펫은 이러한 '성격 규정'이나 '의도'가 **사실임을 명확히 확인해주는 내용**을 포함해야만 합니다. 단순히 사건의 진행 상황만으로 해당 규정의 사실 여부를 판단하지 마세요.**

[입력]
주장:
{claim}

뉴스 일부:
{context}

판단:
""")
    return prompt | get_chat_llm()

def build_reduce_similar_claims_chain():
    """LLM 기반으로 의미적으로 유사하거나 중복되는 주장을 제거하는 체인을 구축합니다."""
    prompt = PromptTemplate.from_template("""
[역할]
당신은 제공된 주장 리스트에서 의미적으로 유사하거나 중복되는 주장을 식별하고, 
고유하고 핵심적인 주장들만 남겨 리스트 형태로 출력하는 전문가입니다.

[지시 사항]
- 각 주장은 의미론적으로 독립적이어야 합니다.
- 동일한 내용을 다른 방식으로 표현한 경우, 가장 명확하고 간결한 하나만 남깁니다.
- 당신이 새로운 주장을 만들어 내지 마십시오.
- 주장을 정치적, 인물적 중요도를 판단해 중요도 순으로 나열하세요.
- 출력 형식은 반드시 JSON 리스트여야 합니다. (예: ["주장1", "주장2"])

[입력 주장 리스트]
{claims_json}

[출력 형식]
["정제된 주장1", "정제된 주장2", ...]
""")
    
    return prompt | get_chat_llm()

def build_channel_type_classifier():
    prompt = PromptTemplate.from_template("""
[채널/영상 유형 분류 기준]
아래 중 가장 적합한 유형 하나를 선택하세요:

1. 선동형: 감정적 자극, 선동, 과장, 왜곡, 음모론, 혐오 발언이 많은 경우
2. 정보형: 객관적 정보, 데이터, 공식 자료 인용, 분석 위주
3. 논평형: 해설, 개인 의견, 가치 판단, 해석 위주
4. 혼합형: 정보와 선동, 정보와 논평 등 복수 유형이 혼합된 경우
5. 오락/가십형: 연예, 잡담, 풍자, 무당 예측, 썰 등 오락성 강한 경우
6. 기타/미분류: 위 범주에 속하지 않는 경우

[예시 출력]
채널 유형: 선동형  
분류 근거: 감정적 언사와 음모론, 검증 어려운 주장이 반복적으로 나타남.


[자막 전문]
{transcript}

당신의 답변:
""")
    return prompt | get_chat_llm()
