import re
import asyncio
import aiohttp
import hashlib
import os
import requests
from urllib.parse import urlparse, urlunparse, parse_qs
from bs4 import BeautifulSoup
from newspaper import Article
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service # Service ê°ì²´ ì¶”ê°€
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import logging

# Whisper ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from openai import OpenAI
import yt_dlp

# Logging setup
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

def _clean_text(text: str) -> str:
    """
    ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ì œê±°í•˜ê³ , ì €ì‘ê¶Œ ë¬¸êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """
    # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'\s+', ' ', text).strip()
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ìµœëŒ€ ë‘ ê°œë¡œ ì œí•œ
    text = re.sub(r'(\n){3,}', '\n\n', text)
    # ì €ì‘ê¶Œ ë¬¸êµ¬ ì œê±° (ì˜ˆì‹œ íŒ¨í„´, í•„ìš”ì— ë”°ë¼ ì¶”ê°€ ê°€ëŠ¥)
    text = re.sub(r'Copyright\s*.*ë¬´ë‹¨ì „ì¬.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Â©\s*.*All rights reserved.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'ì €ì‘ê¶Œì\s*.*ë¬´ë‹¨ë³µì œ.*', '', text, flags=re.IGNORECASE)
    return text.strip()

def _extract_article_content_with_selectors(html_content: str, url: str) -> str:
    """
    ë‰´ìŠ¤ HTMLì—ì„œ íŠ¹ì • ì–¸ë¡ ì‚¬ë³„ CSS ì„ íƒìë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # ê²½í–¥ì‹ ë¬¸ URL ë³€í™˜
    if "news.khan.co.kr/kh_news/khan_art_view.html" in url:
        match = re.search(r'artid=(\d+)', url)
        if match:
            art_id = match.group(1)
            url = f"https://www.khan.co.kr/article/{art_id}"

    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    # ë„ë©”ì¸ë³„ CSS ì„ íƒì ë§¤í•‘
    selectors = {
        "hani.co.kr": "div.article-text p.text",
        "khan.co.kr": "#articleBody p.content_text",
        "segye.com": "article.viewBox2",
        "hankookilbo.com": "div.col-main p.read",
        "asiatoday.co.kr": "div.news_bm",
        "seoul.co.kr": "div.viewContent",
        "donga.com": "section.news_view",
        "naeil.com": "div.article-view p",
    }

    selector = None
    for dom, sel in selectors.items():
        if dom in domain:
            selector = sel
            break

    if not selector:
        return "" # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ë¡ ì‚¬ ë„ë©”ì¸ì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

    soup = BeautifulSoup(html_content, 'html.parser')
    article_elements = []

    # íŠ¹ì • ì„ íƒìì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ ë¶„ë¦¬
    if domain in ["hani.co.kr", "khan.co.kr", "hankookilbo.com", "naeil.com"]:
        # p íƒœê·¸ê°€ ì§ì ‘ ë³¸ë¬¸ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ” ê²½ìš°
        elements = soup.select(selector)
        for p_tag in elements:
            # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
            for br in p_tag.find_all('br'):
                br.replace_with('\\n')
            text = p_tag.get_text(separator=' ').strip()
            if text: # ë¹ˆ ë¬¸ìì—´ì€ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                article_elements.append(text)
    elif domain in ["segye.com", "asiatoday.co.kr", "seoul.co.kr", "donga.com"]:
        # ì „ì²´ ë³¸ë¬¸ ì˜ì—­ì„ ì„ íƒí•˜ê³  ë‚´ë¶€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        main_content_div = soup.select_one(selector)
        if main_content_div:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ì´ë¯¸ì§€, í…Œì´ë¸” ë“±)
            for tag in main_content_div.find_all(['script', 'style', 'img', 'table', 'figure', 'figcaption', 'aside', 'nav', 'footer', 'header', 'iframe', 'video', 'audio', 'meta', 'link', 'form', 'input', 'button', 'select', 'textarea', 'svg', 'canvas', 'map', 'area', 'object', 'param', 'embed', 'source', 'track', 'picture', 'portal', 'slot', 'template', 'noscript', 'ins', 'del', 'bdo', 'bdi', 'rp', 'rt', 'rtc', 'ruby', 'data', 'time', 'mark', 'small', 'sub', 'sup', 'abbr', 'acronym', 'address', 'b', 'big', 'blockquote', 'center', 'cite', 'code', 'dd', 'dfn', 'dir', 'dl', 'dt', 'em', 'font', 'i', 'kbd', 'li', 'menu', 'ol', 'pre', 'q', 's', 'samp', 'strike', 'strong', 'tt', 'u', 'var', 'ul', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                tag.decompose()
            
            # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
            for br in main_content_div.find_all('br'):
                br.replace_with('\\n')

            # p íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
            paragraphs = []
            for content in main_content_div.contents:
                if content.name == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())
            
            article_elements = paragraphs
    
    # ì¶”ì¶œëœ ë¬¸ë‹¨ë“¤ì„ ê²°í•©
    full_text = '\n\n'.join(filter(None, article_elements))
    
    return full_text

def extract_video_id(url: str):
    """
    ìœ íŠœë¸Œ URLì—ì„œ 11ìë¦¬ video_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ í˜•íƒœì˜ URLì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì •ê·œì‹ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
    """
    try:
        # ëª¨ë“  ê°€ëŠ¥í•œ YouTube URL íŒ¨í„´ì„ ì²˜ë¦¬í•˜ëŠ” ì •ê·œì‹
        match = re.search(
            r"(?:v=|/|youtu\.be/|shorts/|embed/)([0-9A-Za-z_-]{11})", url
        )
        if match:
            video_id = match.group(1)
            logging.info(f"[ë””ë²„ê¹…] URLì—ì„œ ì¶”ì¶œëœ video_id: {video_id}")
            return video_id
    except Exception as e:
        logging.error(f"ìœ íŠœë¸Œ video_id ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def fetch_youtube_transcript(video_url):
    """
    EC2 ë‚´ë¶€ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ í›„ Whisperë¡œ ìë§‰ ì¶”ì¶œ
    - openai ë¼ì´ë¸ŒëŸ¬ë¦¬ v1.0.0+ì— ë§ì¶° API í˜¸ì¶œ ë°©ì‹ ìˆ˜ì •
    - yt-dlp ë‹¤ìš´ë¡œë“œ íŒŒì¼ì— í™•ì¥ìë¥¼ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
    """
    video_id = extract_video_id(video_url)
    logging.info(f"[ë””ë²„ê¹…] ì¶”ì¶œëœ video_id: {video_id}")
    if not video_id:
        logging.error("ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return ""
    
    # OpenAI v1.0.0+ API ì‚¬ìš©
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        logging.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return ""

    cookies_path = "/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt"
    # yt-dlpê°€ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ëª…ì„ ì§€ì •í•©ë‹ˆë‹¤. íŒŒì¼ í™•ì¥ìë¥¼ í¬í•¨í•˜ë„ë¡ í…œí”Œë¦¿ ìˆ˜ì •.
    temp_audio_file_template = f"{video_id}.%(ext)s"
    downloaded_file_paths = []

    try:
        ydl_opts = {
            'format': 'bestaudio/best', # ìµœì ì˜ ì˜¤ë””ì˜¤ í¬ë§·ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
            'outtmpl': temp_audio_file_template,
            'cookiefile': cookies_path,
            'quiet': True,
        }

        logging.info(f"ğŸ¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì˜ ì‹¤ì œ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            download_info = ydl.sanitize_info(info)
            if 'requested_downloads' in download_info:
                downloaded_file_paths = [d['filepath'] for d in download_info['requested_downloads']]
            elif '_filename' in download_info:
                # ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œì˜ ê²½ìš°
                downloaded_file_paths.append(download_info['_filename'])
            
            if not downloaded_file_paths:
                raise Exception("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

            actual_audio_file = downloaded_file_paths[0]

        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {actual_audio_file}")

        with open(actual_audio_file, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ko"
            )
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
        return transcript.text

    except Exception as e:
        logging.exception(f"yt-dlp ë˜ëŠ” Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        # ë‹¤ìš´ë¡œë“œëœ ëª¨ë“  ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for file_path in downloaded_file_paths:
            if os.path.exists(file_path):
                os.remove(file_path)
                logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")


def extract_chosun_with_selenium(url: str):
    """
    ë™ê¸°ì ìœ¼ë¡œ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    
    driver = None
    try:
        logging.info(f"ğŸ“° Seleniumìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        # Service ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ driver ìƒì„±
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        
        # ì¡°ì„ ì¼ë³´ ì£¼ê°„ì¡°ì„  ë° ì¼ë°˜ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒì
        article_selector = "article#article-view-content-div, article.layout__article-main section.article-body"
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, article_selector)))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        article_content = []
        article_body_container = soup.select_one('article.layout__article-main section.article-body')
        if not article_body_container:
            article_body_container = soup.select_one('article#article-view-content-div')
        
        if article_body_container:
            paragraphs = article_body_container.find_all("p")
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                    article_content.append(text)
            full_text = "\\n".join(article_content)
            
            if full_text and len(full_text) > 100:
                logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""
            
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()

def _extract_generic_with_selenium(url: str) -> str:
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    driver = None
    try:
        logging.info(f"ğŸ“° Selenium (Generic)ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        
        # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤ (ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì— ì ìš©ë  ìˆ˜ ìˆë„ë¡)
        generic_article_selectors = [
            "div.article_content", "div#articleBodyContents", "div#article_body", 
            "div.news_content", "article.article_view", "div.view_content",
            "div.article-text", "div.article-body", "div.entry-content",
            "div.contents_area", "div.news_view", "div.viewContent",
            "article.viewBox2", "div.col-main", "div.news_bm", "section.news_view",
            "div.article-view"
        ]
        
        # í˜ì´ì§€ ë¡œë“œë¥¼ ê¸°ë‹¤ë¦¬ê±°ë‚˜, ë³¸ë¬¸ ìš”ì†Œ ì¤‘ í•˜ë‚˜ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ", ".join(generic_article_selectors))))
        except TimeoutException:
            logging.warning(f"âš ï¸ Selenium (Generic): ë³¸ë¬¸ ìš”ì†Œê°€ 10ì´ˆ ë‚´ì— ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ í˜ì´ì§€ ì†ŒìŠ¤ ì‚¬ìš©.")

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        article_body_container = None
        for selector in generic_article_selectors:
            article_body_container = soup.select_one(selector)
            if article_body_container:
                break
        
        if article_body_container:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ì´ë¯¸ì§€, í…Œì´ë¸” ë“±)
            for tag in article_body_container.find_all(['script', 'style', 'img', 'table', 'figure', 'figcaption', 'aside', 'nav', 'footer', 'header', 'iframe', 'video', 'audio', 'meta', 'link', 'form', 'input', 'button', 'select', 'textarea', 'svg', 'canvas', 'map', 'area', 'object', 'param', 'embed', 'source', 'track', 'picture', 'portal', 'slot', 'template', 'noscript', 'ins', 'del', 'bdo', 'bdi', 'rp', 'rt', 'rtc', 'ruby', 'data', 'time', 'mark', 'small', 'sub', 'sup', 'abbr', 'acronym', 'address', 'b', 'big', 'blockquote', 'center', 'cite', 'code', 'dd', 'dfn', 'dir', 'dl', 'dt', 'em', 'font', 'i', 'kbd', 'li', 'menu', 'ol', 'pre', 'q', 's', 'samp', 'strike', 'strong', 'tt', 'u', 'var', 'ul', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                tag.decompose()
            
            # <br> íƒœê·¸ë¥¼ \nìœ¼ë¡œ ë³€í™˜
            for br in article_body_container.find_all('br'):
                br.replace_with('\\n')

            # p íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
            paragraphs = []
            for content in article_body_container.contents:
                if content.name == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())
            
            full_text = "\\n\\n".join(filter(None, paragraphs))

            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            # íŠ¹ì • ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´, í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ì‹œë„
            logging.warning("Selenium (Generic): íŠ¹ì • ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            full_text = soup.get_text(separator='\\n', strip=True)
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ë„ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
            
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium (Generic) í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium (Generic) í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()

async def get_article_text(url: str):
    """
    ë¹„ë™ê¸°ì ìœ¼ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜.
    ì¡°ì„ ì¼ë³´ì˜ ê²½ìš° Seleniumì„ ë¨¼ì € ì‚¬ìš©í•˜ê³ ,
    ê·¸ ì™¸ì˜ ê²½ìš° aiohttp -> newspaper -> BeautifulSoup ìˆœì„œë¡œ í´ë°±ì„ ì‹œë„í•©ë‹ˆë‹¤.
    """
    logging.info(f"ğŸ“° ë¹„ë™ê¸°ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {url}")
    parsed_url = urlparse(url)
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))

    # 1. ì¡°ì„ ì¼ë³´ URLì¼ ê²½ìš°, Seleniumì„ ë¨¼ì € ì‹¤í–‰
    if "chosun.com" in parsed_url.netloc:
        logging.info("â­ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ê°ì§€. Selenium í¬ë¡¤ë§ì„ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸° ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            text = await asyncio.to_thread(extract_chosun_with_selenium, url)
            if text and len(text) > 100:
                logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return _clean_text(text) # Selenium ê²°ê³¼ë„ ì •ì œ
            else:
                logging.warning("âš ï¸ Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ì„ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return None
        except Exception as e:
            logging.error(f"âŒ asyncio.to_thread Selenium ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    # 2. ê·¸ ì™¸ì˜ URLì¼ ê²½ìš°, ê¸°ì¡´ì˜ íš¨ìœ¨ì ì¸ ë°©ë²• ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
    # aiohttp + newspaper ì‹œë„
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(clean_url, timeout=30) as response:
                response.raise_for_status()
                html_content = await response.text()
                
                article = Article(clean_url, language='ko')
                article.download(input_html=html_content)
                article.parse()
                
                if article.text and len(article.text) > 300:
                    logging.info(f"âœ… newspaperë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(article.text)}ì): {url}")
                    return _clean_text(article.text) # newspaper ê²°ê³¼ ì •ì œ
                else:
                    logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•¨. BeautifulSoup í´ë°± ì‹œë„: {url}")
    except aiohttp.ClientError as e:
        logging.warning(f"âš ï¸ aiohttp í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ ë°œìƒ. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ì‹¤íŒ¨. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")

    # requests + BeautifulSoup í´ë°± ì‹œë„ (ì–¸ë¡ ì‚¬ë³„ ì„ íƒì ì ìš©)
    extracted_text_from_selectors = ""
    try:
        logging.warning(f"âš ï¸ requests+BeautifulSoupìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì¬ì‹œë„ (ì–¸ë¡ ì‚¬ë³„ ì„ íƒì ì ìš©): {url}")
        response = requests.get(clean_url, headers=headers, timeout=30)
        response.raise_for_status()
        html_content = response.text
        
        # ì–¸ë¡ ì‚¬ë³„ ì„ íƒìë¥¼ ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ
        extracted_text_from_selectors = _extract_article_content_with_selectors(html_content, url)

        if extracted_text_from_selectors and len(extracted_text_from_selectors) > 100:
            cleaned_final_text = _clean_text(extracted_text_from_selectors)
            logging.info(f"âœ… requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„)ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(cleaned_final_text)}ì): {url}")
            return cleaned_final_text
        else:
            logging.warning(f"âš ï¸ requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„)ë¡œ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ. ìµœì¢… í´ë°± ì‹œë„.")
            
    except requests.exceptions.RequestException as e:
        logging.warning(f"âš ï¸ requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„) ì‹¤íŒ¨. ìµœì¢… í´ë°± ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ BeautifulSoup íŒŒì‹± ì‹¤íŒ¨ (ì–¸ë¡ ì‚¬ë³„). ìµœì¢… í´ë°± ì‹œë„: {url} -> {e}")

    # ìµœì¢… í´ë°±: Selenium (Generic) ì‹œë„
    logging.warning(f"âš ï¸ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨. ìµœì¢… í´ë°±: Selenium (Generic) ì‹œë„: {url}")
    try:
        text = await asyncio.to_thread(_extract_generic_with_selenium, url)
        if text and len(text) > 100:
            logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ìµœì¢… ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            return _clean_text(text) # Selenium ê²°ê³¼ë„ ì •ì œ
        else:
            logging.warning("âš ï¸ Selenium (Generic)ìœ¼ë¡œë„ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤.")
            return None
    except Exception as e:
        logging.error(f"âŒ asyncio.to_thread Selenium (Generic) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

    logging.error(f"âŒ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}")
    return None

def clean_news_title(title):
    """
    ë‰´ìŠ¤ ì œëª©ì—ì„œ ì–¸ë¡ ì‚¬ëª…, ìŠ¬ë¡œê±´, íŠ¹ìˆ˜ íƒœê·¸, ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë“±ì„ ì œê±°í•˜ì—¬ ì •ì œí•©ë‹ˆë‹¤.
    """
    patterns_to_remove = [
        r'ëŒ€í•œë¯¼êµ­ ì˜¤í›„ë¥¼ ì—¬ëŠ” ìœ ì¼ì„ê°„ ë¬¸í™”ì¼ë³´',
        r'\| ë¬¸í™”ì¼ë³´', r'ë¬¸í™”ì¼ë³´',
        r'\| ì¤‘ì•™ì¼ë³´', r'ì¤‘ì•™ì¼ë³´',
        r'\| ê²½í–¥ì‹ ë¬¸', r'ê²½í–¥ì‹ ë¬¸',
        r'ë¨¸ë‹ˆíˆ¬ë°ì´', r'MBN', r'ì—°í•©ë‰´ìŠ¤', r'SBS ë‰´ìŠ¤', r'MBC ë‰´ìŠ¤', r'KBS ë‰´ìŠ¤',
        r'ë™ì•„ì¼ë³´', r'ì¡°ì„ ì¼ë³´', r'í•œê²¨ë ˆ', r'êµ­ë¯¼ì¼ë³´', r'ì„œìš¸ì‹ ë¬¸', r'ì„¸ê³„ì¼ë³´',
        r'ë…¸ì»·ë‰´ìŠ¤', r'í—¤ëŸ´ë“œê²½ì œ', r'ë§¤ì¼ê²½ì œ', r'í•œêµ­ê²½ì œ', r'ì•„ì‹œì•„ê²½ì œ',
        r'YTN', r'JTBC', r'TVì¡°ì„ ', r'ì±„ë„A', r'ë°ì¼ë¦¬ì•ˆ', r'ë‰´ì‹œìŠ¤',
        r'ë‰´ìŠ¤1', r'ì—°í•©ë‰´ìŠ¤TV', r'ë‰´ìŠ¤í•Œ', r'ì´ë°ì¼ë¦¬', r'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
        r'ì•„ì£¼ê²½ì œ', r'UPIë‰´ìŠ¤', r'ZUM ë‰´ìŠ¤', r'ë„¤ì´íŠ¸ ë‰´ìŠ¤', r'ë‹¤ìŒ ë‰´ìŠ¤',
    ]

    patterns_to_remove_regex = [
        r'\[.*\]',
        r'\(.*?\)',
        r'\{.*?\}',
        r'<[^>]+>',
        r'\[\s*\w+\s*\]',
    ]

    symbols_to_remove = [
        r'\|', r':', r'_', r'-', r'\+', r'=', r'/', r'\\', r'\'', r'\"', r'â€˜', r'â€™', r'â€œ', r'â€', r'â€¦', r'Â·', r'â–²', r'â–¼', r'â– ', r'â–¡', r'â—', r'â—‹', r'â—†', r'â—‡', r'â˜…', r'â˜†', r'â€»', r'!', r'@', r'#', r'$', r'%', r'^', r'&', r'\*', r'\(', r'\)', r'~', r'`', r'\{', r'\}', r'\[', r'\]', r';', r',', r'\.', r'<', r'>', r'?'
    ]

    cleaned_title = title
    
    for pattern in patterns_to_remove:
        cleaned_title = re.sub(re.escape(pattern), '', cleaned_title, flags=re.IGNORECASE).strip()
    
    for pattern_regex in patterns_to_remove_regex + symbols_to_remove:
        cleaned_title = re.sub(pattern_regex, '', cleaned_title).strip()
    
    cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
    
    return cleaned_title

async def search_news_google_cs(query):
    logging.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    """Google Custom Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")

    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": google_api_key,
        "cx": google_cse_id,
        "q": query,
        "num": 10,
        "hl": "ko",
        "gl": "kr"
    }
    async with aiohttp.ClientSession() as session:
        async with session.get(url, params=params) as resp:
            data = await resp.json()
            if "error" in data:
                logging.error(f"Google CSE API ì˜¤ë¥˜: {data['error']['message']}")
                if "quotaExceeded" in data['error']['message']:
                    logging.warning("ê²½ê³ : Google CSE API í• ë‹¹ëŸ‰(Quota)ì´ ì´ˆê³¼ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Google Cloud Consoleì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return []
            return data.get("items", [])

def calculate_fact_check_confidence(criteria_scores):
    """
    íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ 0%ì—ì„œ 100%ê¹Œì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if not criteria_scores:
        return 0

    total_possible_score = 0
    total_actual_score = 0

    for criterion, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: '{criterion}'ì˜ ì ìˆ˜ '{score}'ê°€ ìœ íš¨í•œ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        total_possible_score += 5
        total_actual_score += score

    if total_actual_score == 0 and total_possible_score > 0:
        return 0

    if total_possible_score == 0:
        return 0

    confidence_percentage = (total_actual_score / total_possible_score) * 100

    return max(0, min(100, round(confidence_percentage)))

def calculate_source_diversity_score(evidence):
    """
    ì œê³µëœ ê·¼ê±°(evidence)ì˜ ì¶œì²˜ ë‹¤ì–‘ì„±ì„ ê³„ì‚°í•˜ì—¬ 0-5ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not evidence:
        return 0
    
    unique_sources = set()
    for item in evidence:
        # source_titleì´ ìˆë‹¤ë©´ ì´ë¥¼ ì‚¬ìš© (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬)
        if item.get("source_title"):
            unique_sources.add(item["source_title"].lower())
        # source_titleì´ ì—†ìœ¼ë©´ URLì˜ ë„ë©”ì¸ì„ ì‚¬ìš©
        elif item.get("url"):
            try:
                domain = urlparse(item["url"]).netloc
                if domain:
                    unique_sources.add(domain.lower())
            except Exception:
                # URL íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë¬´ì‹œ
                pass

    num_unique_sources = len(unique_sources)

    if num_unique_sources >= 4:
        return 5
    elif num_unique_sources == 3:
        return 4
    elif num_unique_sources == 2:
        return 3
    elif num_unique_sources == 1:
        return 1
    else:
        return 0