# core/lambdas.py
import os
import re
import json
import time
import asyncio
import logging
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, parse_qs

# --- optional deps (ì¡´ì¬ ì•ˆ í•´ë„ í„°ì§€ì§€ ì•Šê²Œ) ---
try:
    import aiohttp
except Exception:
    aiohttp = None

try:
    import requests
except Exception:
    requests = None

try:
    from bs4 import BeautifulSoup
except Exception:
    BeautifulSoup = None

try:
    from newspaper import Article
except Exception:
    Article = None

# Seleniumì€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš©
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
except Exception:
    webdriver = None
    ChromeOptions = None
    By = None
    WebDriverWait = None
    EC = None

# ìœ íŠœë¸Œ ìë§‰
try:
    from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
except Exception:
    YouTubeTranscriptApi = None
    TranscriptsDisabled = Exception
    NoTranscriptFound = Exception

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# ìœ í‹¸
# ------------------------------------------------------------------------------
def extract_video_id(youtube_url: str) -> Optional[str]:
    """
    ìœ íŠœë¸Œ URLì—ì„œ video_id ì¶”ì¶œ
    """
    try:
        if "youtu.be/" in youtube_url:
            return youtube_url.rstrip("/").split("/")[-1].split("?")[0]
        parsed = urlparse(youtube_url)
        if parsed.netloc.endswith("youtube.com"):
            qs = parse_qs(parsed.query)
            if "v" in qs and qs["v"]:
                return qs["v"][0]
            # /shorts/<id>
            parts = parsed.path.strip("/").split("/")
            if len(parts) >= 2 and parts[0] == "shorts":
                return parts[1]
    except Exception:
        pass
    return None


def fetch_youtube_transcript(youtube_url: str) -> str:
    """
    YouTubeTranscriptApiê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜(ì—ëŸ¬ ì—†ì´)
    """
    video_id = extract_video_id(youtube_url)
    if not video_id:
        logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ ìœ íŠœë¸Œ URL")
        return ""

    if YouTubeTranscriptApi is None:
        logger.warning("YouTubeTranscriptApi ë¯¸ì„¤ì¹˜: transcript ìƒëµ")
        return ""

    try:
        # í•œ/ì˜ ìš°ì„  ì‹œë„
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        preferred = None
        for lang_code in ("ko", "en", "a.en"):
            try:
                preferred = transcript_list.find_manually_created_transcript([lang_code])
                break
            except Exception:
                try:
                    preferred = transcript_list.find_transcript([lang_code])
                    break
                except Exception:
                    continue

        if preferred is None:
            preferred = transcript_list.find_transcript(transcript_list._translation_languages or ["en"])

        lines = preferred.fetch()
        text = " ".join([seg.get("text", "").replace("\n", " ").strip() for seg in lines if seg.get("text")])
        return text.strip()
    except (TranscriptsDisabled, NoTranscriptFound):
        logger.warning("ìë§‰ ì‚¬ìš© ë¶ˆê°€/ì—†ìŒ")
        return ""
    except Exception as e:
        logger.exception(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""


def clean_news_title(title: str) -> str:
    """
    ë‰´ìŠ¤ ì œëª© ì „ì²˜ë¦¬: ê´„í˜¸/ëŒ€ê´„í˜¸ íƒœê·¸ ì œê±°, ê³µë°± ì •ëˆ
    """
    if not title:
        return ""
    t = re.sub(r"\[[^\]]*\]", " ", title)
    t = re.sub(r"\([^\)]*\)", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


# ------------------------------------------------------------------------------
# Google CSE
# ------------------------------------------------------------------------------
async def search_news_google_cs(query: str) -> List[Dict[str, Any]]:
    """
    Google Custom Search (CSE)ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰.
    í™˜ê²½ë³€ìˆ˜:
      - GOOGLE_CSE_API_KEY
      - GOOGLE_CSE_CX
    ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´, ì—ëŸ¬ ì—†ì´ [] ë°˜í™˜
    """
    api_key = os.environ.get("GOOGLE_CSE_API_KEY")
    cx = os.environ.get("GOOGLE_CSE_CX") or os.environ.get("GOOGLE_CSE_CX_ID")
    if not api_key or not cx:
        logger.warning("CSE í‚¤/ì‹ë³„ì ë¯¸ì„¤ì • â†’ ê²€ìƒ‰ ìƒëµ")
        return []

    if aiohttp is None:
        logger.warning("aiohttp ë¯¸ì„¤ì¹˜ â†’ ê²€ìƒ‰ ìƒëµ")
        return []

    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cx,
        "q": query,
        "num": 10,
        "safe": "off",
        "hq": "site:news",
        "gl": "kr",
        "lr": "lang_ko",
    }
    logger.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
            async with session.get(url, params=params) as resp:
                if resp.status != 200:
                    logger.warning(f"CSE ì‘ë‹µ ì˜¤ë¥˜: HTTP {resp.status}")
                    return []
                data = await resp.json()
                items = data.get("items", []) or []
                # ìµœì†Œí•œì˜ í‘œì¤€í™”
                out = []
                for it in items:
                    out.append({
                        "title": it.get("title"),
                        "link": it.get("link"),
                        "snippet": it.get("snippet"),
                        "displayLink": it.get("displayLink"),
                    })
                return out
    except Exception as e:
        logger.exception(f"CSE í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ------------------------------------------------------------------------------
# ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
# ------------------------------------------------------------------------------
def _parse_with_newspaper(url: str) -> str:
    if Article is None:
        return ""
    try:
        art = Article(url, language="ko")
        art.download()
        art.parse()
        text = (art.text or "").strip()
        return text
    except Exception:
        return ""


def _parse_with_requests_bs4(url: str) -> str:
    if requests is None or BeautifulSoup is None:
        return ""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = resp.apparent_encoding or "utf-8"
        if resp.status_code != 200:
            return ""
        html = resp.text
        # ì–¸ë¡ ì‚¬ë³„ ê°€ì¥ í”í•œ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ í›„ë³´
        soup = BeautifulSoup(html, "html.parser")
        selectors = [
            "article", ".article", "#article", ".art_body", "#newsEndContents",
            ".news_body", ".article_body", "#articleBody", "#articeBody", ".article_view",
            ".story-news", ".media_end_head", ".view_con", "#CmAdContent", ".tts_body",
            "section#article-view", "div#article-view-content-div", ".article-txt", ".at_contents"
        ]
        best = ""
        for sel in selectors:
            node = soup.select_one(sel)
            if node and node.get_text(strip=True):
                text = node.get_text(separator=" ", strip=True)
                if len(text) > len(best):
                    best = text
        if not best:
            # ì „ì²´ í…ìŠ¤íŠ¸ë¼ë„
            best = soup.get_text(separator=" ", strip=True)
        return best.strip()
    except Exception as e:
        logger.warning(f"requests+BeautifulSoup ì‹¤íŒ¨: {url} -> {e}")
        return ""


def _selenium_driver() -> Optional["webdriver.Chrome"]:
    """
    ê°€ëŠ¥í•œ ê²½ë¡œë¡œ ìµœëŒ€í•œ ì‹œë„:
    - í™˜ê²½ë³€ìˆ˜ CHROMEDRIVER (ìƒëŒ€ê²½ë¡œ í—ˆìš©)
    - ì‹œìŠ¤í…œ PATH
    """
    if webdriver is None or ChromeOptions is None:
        return None
    try:
        options = ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1280,2000")
        bin_path = os.environ.get("CHROME_BINARY")
        if bin_path:
            options.binary_location = bin_path

        driver_path = os.environ.get("CHROMEDRIVER")
        if driver_path:
            return webdriver.Chrome(driver_path, options=options)  # ìƒëŒ€ê²½ë¡œ OK

        # ë“œë¼ì´ë²„ ê²½ë¡œ ë¯¸ì§€ì • â†’ ì‹œìŠ¤í…œ PATH ë‚´ í¬ë¡¬ë“œë¼ì´ë²„ ì‚¬ìš©
        return webdriver.Chrome(options=options)
    except Exception as e:
        logger.warning(f"Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None


def _parse_with_selenium(url: str, wait_selector: Optional[str] = None) -> str:
    driver = _selenium_driver()
    if driver is None:
        return ""
    try:
        driver.get(url)
        # ì„ íƒì ê¸°ë‹¤ë¦¬ê¸° (ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸)
        text = ""
        try:
            if wait_selector and WebDriverWait is not None and EC is not None and By is not None:
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector)))
                node = driver.find_element(By.CSS_SELECTOR, wait_selector)
                text = node.text.strip()
        except Exception:
            pass

        if not text:
            # í”í•œ ë³¸ë¬¸ ì„ íƒìë“¤
            candidates = [
                "article", ".article", "#article", ".art_body", "#newsEndContents",
                ".news_body", ".article_body", "#articleBody", "#articeBody", ".article_view",
                ".view_con", ".news_view", ".content", ".container"
            ]
            for sel in candidates:
                try:
                    if By:
                        nodes = driver.find_elements(By.CSS_SELECTOR, sel)
                        for n in nodes:
                            t = n.text.strip()
                            if len(t) > len(text):
                                text = t
                except Exception:
                    continue

        if not text:
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸
            text = driver.find_element(By.TAG_NAME, "body").text.strip() if By else ""

        return text
    except Exception as e:
        logger.warning(f"Selenium íŒŒì‹± ì‹¤íŒ¨: {url} -> {e}")
        return ""
    finally:
        try:
            driver.quit()
        except Exception:
            pass


async def get_article_text(url: str) -> str:
    """
    ë¹„ë™ê¸° ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ:
      1) newspaper
      2) requests+BeautifulSoup
      3) Selenium generic
    ì‚¬ì´íŠ¸/ìƒí™© ë”°ë¼ ì–´ëŠ ë‹¨ê³„ì—ì„œë“  ì„±ê³µí•˜ë©´ ì¦‰ì‹œ ë°˜í™˜
    """
    logger.info(f"ğŸ“° ë¹„ë™ê¸°ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {url}")

    # 1) newspaper (ìŠ¤ë ˆë“œ í’€)
    loop = asyncio.get_running_loop()
    text = ""
    try:
        text = await loop.run_in_executor(None, _parse_with_newspaper, url)
        if len(text) >= 600:
            logger.info(f"âœ… newspaperë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)}ì): {url}")
            return text
        else:
            logger.warning(f"âš ï¸ newspaper ê²°ê³¼ ë¶ˆì¶©ë¶„. BeautifulSoup ì„ íƒì í´ë°±: {url}")
    except Exception as e:
        logger.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ì‹¤íŒ¨. ë‹¤ìŒ ì‹œë„: {url} -> {e}")

    # 2) requests+BeautifulSoup
    try:
        text = await loop.run_in_executor(None, _parse_with_requests_bs4, url)
        if len(text) >= 600:
            logger.info(f"âœ… requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„) ì„±ê³µ ({len(text)}ì): {url}")
            return text
        else:
            logger.warning(f"âš ï¸ ì–¸ë¡ ì‚¬ë³„ ì„ íƒì ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ. Selenium í´ë°±.")
    except Exception as e:
        logger.warning(f"âš ï¸ requests ë‹¨ê³„ ì‹¤íŒ¨. Selenium í´ë°±: {url} -> {e}")

    # 3) Selenium (ë„ë©”ì¸ íŠ¹í™” ìš°ì„  selector ì—†ìŒ: generic)
    logger.warning(f"âš ï¸ ìµœì¢… í´ë°±: Selenium (Generic) ì‹œë„: {url}")
    s_text = await loop.run_in_executor(None, _parse_with_selenium, url, None)
    if len(s_text) >= 300:
        logger.info("âœ… Selenium (Generic)ìœ¼ë¡œ ìµœì¢… ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
        return s_text
    elif s_text:
        logger.warning("Selenium (Generic): ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ.")
    else:
        logger.warning("âš ï¸ Selenium (Generic)ë„ ë¶ˆì¶©ë¶„/ì‹¤íŒ¨.")
    return ""


# ------------------------------------------------------------------------------
# ì ìˆ˜ ê³„ì‚°
# ------------------------------------------------------------------------------
def calculate_source_diversity_score(evidence: List[Dict[str, Any]]) -> int:
    """
    ë™ì¼ ë„ë©”ì¸/ì œëª© ì¤‘ë³µ ì œê±°í•œ ë‹¤ì–‘í•œ ì¶œì²˜ ê°œìˆ˜ì— ë”°ë¼ 0~5ì 
    """
    try:
        if not evidence:
            return 0
        unique = set()
        for item in evidence:
            st = (item or {}).get("source_title")
            if st:
                unique.add(st.lower().strip())
                continue
            url = (item or {}).get("url")
            if url:
                try:
                    dom = urlparse(url).netloc
                    if dom:
                        unique.add(dom.lower())
                except Exception:
                    pass
        n = len(unique)
        if n >= 4:
            return 5
        if n == 3:
            return 4
        if n == 2:
            return 3
        if n == 1:
            return 1
        return 0
    except Exception as e:
        logger.exception(f"source diversity ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0


def calculate_fact_check_confidence(features: Dict[str, Any]) -> int:
    """
    ê°„ë‹¨ ê°€ì¤‘ì¹˜ í•©ì‚° (0~100)
      - evidence_count: 0~5ê°œ â†’ 0~60ì 
      - source_diversity: 0~5ì  â†’ 0~40ì 
    """
    try:
        ev = max(0, min(int(features.get("evidence_count", 0)), 5))
        sd = max(0, min(int(features.get("source_diversity", 0)), 5))
        score = round(ev * 12 + sd * 8)  # 5*12 + 5*8 = 100
        return max(0, min(score, 100))
    except Exception:
        return 0
