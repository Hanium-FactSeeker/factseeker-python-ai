import re
import asyncio
import aiohttp
import hashlib
import os
import requests
from urllib.parse import urlparse, urlunparse
from bs4 import BeautifulSoup
from newspaper import Article
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
import logging

# Whisper ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import openai
import yt_dlp

# Google API Key ë° CSE IDëŠ” main.pyì—ì„œ ë¡œë“œë˜ë¯€ë¡œ, ì—¬ê¸°ì„œ ì§ì ‘ ì°¸ì¡°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# ëŒ€ì‹ , í•„ìš”í•œ ê²½ìš° í•¨ìˆ˜ ì¸ìë¡œ ë°›ê±°ë‚˜ ì „ì—­ ì„¤ì • ê°ì²´ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” get_article_textê°€ ì™¸ë¶€ ì˜ì¡´ì„±ì„ ê°€ì§€ì§€ ì•Šë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.


def extract_video_id(url):
    """YouTube URLì—ì„œ ë¹„ë””ì˜¤ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'(?:v=|/)([0-9A-Za-z_-]{11})', url)
    return match.group(1) if match else None


async def fetch_youtube_transcript(video_id):
    """
    yt-dlpì™€ OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube ë¹„ë””ì˜¤ì˜ ìë§‰ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not video_id:
        logging.error("ë¹„ë””ì˜¤ IDê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""

    # OpenAI API í‚¤ ì„¤ì •
    openai.api_key = os.getenv("OPENAI_API_KEY")
    if not openai.api_key:
        logging.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return ""

    temp_audio_file = None
    try:
        # 1. yt-dlpë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube ì˜ìƒì˜ ìŒì› ë‹¤ìš´ë¡œë“œ
        audio_filename = f"{video_id}.mp3"
        ydl_opts = {
            'format': 'bestaudio/best',
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'mp3',
                'preferredquality': '192',
            }],
            'outtmpl': audio_filename,
            'cookiefile': '/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt', 
            'no_check_certificate': True,  # âœ… EC2ì— ì—…ë¡œë“œí•œ ì¿ í‚¤ íŒŒì¼ ì‚¬ìš©
            'quiet': True,
        }
        
        logging.info(f"ğŸ¶ yt-dlpë¡œ YouTube ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_id}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(f'https://www.youtube.com/watch?v={video_id}', download=True)
        
        temp_audio_file = audio_filename
        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {temp_audio_file}")

        # 2. Whisper API í˜¸ì¶œ
        with open(temp_audio_file, "rb") as audio_file:
            transcript = openai.Audio.transcribe(
                "whisper-1",
                audio_file,
                language="ko"  # í•œêµ­ì–´ ëª¨ë¸ ì§€ì •
            )
        
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ìƒì„± ì™„ë£Œ")
        return transcript.text

    except yt_dlp.utils.DownloadError as e:
        logging.exception(f"yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return ""
    except openai.error.OpenAIError as e:
        logging.exception(f"Whisper API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return ""
    except Exception as e:
        logging.exception(f"YouTube ìŒì› ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        # 3. ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_audio_file and os.path.exists(temp_audio_file):
            os.remove(temp_audio_file)
            logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {temp_audio_file}")


def extract_chosun_with_selenium(url):
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ë§ê²Œ ChromeDriver ê²½ë¡œ ì„¤ì • í•„ìš”)
    """
    options = Options()
    options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì•ˆ ë„ìš°ê³  ì‹¤í–‰
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")  # GPU ì‚¬ìš© ë¹„í™œì„±í™” (ì¼ë¶€ í™˜ê²½ì—ì„œ í•„ìš”)
    options.add_argument("--window-size=1920,1080")  # ì°½ í¬ê¸° ì„¤ì •
    
    # --- ì¤‘ìš”: ë¡œì»¬ í™˜ê²½ì— ë§ëŠ” ChromeDriver ê²½ë¡œ ì„¤ì • ---
    # chromedriver_path = "/path/to/your/chromedriver" # ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½
    # driver = webdriver.Chrome(executable_path=chromedriver_path, options=options)
    # ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸í•  ë•ŒëŠ” PATHì— chromedriverê°€ ìˆê±°ë‚˜, ìœ„ì— ì£¼ì„ì²˜ë¦¬ëœ ë¼ì¸ì˜ ì£¼ì„ì„ í’€ê³  ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
    # ì•„ë‹ˆë©´, webdriver_manager ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ìë™ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # from webdriver_manager.chrome import ChromeDriverManager
    # driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
    # ---------------------------------------------------

    driver = None
    try:
        
        driver = webdriver.Chrome(options=options)  # PATHì— chromedriverê°€ ìˆë‹¤ê³  ê°€ì •
        logging.info(f"ğŸŒ Seleniumìœ¼ë¡œ URL ì ‘ì† ì‹œë„: {url}")
        driver.get(url)
        time.sleep(3)  # JS ë Œë”ë§ ê¸°ë‹¤ë¦¼ (ì¶©ë¶„íˆ ê¸°ë‹¤ë ¤ì•¼ í•¨)

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        article_content = []

        article_body_container = soup.select_one('article.layout__article-main section.article-body')
        if not article_body_container:
            article_body_container = soup.select_one('section.article-body')
            
        if article_body_container:
            paragraphs = article_body_container.find_all("p")
            for i, p in enumerate(paragraphs):
                text = p.get_text(strip=True)
                if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                    article_content.append(text)
            full_text = "\n".join(article_content)
            if full_text and len(full_text) > 100:
                return full_text
            else:
                logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""
    except Exception as e:
        logging.exception(f"Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
        return ""
    finally:
        if driver:
            driver.quit()


def get_article_text(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
    ì¡°ì„ ì¼ë³´ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    parsed_url = urlparse(url)
    is_chosun = "chosun.com" in parsed_url.netloc
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))

    if is_chosun:
        selenium_text = extract_chosun_with_selenium(clean_url)
        if selenium_text and len(selenium_text) > 300:
            return selenium_text
        else:
            logging.warning(f"Seleniumìœ¼ë¡œ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±. newspaper ë° requests í´ë°± ì‹œë„.")

    try:
        article = Article(clean_url, language="ko", headers=headers)
        article.download()
        article.parse()
        newspaper_text = article.text
        
        if newspaper_text and len(newspaper_text) > 300:
            return newspaper_text
        else:
            logging.warning(f"newspaper í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•¨ ({len(newspaper_text)}ì). requests + BeautifulSoup í´ë°± ì‹œë„: {clean_url}")

    except Exception as e:
        logging.exception(f"newspaper í¬ë¡¤ë§ ì‹¤íŒ¨ ({clean_url}): {e}. requests + BeautifulSoup í´ë°± ì‹œë„.")
    
    try:
        response = requests.get(clean_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        article_content = []

        if is_chosun:
            article_body_section = soup.select_one('section.article-body')  
            if article_body_section:
                paragraphs = article_body_section.find_all('p')  
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and not any(keyword in text for keyword in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                        article_content.append(text)
                if article_content:
                    full_text = '\n'.join(article_content)
                    if len(full_text) > 300:
                        return full_text
                    else:
                        logging.warning(f"requests+BeautifulSoupë¡œ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œí–ˆìœ¼ë‚˜ ë‚´ìš©ì´ ì§§ìŒ ({len(full_text)}ì).")
                else:
                    logging.warning(f"requests+BeautifulSoupë¡œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œ ('section.article-body')ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")
            else:
                body_elements = soup.select('div.article_content, div#articleBodyContents, div#article_body, div.news_content, article.article_view, div.view_content')
                for elem in body_elements:
                    text = elem.get_text(separator='\n', strip=True)
                    if text:
                        article_content.append(text)
            
            full_text = '\n'.join(article_content)
            if len(full_text) > 300:
                return full_text
            else:
                logging.warning(f"requests+BeautifulSoupë¡œ ì¼ë°˜ ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ ({len(full_text)}ì): {clean_url}")
            
            return full_text if len(full_text) > 300 else ""

    except requests.exceptions.RequestException as e:
        logging.exception(f"HTTP ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({clean_url}): {e}")
        return ""
    except Exception as e:
        logging.exception(f"BeautifulSoup í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({clean_url}): {e}")
        return ""

def clean_news_title(title):
    """
    ë‰´ìŠ¤ ì œëª©ì—ì„œ ì–¸ë¡ ì‚¬ëª…, ìŠ¬ë¡œê±´, íŠ¹ìˆ˜ íƒœê·¸, ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë“±ì„ ì œê±°í•˜ì—¬ ì •ì œí•©ë‹ˆë‹¤.
    """
    patterns_to_remove = [
        r'ëŒ€í•œë¯¼êµ­ ì˜¤í›„ë¥¼ ì—¬ëŠ” ìœ ì¼ì„ê°„ ë¬¸í™”ì¼ë³´',  # ë¬¸í™”ì¼ë³´ ìŠ¬ë¡œê±´
        r'\| ë¬¸í™”ì¼ë³´', r'ë¬¸í™”ì¼ë³´',  # ë¬¸í™”ì¼ë³´ ê´€ë ¨
        r'\| ì¤‘ì•™ì¼ë³´', r'ì¤‘ì•™ì¼ë³´',  # ì¤‘ì•™ì¼ë³´ ê´€ë ¨
        r'\| ê²½í–¥ì‹ ë¬¸', r'ê²½í–¥ì‹ ë¬¸',  # ê²½í–¥ì‹ ë¬¸ ê´€ë ¨
        r'ë¨¸ë‹ˆíˆ¬ë°ì´', r'MBN', r'ì—°í•©ë‰´ìŠ¤', r'SBS ë‰´ìŠ¤', r'MBC ë‰´ìŠ¤', r'KBS ë‰´ìŠ¤',
        r'ë™ì•„ì¼ë³´', r'ì¡°ì„ ì¼ë³´', r'í•œê²¨ë ˆ', r'êµ­ë¯¼ì¼ë³´', r'ì„œìš¸ì‹ ë¬¸', r'ì„¸ê³„ì¼ë³´',
        r'ë…¸ì»·ë‰´ìŠ¤', r'í—¤ëŸ´ë“œê²½ì œ', r'ë§¤ì¼ê²½ì œ', r'í•œêµ­ê²½ì œ', r'ì•„ì‹œì•„ê²½ì œ',
        r'YTN', r'JTBC', r'TVì¡°ì„ ', r'ì±„ë„A', r'ë°ì¼ë¦¬ì•ˆ', r'ë‰´ì‹œìŠ¤',
        r'ë‰´ìŠ¤1', r'ì—°í•©ë‰´ìŠ¤TV', r'ë‰´ìŠ¤í•Œ', r'ì´ë°ì¼ë¦¬', r'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
        r'ì•„ì£¼ê²½ì œ', r'UPIë‰´ìŠ¤', r'ZUM ë‰´ìŠ¤', r'ë„¤ì´íŠ¸ ë‰´ìŠ¤', r'ë‹¤ìŒ ë‰´ìŠ¤',
    ]

    patterns_to_remove_regex = [
        r'\[.*?\]',  # ì˜ˆ: [ë‹¨ë…], [ì†ë³´], [ì¢…í•©], [ì‚¬ì§„], [ì˜ìƒ], [íŒ©íŠ¸ì²´í¬]
        r'\(.*?\)',  # ì˜ˆ: (ì„œìš¸), (ì¢…í•©), (ì˜ìƒ)
        r'\{.*?\}',  # ì˜ˆ: {ë‰´ìŠ¤ì´ˆì }
        r'<[^>]+>',  # HTML íƒœê·¸ ì”ì—¬ë¬¼
        r'\[\s*\w+\s*\]',  # ê³µë°± í¬í•¨ ëŒ€ê´„í˜¸ íƒœê·¸
    ]

    symbols_to_remove = [
        r'\|', r'\:', r'\_', r'\-', r'\+', r'=', r'/', r'\\'  # |, :, _, -, +, =, /, \ ë“±
    ]

    cleaned_title = title
    
    for pattern in patterns_to_remove:
        cleaned_title = re.sub(re.escape(pattern), '', cleaned_title, flags=re.IGNORECASE).strip()
    
    for pattern_regex in patterns_to_remove_regex + symbols_to_remove:
        cleaned_title = re.sub(pattern_regex, '', cleaned_title).strip()
    
    cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
    
    return cleaned_title

async def search_news_google_cs(query):
    logging.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    """Google Custom Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")

    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": google_api_key,
        "cx": google_cse_id,
        "q": query,
        "num": 10,
        "hl": "ko",
        "gl": "kr"
    }
    async with aiohttp.ClientSession() as session:
        async with session.get(url, params=params) as resp:
            data = await resp.json()
            if "error" in data:
                logging.error(f"Google CSE API ì˜¤ë¥˜: {data['error']['message']}")
                if "quotaExceeded" in data['error']['message']:
                    logging.warning("ê²½ê³ : Google CSE API í• ë‹¹ëŸ‰(Quota)ì´ ì´ˆê³¼ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Google Cloud Consoleì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return []
            return data.get("items", [])

def calculate_fact_check_confidence(criteria_scores):
    """
    íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ 0%ì—ì„œ 100%ê¹Œì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if not criteria_scores:
        return 0

    total_possible_score = 0
    total_actual_score = 0

    for criterion, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: '{criterion}'ì˜ ì ìˆ˜ '{score}'ê°€ ìœ íš¨í•œ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        total_possible_score += 5
        total_actual_score += score

    if total_actual_score == 0 and total_possible_score > 0:
        return 0

    if total_possible_score == 0:
        return 0

    confidence_percentage = (total_actual_score / total_possible_score) * 100

    return max(0, min(100, round(confidence_percentage)))

def calculate_source_diversity_score(evidence):
    """
    ì œê³µëœ ê·¼ê±°(evidence)ì˜ ì¶œì²˜ ë‹¤ì–‘ì„±ì„ ê³„ì‚°í•˜ì—¬ 0-5ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not evidence:
        return 0
    
    unique_sources = set()
    for item in evidence:
        # source_titleì´ ìˆë‹¤ë©´ ì´ë¥¼ ì‚¬ìš© (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬)
        if item.get("source_title"):
            unique_sources.add(item["source_title"].lower())
        # source_titleì´ ì—†ìœ¼ë©´ URLì˜ ë„ë©”ì¸ì„ ì‚¬ìš©
        elif item.get("url"):
            try:
                from urllib.parse import urlparse
                domain = urlparse(item["url"]).netloc
                if domain:
                    unique_sources.add(domain.lower())
            except Exception:
                # URL íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë¬´ì‹œ
                pass

    num_unique_sources = len(unique_sources)

    if num_unique_sources >= 4:
        return 5
    elif num_unique_sources == 3:
        return 4
    elif num_unique_sources == 2:
        return 3
    elif num_unique_sources == 1:
        return 1  # ë‹¨ì¼ ì¶œì²˜ë¼ë„ ë‹¤ì–‘ì„± ì ìˆ˜ëŠ” ë‚®ê²Œ ì±…ì •
    else:
        return 0  # ì¶œì²˜ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°