import os
import re
import time
import json
import asyncio
import logging
import hashlib
from urllib.parse import urlparse, urlunparse

import aiohttp
import requests
from bs4 import BeautifulSoup
from newspaper import Article

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from openai import OpenAI
import yt_dlp


logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# -----------------------------
# Utilities
# -----------------------------
def _clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'(\n){3,}', '\n\n', text)
    text = re.sub(r'Copyright\s*.*ë¬´ë‹¨ì „ì¬.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Â©\s*.*All rights reserved.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'ì €ì‘ê¶Œì\s*.*ë¬´ë‹¨ë³µì œ.*', '', text, flags=re.IGNORECASE)
    return text.strip()


def extract_video_id(url: str):
    try:
        m = re.search(r"(?:v=|/|youtu\.be/|shorts/|embed/)([0-9A-Za-z_-]{11})", url)
        if m:
            vid = m.group(1)
            logging.info(f"[ë””ë²„ê¹…] URLì—ì„œ ì¶”ì¶œëœ video_id: {vid}")
            return vid
    except Exception as e:
        logging.error(f"ìœ íŠœë¸Œ video_id ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


# -----------------------------
# Title cleaner (ì™„í™” + ì„¸ì´í”„ê°€ë“œ)
# -----------------------------
_MEDIA = (
    r"(ë¬¸í™”ì¼ë³´|ì¤‘ì•™ì¼ë³´|ê²½í–¥ì‹ ë¬¸|ë¨¸ë‹ˆíˆ¬ë°ì´|MBN|ì—°í•©ë‰´ìŠ¤|SBS ë‰´ìŠ¤|MBC ë‰´ìŠ¤|KBS ë‰´ìŠ¤|ë™ì•„ì¼ë³´|"
    r"ì¡°ì„ ì¼ë³´|í•œê²¨ë ˆ|êµ­ë¯¼ì¼ë³´|ì„œìš¸ì‹ ë¬¸|ì„¸ê³„ì¼ë³´|ë…¸ì»·ë‰´ìŠ¤|í—¤ëŸ´ë“œê²½ì œ|ë§¤ì¼ê²½ì œ|í•œêµ­ê²½ì œ|ì•„ì‹œì•„ê²½ì œ|"
    r"YTN|JTBC|TVì¡°ì„ |ì±„ë„A|ë°ì¼ë¦¬ì•ˆ|ë‰´ì‹œìŠ¤|ë‰´ìŠ¤1|ì—°í•©ë‰´ìŠ¤TV|ë‰´ìŠ¤í•Œ|ì´ë°ì¼ë¦¬|íŒŒì´ë‚¸ì…œë‰´ìŠ¤|"
    r"ì•„ì£¼ê²½ì œ|UPIë‰´ìŠ¤|ZUM ë‰´ìŠ¤|ë„¤ì´íŠ¸ ë‰´ìŠ¤|ë‹¤ìŒ ë‰´ìŠ¤)"
)

def clean_news_title(title: str) -> str:
    if not title:
        return ""
    raw = title

    # HTML íƒœê·¸ ì œê±°
    t = re.sub(r"<[^>]+>", " ", raw)

    # ì‹œì‘ë¶€ì˜ ì§§ì€ ëŒ€ê´„í˜¸ íƒœê·¸ ì œê±° (ì˜ˆ: [ë‹¨ë…], [ì†ë³´])
    t = re.sub(r"^\s*\[[^\]]{1,12}\]\s*", "", t)

    # ì–‘ëì˜ ì–¸ë¡ ì‚¬ í‘œê¸° ì œê±° (| ë˜ëŠ” - ë¡œ êµ¬ë¶„ëœ ê²½ìš°)
    t = re.sub(rf"^\s*{_MEDIA}\s*[\|\-]\s*", "", t)
    t = re.sub(rf"\s*[\|\-]\s*{_MEDIA}\s*$", "", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip()

    # ì„¸ì´í”„ê°€ë“œ: ë„ˆë¬´ ì§§ì•„ì§€ë©´ ì›ì œëª© ìœ ì§€
    if len(t) < 2:
        return raw.strip()
    return t


# -----------------------------
# CSE ê²€ìƒ‰ (ë‹¤ìš´ì‹œí”„íŠ¸ í¬í•¨)
# -----------------------------
async def search_news_naver_api(query: str):
    logging.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    naver_client_id = os.getenv("NAVER_CLIENT_ID")
    naver_client_secret = os.getenv("NAVER_CLIENT_SECRET")

    if not naver_client_id or not naver_client_secret:
        logging.error("ë„¤ì´ë²„ API í‚¤/ì‹œí¬ë¦¿ ëˆ„ë½")
        return []

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": naver_client_id,
        "X-Naver-Client-Secret": naver_client_secret,
        "Accept": "application/json"
    }
    params = {
        "query": query,
        "display": 10,  # ìµœëŒ€ 10ê°œ ê²°ê³¼
        "sort": "sim"  # sim (ìœ ì‚¬ë„ìˆœ) ë˜ëŠ” date (ë‚ ì§œìˆœ)
    }

    async with aiohttp.ClientSession(headers=headers) as session:
        try:
            async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=15)) as resp:
                resp.raise_for_status()
                data = await resp.json()
                items = []
                for item in data.get("items", []):
                    items.append({
                        "title": item.get("title", "").replace("<b>", "").replace("</b>", ""),
                        "link": item.get("link", ""),
                        "snippet": item.get("description", "").replace("<b>", "").replace("</b>", "")
                    })
                return items
        except aiohttp.ClientError as e:
            logging.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        except Exception as e:
            logging.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []


# -----------------------------
# Article extraction (ì–¸ë¡ ì‚¬ ì„ íƒì + Selenium)
# -----------------------------
def _extract_article_content_with_selectors(html_content: str, url: str) -> str:
    # ê²½í–¥ì‹ ë¬¸ êµ¬í˜• URL â†’ ì‹ í˜• ì „í™˜
    if "news.khan.co.kr/kh_news/khan_art_view.html" in url:
        m = re.search(r'artid=(\d+)', url)
        if m:
            art_id = m.group(1)
            url = f"https://www.khan.co.kr/article/{art_id}"

    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    selectors = {
        "hani.co.kr": "div.article-text p.text",
        "khan.co.kr": "#articleBody p, div#articleBody p, #articleBody p.content_text",
        "segye.com": "article.viewBox2",
        "hankookilbo.com": "div.col-main p.read",
        "asiatoday.co.kr": "div.news_bm",
        "seoul.co.kr": "div.viewContent",
        "donga.com": "section.news_view",
        "naeil.com": "div.article-view p",
    }

    selector = None
    for dom, sel in selectors.items():
        if dom in domain:
            selector = sel
            break
    if not selector:
        return ""

    soup = BeautifulSoup(html_content, 'html.parser')
    article_elements = []

    if any(d in domain for d in ["hani.co.kr", "khan.co.kr", "hankookilbo.com", "naeil.com"]):
        elements = soup.select(selector)
        for p_tag in elements:
            for br in p_tag.find_all('br'):
                br.replace_with('\n')
            text = p_tag.get_text(separator=' ').strip()
            if text:
                article_elements.append(text)
    elif any(d in domain for d in ["segye.com", "asiatoday.co.kr", "seoul.co.kr", "donga.com"]):
        main_content_div = soup.select_one(selector)
        if main_content_div:
            for tag in main_content_div.find_all([
                'script', 'style', 'img', 'table', 'figure', 'figcaption', 'aside', 'nav', 'footer', 'header',
                'iframe', 'video', 'audio', 'meta', 'link', 'form', 'input', 'button', 'select', 'textarea', 'svg',
                'canvas', 'map', 'area', 'object', 'param', 'embed', 'source', 'track', 'picture', 'portal', 'slot',
                'template', 'noscript', 'ins', 'del', 'bdo', 'bdi', 'rp', 'rt', 'rtc', 'ruby', 'data', 'time', 'mark',
                'small', 'sub', 'sup', 'abbr', 'acronym', 'address', 'b', 'big', 'blockquote', 'center', 'cite', 'code',
                'dd', 'dfn', 'dir', 'dl', 'dt', 'em', 'font', 'i', 'kbd', 'li', 'menu', 'ol', 'pre', 'q', 's', 'samp',
                'strike', 'strong', 'tt', 'u', 'var', 'ul', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'
            ]):
                tag.decompose()

            for br in main_content_div.find_all('br'):
                br.replace_with('\n')

            paragraphs = []
            for content in main_content_div.contents:
                if getattr(content, "name", None) == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())

            article_elements = paragraphs

    full_text = '\n\n'.join(filter(None, article_elements))
    return full_text


def extract_chosun_with_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = None
    try:
        logging.info(f"ğŸ“° Seleniumìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        article_selector = "article#article-view-content-div, article.layout__article-main section.article-body"
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, article_selector)))

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        article_content = []
        container = soup.select_one('article.layout__article-main section.article-body') or \
                    soup.select_one('article#article-view-content-div')

        if container:
            paragraphs = container.find_all("p")
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                    article_content.append(text)
            full_text = '\n'.join(article_content)

            if full_text and len(full_text) > 100:
                logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()


def _extract_generic_with_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = None
    try:
        logging.info(f"ğŸ“° Selenium (Generic)ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        generic_article_selectors = [
            "div.article_content", "div#articleBodyContents", "div#article_body",
            "div.news_content", "article.article_view", "div.view_content",
            "div.article-text", "div.article-body", "div.entry-content",
            "div.contents_area", "div.news_view", "div.viewContent",
            "article.viewBox2", "div.col-main", "div.news_bm", "section.news_view",
            "div.article-view"
        ]

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ", ".join(generic_article_selectors))))
        except TimeoutException:
            logging.warning("âš ï¸ Selenium (Generic): ë³¸ë¬¸ ìš”ì†Œê°€ 10ì´ˆ ë‚´ì— ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ í˜ì´ì§€ ì†ŒìŠ¤ ì‚¬ìš©.")

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        container = None
        for selector in generic_article_selectors:
            container = soup.select_one(selector)
            if container:
                break

        if container:
            for tag in container.find_all([
                'script', 'style', 'img', 'table', 'figure', 'figcaption', 'aside', 'nav', 'footer', 'header',
                'iframe', 'video', 'audio', 'meta', 'link', 'form', 'input', 'button', 'select', 'textarea', 'svg',
                'canvas', 'map', 'area', 'object', 'param', 'embed', 'source', 'track', 'picture', 'portal', 'slot',
                'template', 'noscript', 'ins', 'del', 'bdo', 'bdi', 'rp', 'rt', 'rtc', 'ruby', 'data', 'time', 'mark',
                'small', 'sub', 'sup', 'abbr', 'acronym', 'address', 'b', 'big', 'blockquote', 'center', 'cite', 'code',
                'dd', 'dfn', 'dir', 'dl', 'dt', 'em', 'font', 'i', 'kbd', 'li', 'menu', 'ol', 'pre', 'q', 's', 'samp',
                'strike', 'strong', 'tt', 'u', 'var', 'ul', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'
            ]):
                tag.decompose()

            for br in container.find_all('br'):
                br.replace_with('\n')

            paragraphs = []
            for content in container.contents:
                if getattr(content, "name", None) == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())

            full_text = '\n\n'.join(filter(None, paragraphs))
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Selenium (Generic): íŠ¹ì • ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            full_text = soup.get_text(separator='\n', strip=True)
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ë„ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium (Generic) í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium (Generic) í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()


# -----------------------------
# Async article fetch orchestrator
# -----------------------------
async def get_article_text(url: str) -> str:
    logging.info(f"ğŸ“° ë¹„ë™ê¸°ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {url}")
    parsed_url = urlparse(url)
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

    # íŠ¹ì • ì–¸ë¡ ì‚¬: Selenium ìš°ì„  (ì¡°ì„ ì€ ì „ìš©, ë‚˜ë¨¸ì§€ëŠ” Generic)
    SELENIUM_FIRST_DOMAINS = [
        "chosun.com",
        "hani.co.kr",
        "khan.co.kr",
        "segye.com",
        "hankookilbo.com",
        "asiatoday.co.kr",
        "seoul.co.kr",
        "donga.com",
        "naeil.com",
    ]
    if any(d in parsed_url.netloc for d in SELENIUM_FIRST_DOMAINS):
        try:
            if "chosun.com" in parsed_url.netloc:
                logging.info("â­ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ê°ì§€. Selenium(ì „ìš©) í¬ë¡¤ë§ì„ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.")
                text = await asyncio.to_thread(extract_chosun_with_selenium, url)
            else:
                logging.info("â­ íŠ¹ì • ì–¸ë¡ ì‚¬ ê¸°ì‚¬ ê°ì§€. Selenium(Generic) í¬ë¡¤ë§ì„ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.")
                text = await asyncio.to_thread(_extract_generic_with_selenium, url)
            if text and len(text) > 100:
                return _clean_text(text)
            else:
                logging.warning("âš ï¸ Selenium ìš°ì„  í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤. ì–¸ë¡ ì‚¬ ì…€ë ‰í„° í´ë°± ì‹œë„.")
        except Exception as e:
            logging.error(f"âŒ asyncio.to_thread Selenium ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            logging.info("â¡ï¸ ì–¸ë¡ ì‚¬ ì…€ë ‰í„° í´ë°± ì‹œë„")

        # Fallback: requests + BeautifulSoup (ì–¸ë¡ ì‚¬ë³„ ì„ íƒì)ë¡œ ì¬ì‹œë„
        try:
            response = requests.get(clean_url, headers=headers, timeout=15)
            response.raise_for_status()
            html_content = response.text

            extracted = _extract_article_content_with_selectors(html_content, url)
            if extracted and len(extracted) > 100:
                cleaned_final_text = _clean_text(extracted)
                logging.info(f"âœ… ì–¸ë¡ ì‚¬ ì…€ë ‰í„°ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(cleaned_final_text)}ì): {url}")
                return cleaned_final_text
            else:
                logging.warning("âš ï¸ ì–¸ë¡ ì‚¬ ì…€ë ‰í„° í´ë°±ë„ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                return ""
        except Exception as e:
            logging.warning(f"âš ï¸ ì–¸ë¡ ì‚¬ ì…€ë ‰í„° í´ë°± ì‹¤íŒ¨: {url} -> {e}")
            return ""

    # aiohttp + newspaper
    try:
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(clean_url, timeout=30) as response:
                response.raise_for_status()
                html_content = await response.text()

                article = Article(clean_url, language='ko')
                article.download(input_html=html_content)
                article.parse()

                if article.text and len(article.text) > 300:
                    logging.info(f"âœ… newspaperë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(article.text)}ì): {url}")
                    return _clean_text(article.text)
                else:
                    logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•¨. í´ë°± ì—†ì´ ê±´ë„ˆëœ€: {url}")
                    return ""
    except aiohttp.ClientError as e:
        logging.warning(f"âš ï¸ aiohttp í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ ë°œìƒ. í´ë°± ì—†ì´ ê±´ë„ˆëœ€: {url} -> {e}")
        return ""
    except Exception as e:
        logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ì‹¤íŒ¨. í´ë°± ì—†ì´ ê±´ë„ˆëœ€: {url} -> {e}")
        return ""

    # chosun ì „ìš©ì´ ì•„ë‹Œ ë„ë©”ì¸ì€ newspaper ì‹¤íŒ¨ ì‹œ í´ë°± ì—†ì´ ì¤‘ë‹¨
    return ""


# -----------------------------
# YouTube transcript (yt-dlp + Whisper)
# -----------------------------
def fetch_youtube_transcript(video_url: str) -> str:
    vid = extract_video_id(video_url)
    logging.info(f"[ë””ë²„ê¹…] ì¶”ì¶œëœ video_id: {vid}")
    if not vid:
        logging.error("ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return ""

    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        logging.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return ""

    cookies_path = "/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt"
    outtmpl = f"{vid}.%(ext)s"
    downloaded_paths: list[str] = []

    try:
        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': outtmpl,
            'cookiefile': cookies_path,
            'quiet': True,
        }

        logging.info(f"ğŸ¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            sinfo = ydl.sanitize_info(info)
            if 'requested_downloads' in sinfo:
                downloaded_paths = [d['filepath'] for d in sinfo['requested_downloads']]
            elif '_filename' in sinfo:
                downloaded_paths.append(sinfo['_filename'])

            if not downloaded_paths:
                raise RuntimeError("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

            audio_file = downloaded_paths[0]

        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {audio_file}")

        with open(audio_file, "rb") as f:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language="ko"
            )
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
        return transcript.text or ""

    except Exception as e:
        logging.exception(f"yt-dlp ë˜ëŠ” Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        for p in downloaded_paths:
            try:
                if os.path.exists(p):
                    os.remove(p)
                    logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {p}")
            except Exception:
                pass


# -----------------------------
# Scoring helpers
# -----------------------------
def calculate_fact_check_confidence(criteria_scores: dict) -> int:
    if not criteria_scores:
        return 0
    total_possible = 0
    total_actual = 0
    for _, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: ì ìˆ˜ '{score}'ê°€ ìœ íš¨ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¨")
            return 0
        total_possible += 5
        total_actual += score
    if total_possible == 0:
        return 0
    pct = (total_actual / total_possible) * 100
    return max(0, min(100, round(pct)))


def calculate_source_diversity_score(evidence: list[dict]) -> int:
    if not evidence:
        return 0
    unique = set()
    for item in evidence:
        st = item.get("source_title")
        if st:
            unique.add(st.lower())
            continue
        url = item.get("url")
        if url:
            try:
                dom = urlparse(url).netloc
                if dom:
                    unique.add(dom.lower())
            except Exception:
                pass
    n = len(unique)
    if n >= 4:
        return 5
    if n == 3:
        return 4
    if n == 2:
        return 3
    if n == 1:
        return 1
    return 0


# -----------------------------
# JSON ì¦ê±° ë³¸ë¬¸ ì „ì²˜ë¦¬
# -----------------------------
def clean_evidence_content(content: str) -> str:
    """
    JSON ì¦ê±° ë³¸ë¬¸ì—ì„œ ê¸°ìëª…, copyright, HTML íƒœê·¸ ë“±ì„ ì œê±°í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        content: ì›ë³¸ ì¦ê±° ë³¸ë¬¸
        
    Returns:
        ì „ì²˜ë¦¬ëœ ì¦ê±° ë³¸ë¬¸
    """
    if not content:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    content = re.sub(r'<[^>]+>', '', content)
    
    # ê¸°ìëª… íŒ¨í„´ ì œê±° (ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›)
    reporter_patterns = [
        r'[ê°€-í£]+\s*ê¸°ì',  # í•œê¸€ ê¸°ìëª…
        r'[A-Za-z]+\s*ê¸°ì',  # ì˜ë¬¸ ê¸°ìëª…
        r'ê¸°ì\s*[ê°€-í£]+',  # ê¸°ì + í•œê¸€ëª…
        r'ê¸°ì\s*[A-Za-z]+',  # ê¸°ì + ì˜ë¬¸ëª…
        r'[ê°€-í£]+\s*[A-Za-z]+\s*ê¸°ì',  # í•œê¸€+ì˜ë¬¸ ê¸°ìëª…
        r'[A-Za-z]+\s*[ê°€-í£]+\s*ê¸°ì',  # ì˜ë¬¸+í•œê¸€ ê¸°ìëª…
        r'ê¸°ì\s*[ê°€-í£]+\s*[A-Za-z]+',  # ê¸°ì + í•œê¸€+ì˜ë¬¸ëª…
        r'ê¸°ì\s*[A-Za-z]+\s*[ê°€-í£]+',  # ê¸°ì + ì˜ë¬¸+í•œê¸€ëª…
    ]
    
    for pattern in reporter_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # Copyright ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
    copyright_patterns = [
        r'Copyright\s*Â©?\s*\d{4}\s*[ê°€-í£A-Za-z\s]+',
        r'Â©\s*\d{4}\s*[ê°€-í£A-Za-z\s]+',
        r'ì €ì‘ê¶Œ\s*Â©?\s*\d{4}\s*[ê°€-í£A-Za-z\s]+',
        r'ë¬´ë‹¨ì „ì¬\s*ë°\s*ì¬ë°°í¬\s*ê¸ˆì§€',
        r'ë¬´ë‹¨ë³µì œ\s*ê¸ˆì§€',
        r'All\s+rights\s+reserved',
        r'ì €ì‘ê¶Œì\s*[ê°€-í£A-Za-z\s]+',
        r'ë³¸ì‚¬\s*[ê°€-í£A-Za-z\s]+',
        r'ì‹ ë¬¸ì‚¬\s*[ê°€-í£A-Za-z\s]+',
        r'ë‰´ìŠ¤ì‚¬\s*[ê°€-í£A-Za-z\s]+',
    ]
    
    for pattern in copyright_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # ì–¸ë¡ ì‚¬ ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
    media_patterns = [
        r'\[[ê°€-í£A-Za-z\s]+\]',  # ëŒ€ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì–¸ë¡ ì‚¬ëª…
        r'\([ê°€-í£A-Za-z\s]+\)',  # ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì–¸ë¡ ì‚¬ëª…
        r'[ê°€-í£A-Za-z\s]+ë‰´ìŠ¤',  # ~ë‰´ìŠ¤ íŒ¨í„´
        r'[ê°€-í£A-Za-z\s]+ì‹ ë¬¸',  # ~ì‹ ë¬¸ íŒ¨í„´
        r'[ê°€-í£A-Za-z\s]+ì¼ë³´',  # ~ì¼ë³´ íŒ¨í„´
        r'[ê°€-í£A-Za-z\s]+ê²½ì œ',  # ~ê²½ì œ íŒ¨í„´
    ]
    
    for pattern in media_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # ë‚ ì§œ/ì‹œê°„ ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±°
    date_patterns = [
        r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼',
        r'\d{4}-\d{1,2}-\d{1,2}',
        r'\d{1,2}:\d{2}',  # ì‹œê°„
        r'ì˜¤ì „\s*\d{1,2}:\d{2}',
        r'ì˜¤í›„\s*\d{1,2}:\d{2}',
    ]
    
    for pattern in date_patterns:
        content = re.sub(pattern, '', content)
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬ (ë¬¸ì¥ êµ¬ì¡° ë³´ì¡´)
    content = re.sub(r'[ \t]+', ' ', content)  # íƒ­ê³¼ ì—°ì† ê³µë°±ë§Œ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
    content = re.sub(r'\n\s*\n', '\n', content)  # ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
    content = content.strip()
    
    # ë„ˆë¬´ ì§§ì•„ì§„ ê²½ìš° ì›ë³¸ ë°˜í™˜
    if len(content) < 50:
        return content
    
    return content


def clean_evidence_json(evidence_list: list[dict]) -> list[dict]:
    """
    ì¦ê±° ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì—ì„œ ë³¸ë¬¸ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        evidence_list: ì¦ê±° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì „ì²˜ë¦¬ëœ ì¦ê±° ë¦¬ìŠ¤íŠ¸
    """
    if not evidence_list:
        return []
    
    cleaned_evidence = []
    for evidence in evidence_list:
        cleaned_evidence_item = evidence.copy()
        
        # snippet í•„ë“œ ì „ì²˜ë¦¬
        if 'snippet' in cleaned_evidence_item:
            cleaned_evidence_item['snippet'] = clean_evidence_content(
                cleaned_evidence_item['snippet']
            )
        
        # justification í•„ë“œ ì „ì²˜ë¦¬
        if 'justification' in cleaned_evidence_item:
            cleaned_evidence_item['justification'] = clean_evidence_content(
                cleaned_evidence_item['justification']
            )
        
        cleaned_evidence.append(cleaned_evidence_item)
    
    return cleaned_evidence
