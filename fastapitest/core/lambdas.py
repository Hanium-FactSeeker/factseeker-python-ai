import re
import asyncio
import aiohttp
import hashlib
import os
import requests
from urllib.parse import urlparse, urlunparse, parse_qs
from bs4 import BeautifulSoup
from newspaper import Article
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service # Service ê°ì²´ ì¶”ê°€
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import logging

# Whisper ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from openai import OpenAI
import yt_dlp

# Logging setup
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# EC2ì—ì„œ ì„¤ì¹˜ëœ chromedriver ê²½ë¡œ (ì¼ë°˜ì ì¸ ì„¤ì¹˜ ê²½ë¡œ)
CHROMEDRIVER_PATH = "/usr/local/bin/chromedriver"


def extract_video_id(url: str):
    """
    ìœ íŠœë¸Œ URLì—ì„œ 11ìë¦¬ video_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ í˜•íƒœì˜ URLì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì •ê·œì‹ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
    """
    try:
        # ëª¨ë“  ê°€ëŠ¥í•œ YouTube URL íŒ¨í„´ì„ ì²˜ë¦¬í•˜ëŠ” ì •ê·œì‹
        match = re.search(
            r"(?:v=|/|youtu\.be/|shorts/|embed/)([0-9A-Za-z_-]{11})", url
        )
        if match:
            video_id = match.group(1)
            logging.info(f"[ë””ë²„ê¹…] URLì—ì„œ ì¶”ì¶œëœ video_id: {video_id}")
            return video_id
    except Exception as e:
        logging.error(f"ìœ íŠœë¸Œ video_id ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def fetch_youtube_transcript(video_url):
    """
    EC2 ë‚´ë¶€ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ í›„ Whisperë¡œ ìë§‰ ì¶”ì¶œ
    - openai ë¼ì´ë¸ŒëŸ¬ë¦¬ v1.0.0+ì— ë§ì¶° API í˜¸ì¶œ ë°©ì‹ ìˆ˜ì •
    - yt-dlp ë‹¤ìš´ë¡œë“œ íŒŒì¼ì— í™•ì¥ìë¥¼ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
    """
    video_id = extract_video_id(video_url)
    logging.info(f"[ë””ë²„ê¹…] ì¶”ì¶œëœ video_id: {video_id}")
    if not video_id:
        logging.error("ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return ""
    
    # OpenAI v1.0.0+ API ì‚¬ìš©
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        logging.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return ""

    cookies_path = "/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt"
    # yt-dlpê°€ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ëª…ì„ ì§€ì •í•©ë‹ˆë‹¤. íŒŒì¼ í™•ì¥ìë¥¼ í¬í•¨í•˜ë„ë¡ í…œí”Œë¦¿ ìˆ˜ì •.
    temp_audio_file_template = f"{video_id}.%(ext)s"
    downloaded_file_paths = []

    try:
        ydl_opts = {
            'format': 'bestaudio/best', # ìµœì ì˜ ì˜¤ë””ì˜¤ í¬ë§·ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
            'outtmpl': temp_audio_file_template,
            'cookiefile': cookies_path,
            'quiet': True,
        }

        logging.info(f"ğŸ¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì˜ ì‹¤ì œ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            download_info = ydl.sanitize_info(info)
            if 'requested_downloads' in download_info:
                downloaded_file_paths = [d['filepath'] for d in download_info['requested_downloads']]
            elif '_filename' in download_info:
                # ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œì˜ ê²½ìš°
                downloaded_file_paths.append(download_info['_filename'])
            
            if not downloaded_file_paths:
                raise Exception("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

            actual_audio_file = downloaded_file_paths[0]

        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {actual_audio_file}")

        with open(actual_audio_file, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ko"
            )
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
        return transcript.text

    except Exception as e:
        logging.exception(f"yt-dlp ë˜ëŠ” Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        # ë‹¤ìš´ë¡œë“œëœ ëª¨ë“  ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for file_path in downloaded_file_paths:
            if os.path.exists(file_path):
                os.remove(file_path)
                logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")


def extract_chosun_with_selenium(url: str):
    """
    ë™ê¸°ì ìœ¼ë¡œ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # EC2 í™˜ê²½ì— ë§ëŠ” Chromedriver ê²½ë¡œë¥¼ Service ê°ì²´ë¡œ ì „ë‹¬
    service = Service(executable_path=CHROMEDRIVER_PATH)
    
    driver = None
    try:
        logging.info(f"ğŸ“° Seleniumìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        # Service ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ driver ìƒì„±
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        
        # ì¡°ì„ ì¼ë³´ ì£¼ê°„ì¡°ì„  ë° ì¼ë°˜ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒì
        article_selector = "article#article-view-content-div, article.layout__article-main section.article-body"
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, article_selector)))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        article_content = []
        article_body_container = soup.select_one('article.layout__article-main section.article-body')
        if not article_body_container:
            article_body_container = soup.select_one('article#article-view-content-div')
        
        if article_body_container:
            paragraphs = article_body_container.find_all("p")
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                    article_content.append(text)
            full_text = "\n".join(article_content)
            
            if full_text and len(full_text) > 100:
                logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""
            
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return None
    except Exception as e:
        logging.exception(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    finally:
        if driver:
            driver.quit()

async def get_article_text(url: str):
    """
    ë¹„ë™ê¸°ì ìœ¼ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜.
    aiohttp -> newspaper -> BeautifulSoup -> Selenium ìˆœì„œë¡œ í´ë°±ì„ ì‹œë„í•©ë‹ˆë‹¤.
    """
    logging.info(f"ğŸ“° ë¹„ë™ê¸°ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {url}")
    parsed_url = urlparse(url)
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))

    # 1. aiohttp + newspaper ì‹œë„
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(clean_url, timeout=30) as response:
                response.raise_for_status()
                html_content = await response.text()
                
                article = Article(clean_url, language='ko')
                article.download(input_html=html_content)
                article.parse()
                
                if article.text and len(article.text) > 300:
                    logging.info(f"âœ… newspaperë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(article.text)}ì): {url}")
                    return article.text
                else:
                    logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•¨. BeautifulSoup í´ë°± ì‹œë„: {url}")
    except aiohttp.ClientError as e:
        logging.warning(f"âš ï¸ aiohttp í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ ë°œìƒ. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ì‹¤íŒ¨. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")

    # 2. requests + BeautifulSoup í´ë°± ì‹œë„
    try:
        logging.warning(f"âš ï¸ requests+BeautifulSoupìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì¬ì‹œë„: {url}")
        response = requests.get(clean_url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        article_content = []
        if "chosun.com" in parsed_url.netloc:
            # ì¡°ì„ ì¼ë³´ì— ëŒ€í•œ íŠ¹ì • ì„ íƒì
            article_body_section = soup.select_one('section.article-body') or soup.select_one('article#article-view-content-div')
            if article_body_section:
                paragraphs = article_body_section.find_all('p')
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                        article_content.append(text)
        else:
            # ì¼ë°˜ì ì¸ ì–¸ë¡ ì‚¬ ë³¸ë¬¸ ì„ íƒì
            body_elements = soup.select('div.article_content, div#articleBodyContents, div#article_body, div.news_content, article.article_view, div.view_content')
            for elem in body_elements:
                text = elem.get_text(separator='\n', strip=True)
                if text:
                    article_content.append(text)

        full_text = '\n'.join([c for c in article_content if c])
        if full_text and len(full_text) > 300:
            logging.info(f"âœ… requests+BeautifulSoupìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(full_text)}ì): {url}")
            return full_text
        else:
            logging.warning(f"âš ï¸ requests+BeautifulSoupë¡œ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ. Selenium ì‹œë„: {clean_url}")
            return ""
            
    except requests.exceptions.RequestException as e:
        logging.warning(f"âš ï¸ requests+BeautifulSoup ì‹¤íŒ¨. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ BeautifulSoup íŒŒì‹± ì‹¤íŒ¨. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")

    # 3. Selenium í´ë°± ì‹œë„ (ì£¼ë¡œ ì¡°ì„ ì¼ë³´ íŠ¹ì •)
    if "chosun.com" in url:
        logging.warning("âš ï¸ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ â†’ Selenium í¬ë¡¤ë§ ì‹œë„.")
        try:
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸° ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            text = await asyncio.to_thread(extract_chosun_with_selenium, url)
            if text and len(text) > 100:
                logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return text
        except Exception as e:
            logging.error(f"âŒ asyncio.to_thread Selenium ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    logging.error(f"âŒ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}")
    return None

def clean_news_title(title):
    """
    ë‰´ìŠ¤ ì œëª©ì—ì„œ ì–¸ë¡ ì‚¬ëª…, ìŠ¬ë¡œê±´, íŠ¹ìˆ˜ íƒœê·¸, ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë“±ì„ ì œê±°í•˜ì—¬ ì •ì œí•©ë‹ˆë‹¤.
    """
    patterns_to_remove = [
        r'ëŒ€í•œë¯¼êµ­ ì˜¤í›„ë¥¼ ì—¬ëŠ” ìœ ì¼ì„ê°„ ë¬¸í™”ì¼ë³´', 
        r'\| ë¬¸í™”ì¼ë³´', r'ë¬¸í™”ì¼ë³´',
        r'\| ì¤‘ì•™ì¼ë³´', r'ì¤‘ì•™ì¼ë³´',
        r'\| ê²½í–¥ì‹ ë¬¸', r'ê²½í–¥ì‹ ë¬¸',
        r'ë¨¸ë‹ˆíˆ¬ë°ì´', r'MBN', r'ì—°í•©ë‰´ìŠ¤', r'SBS ë‰´ìŠ¤', r'MBC ë‰´ìŠ¤', r'KBS ë‰´ìŠ¤',
        r'ë™ì•„ì¼ë³´', r'ì¡°ì„ ì¼ë³´', r'í•œê²¨ë ˆ', r'êµ­ë¯¼ì¼ë³´', r'ì„œìš¸ì‹ ë¬¸', r'ì„¸ê³„ì¼ë³´',
        r'ë…¸ì»·ë‰´ìŠ¤', r'í—¤ëŸ´ë“œê²½ì œ', r'ë§¤ì¼ê²½ì œ', r'í•œêµ­ê²½ì œ', r'ì•„ì‹œì•„ê²½ì œ',
        r'YTN', r'JTBC', r'TVì¡°ì„ ', r'ì±„ë„A', r'ë°ì¼ë¦¬ì•ˆ', r'ë‰´ì‹œìŠ¤',
        r'ë‰´ìŠ¤1', r'ì—°í•©ë‰´ìŠ¤TV', r'ë‰´ìŠ¤í•Œ', r'ì´ë°ì¼ë¦¬', r'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
        r'ì•„ì£¼ê²½ì œ', r'UPIë‰´ìŠ¤', r'ZUM ë‰´ìŠ¤', r'ë„¤ì´íŠ¸ ë‰´ìŠ¤', r'ë‹¤ìŒ ë‰´ìŠ¤',
    ]

    patterns_to_remove_regex = [
        r'\[.*?\]',
        r'\(.*?\)',
        r'\{.*?\}',
        r'<[^>]+>',
        r'\[\s*\w+\s*\]',
    ]

    symbols_to_remove = [
        r'\|', r'\:', r'\_', r'\-', r'\+', r'=', r'/', r'\\'
    ]

    cleaned_title = title
    
    for pattern in patterns_to_remove:
        cleaned_title = re.sub(re.escape(pattern), '', cleaned_title, flags=re.IGNORECASE).strip()
    
    for pattern_regex in patterns_to_remove_regex + symbols_to_remove:
        cleaned_title = re.sub(pattern_regex, '', cleaned_title).strip()
    
    cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
    
    return cleaned_title

async def search_news_google_cs(query):
    logging.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    """Google Custom Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")

    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": google_api_key,
        "cx": google_cse_id,
        "q": query,
        "num": 10,
        "hl": "ko",
        "gl": "kr"
    }
    async with aiohttp.ClientSession() as session:
        async with session.get(url, params=params) as resp:
            data = await resp.json()
            if "error" in data:
                logging.error(f"Google CSE API ì˜¤ë¥˜: {data['error']['message']}")
                if "quotaExceeded" in data['error']['message']:
                    logging.warning("ê²½ê³ : Google CSE API í• ë‹¹ëŸ‰(Quota)ì´ ì´ˆê³¼ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Google Cloud Consoleì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return []
            return data.get("items", [])

def calculate_fact_check_confidence(criteria_scores):
    """
    íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ 0%ì—ì„œ 100%ê¹Œì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if not criteria_scores:
        return 0

    total_possible_score = 0
    total_actual_score = 0

    for criterion, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: '{criterion}'ì˜ ì ìˆ˜ '{score}'ê°€ ìœ íš¨í•œ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        total_possible_score += 5
        total_actual_score += score

    if total_actual_score == 0 and total_possible_score > 0:
        return 0

    if total_possible_score == 0:
        return 0

    confidence_percentage = (total_actual_score / total_possible_score) * 100

    return max(0, min(100, round(confidence_percentage)))

def calculate_source_diversity_score(evidence):
    """
    ì œê³µëœ ê·¼ê±°(evidence)ì˜ ì¶œì²˜ ë‹¤ì–‘ì„±ì„ ê³„ì‚°í•˜ì—¬ 0-5ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not evidence:
        return 0
    
    unique_sources = set()
    for item in evidence:
        # source_titleì´ ìˆë‹¤ë©´ ì´ë¥¼ ì‚¬ìš© (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬)
        if item.get("source_title"):
            unique_sources.add(item["source_title"].lower())
        # source_titleì´ ì—†ìœ¼ë©´ URLì˜ ë„ë©”ì¸ì„ ì‚¬ìš©
        elif item.get("url"):
            try:
                domain = urlparse(item["url"]).netloc
                if domain:
                    unique_sources.add(domain.lower())
            except Exception:
                # URL íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë¬´ì‹œ
                pass

    num_unique_sources = len(unique_sources)

    if num_unique_sources >= 4:
        return 5
    elif num_unique_sources == 3:
        return 4
    elif num_unique_sources == 2:
        return 3
    elif num_unique_sources == 1:
        return 1
    else:
        return 0