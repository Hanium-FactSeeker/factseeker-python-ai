import re
import asyncio
import aiohttp
import hashlib
import os
import requests
from urllib.parse import urlparse, urlunparse, parse_qs
from bs4 import BeautifulSoup
from newspaper import Article
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import logging

# Whisper ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from openai import OpenAI
import yt_dlp

# Logging setup
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


def extract_video_id(url: str):
    """
    ìœ íŠœë¸Œ URLì—ì„œ 11ìë¦¬ video_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ í˜•íƒœì˜ URLì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì •ê·œì‹ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.
    """
    try:
        # ëª¨ë“  ê°€ëŠ¥í•œ YouTube URL íŒ¨í„´ì„ ì²˜ë¦¬í•˜ëŠ” ì •ê·œì‹
        match = re.search(
            r"(?:v=|/|youtu\.be/|shorts/|embed/)([0-9A-Za-z_-]{11})", url
        )
        if match:
            video_id = match.group(1)
            logging.info(f"[ë””ë²„ê¹…] URLì—ì„œ ì¶”ì¶œëœ video_id: {video_id}")
            return video_id

    except Exception as e:
        logging.exception(f"extract_video_id ì˜¤ë¥˜: {e}")

    logging.error("âŒ video_id ì¶”ì¶œ ì‹¤íŒ¨")
    return None

def fetch_youtube_transcript(video_url):
    """
    EC2 ë‚´ë¶€ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ í›„ Whisperë¡œ ìë§‰ ì¶”ì¶œ
    - openai ë¼ì´ë¸ŒëŸ¬ë¦¬ v1.0.0+ì— ë§ì¶° API í˜¸ì¶œ ë°©ì‹ ìˆ˜ì •
    - yt-dlp ë‹¤ìš´ë¡œë“œ íŒŒì¼ì— í™•ì¥ìë¥¼ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
    """
    video_id = extract_video_id(video_url)
    logging.info(f"[ë””ë²„ê¹…] ì¶”ì¶œëœ video_id: {video_id}")
    if not video_id:
        logging.error("ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return ""
    
    # OpenAI v1.0.0+ API ì‚¬ìš©
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        logging.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return ""

    cookies_path = "/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt"
    # yt-dlpê°€ ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ëª…ì„ ì§€ì •í•©ë‹ˆë‹¤. íŒŒì¼ í™•ì¥ìë¥¼ í¬í•¨í•˜ë„ë¡ í…œí”Œë¦¿ ìˆ˜ì •.
    temp_audio_file_template = f"{video_id}.%(ext)s"
    downloaded_file_paths = []

    try:
        ydl_opts = {
            'format': 'bestaudio/best', # ìµœì ì˜ ì˜¤ë””ì˜¤ í¬ë§·ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
            'outtmpl': temp_audio_file_template,
            'cookiefile': cookies_path,
            'quiet': True,
        }

        logging.info(f"ğŸ¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì˜ ì‹¤ì œ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            download_info = ydl.sanitize_info(info)
            if 'requested_downloads' in download_info:
                downloaded_file_paths = [d['filepath'] for d in download_info['requested_downloads']]
            elif '_filename' in download_info:
                # ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œì˜ ê²½ìš°
                downloaded_file_paths.append(download_info['_filename'])
            
            if not downloaded_file_paths:
                raise Exception("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

            actual_audio_file = downloaded_file_paths[0]

        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {actual_audio_file}")

        with open(actual_audio_file, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ko"
            )
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
        return transcript.text

    except Exception as e:
        logging.exception(f"yt-dlp ë˜ëŠ” Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        # ë‹¤ìš´ë¡œë“œëœ ëª¨ë“  ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for file_path in downloaded_file_paths:
            if os.path.exists(file_path):
                os.remove(file_path)
                logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")


def extract_chosun_with_selenium(url):
    """
    Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ì— ë§ê²Œ ChromeDriver ê²½ë¡œ ì„¤ì • í•„ìš”)
    """
    options = Options()
    options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì•ˆ ë„ìš°ê³  ì‹¤í–‰
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")  # GPU ì‚¬ìš© ë¹„í™œì„±í™” (ì¼ë¶€ í™˜ê²½ì—ì„œ í•„ìš”)
    options.add_argument("--window-size=1920,1080")  # ì°½ í¬ê¸° ì„¤ì •
    
    driver = None
    try:
        driver = webdriver.Chrome(options=options)  # PATHì— chromedriverê°€ ìˆë‹¤ê³  ê°€ì •
        logging.info(f"ğŸŒ Seleniumìœ¼ë¡œ URL ì ‘ì† ì‹œë„: {url}")
        driver.get(url)
        time.sleep(3)  # JS ë Œë”ë§ ê¸°ë‹¤ë¦¼ (ì¶©ë¶„íˆ ê¸°ë‹¤ë ¤ì•¼ í•¨)

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        article_content = []

        article_body_container = soup.select_one('article.layout__article-main section.article-body')
        if not article_body_container:
            article_body_container = soup.select_one('section.article-body')
            
        if article_body_container:
            paragraphs = article_body_container.find_all("p")
            for i, p in enumerate(paragraphs):
                text = p.get_text(strip=True)
                if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                    article_content.append(text)
            full_text = "\n".join(article_content)
            if full_text and len(full_text) > 100:
                return full_text
            else:
                logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""
    except Exception as e:
        logging.exception(f"Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
        return ""
    finally:
        if driver:
            driver.quit()


def get_article_text(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
    ì¡°ì„ ì¼ë³´ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    parsed_url = urlparse(url)
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))

    # 1. ì¡°ì„ ì¼ë³´ì— ëŒ€í•œ Selenium íŠ¹ë³„ ì²˜ë¦¬
    if "chosun.com" in parsed_url.netloc:
        selenium_text = extract_chosun_with_selenium(clean_url)
        if selenium_text and len(selenium_text) > 300:
            return selenium_text
        else:
            logging.warning(f"Seleniumìœ¼ë¡œ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±. newspaper ë° requests í´ë°± ì‹œë„.")

    # 2. newspaper ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì¼ë°˜ì ì¸ í¬ë¡¤ë§ ì‹œë„
    try:
        article = Article(clean_url, language="ko", headers=headers)
        article.download()
        article.parse()
        newspaper_text = article.text
        
        if newspaper_text and len(newspaper_text) > 300:
            return newspaper_text
        else:
            logging.warning(f"newspaper í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•¨ ({len(newspaper_text)}ì). requests + BeautifulSoup í´ë°± ì‹œë„: {clean_url}")

    except Exception as e:
        logging.exception(f"newspaper í¬ë¡¤ë§ ì‹¤íŒ¨ ({clean_url}): {e}. requests + BeautifulSoup í´ë°± ì‹œë„.")
    
    # 3. requestsì™€ BeautifulSoupë¥¼ ì‚¬ìš©í•œ ë§ˆì§€ë§‰ í´ë°±
    try:
        response = requests.get(clean_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        article_content = []

        # ì¡°ì„ ì¼ë³´ì´ê±°ë‚˜ ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ë¼ë„ ë³¸ë¬¸ ì„ íƒì ë¡œì§ ì¬ì‹œë„
        if "chosun.com" in parsed_url.netloc:
            article_body_section = soup.select_one('section.article-body')  
            if article_body_section:
                paragraphs = article_body_section.find_all('p')  
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and not any(keyword in text for keyword in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                        article_content.append(text)
                
        # ì¼ë°˜ì ì¸ ì–¸ë¡ ì‚¬ ë³¸ë¬¸ ì„ íƒì
        else:
            body_elements = soup.select('div.article_content, div#articleBodyContents, div#article_body, div.news_content, article.article_view, div.view_content')
            for elem in body_elements:
                text = elem.get_text(separator='\n', strip=True)
                if text:
                    article_content.append(text)
        
        full_text = '\n'.join([c for c in article_content if c]) # None ê°’ ì œê±°
        if len(full_text) > 300:
            return full_text
        else:
            logging.warning(f"requests+BeautifulSoupë¡œ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ ({len(full_text)}ì): {clean_url}")
            return ""

    except requests.exceptions.RequestException as e:
        logging.exception(f"HTTP ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({clean_url}): {e}")
        return ""
    except Exception as e:
        logging.exception(f"BeautifulSoup í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({clean_url}): {e}")
        return ""


def clean_news_title(title):
    """
    ë‰´ìŠ¤ ì œëª©ì—ì„œ ì–¸ë¡ ì‚¬ëª…, ìŠ¬ë¡œê±´, íŠ¹ìˆ˜ íƒœê·¸, ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ë“±ì„ ì œê±°í•˜ì—¬ ì •ì œí•©ë‹ˆë‹¤.
    """
    patterns_to_remove = [
        r'ëŒ€í•œë¯¼êµ­ ì˜¤í›„ë¥¼ ì—¬ëŠ” ìœ ì¼ì„ê°„ ë¬¸í™”ì¼ë³´',  # ë¬¸í™”ì¼ë³´ ìŠ¬ë¡œê±´
        r'\| ë¬¸í™”ì¼ë³´', r'ë¬¸í™”ì¼ë³´',  # ë¬¸í™”ì¼ë³´ ê´€ë ¨
        r'\| ì¤‘ì•™ì¼ë³´', r'ì¤‘ì•™ì¼ë³´',  # ì¤‘ì•™ì¼ë³´ ê´€ë ¨
        r'\| ê²½í–¥ì‹ ë¬¸', r'ê²½í–¥ì‹ ë¬¸',  # ê²½í–¥ì‹ ë¬¸ ê´€ë ¨
        r'ë¨¸ë‹ˆíˆ¬ë°ì´', r'MBN', r'ì—°í•©ë‰´ìŠ¤', r'SBS ë‰´ìŠ¤', r'MBC ë‰´ìŠ¤', r'KBS ë‰´ìŠ¤',
        r'ë™ì•„ì¼ë³´', r'ì¡°ì„ ì¼ë³´', r'í•œê²¨ë ˆ', r'êµ­ë¯¼ì¼ë³´', r'ì„œìš¸ì‹ ë¬¸', r'ì„¸ê³„ì¼ë³´',
        r'ë…¸ì»·ë‰´ìŠ¤', r'í—¤ëŸ´ë“œê²½ì œ', r'ë§¤ì¼ê²½ì œ', r'í•œêµ­ê²½ì œ', r'ì•„ì‹œì•„ê²½ì œ',
        r'YTN', r'JTBC', r'TVì¡°ì„ ', r'ì±„ë„A', r'ë°ì¼ë¦¬ì•ˆ', r'ë‰´ì‹œìŠ¤',
        r'ë‰´ìŠ¤1', r'ì—°í•©ë‰´ìŠ¤TV', r'ë‰´ìŠ¤í•Œ', r'ì´ë°ì¼ë¦¬', r'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
        r'ì•„ì£¼ê²½ì œ', r'UPIë‰´ìŠ¤', r'ZUM ë‰´ìŠ¤', r'ë„¤ì´íŠ¸ ë‰´ìŠ¤', r'ë‹¤ìŒ ë‰´ìŠ¤',
    ]

    patterns_to_remove_regex = [
        r'\[.*?\]',  # ì˜ˆ: [ë‹¨ë…], [ì†ë³´], [ì¢…í•©], [ì‚¬ì§„], [ì˜ìƒ], [íŒ©íŠ¸ì²´í¬]
        r'\(.*?\)',  # ì˜ˆ: (ì„œìš¸), (ì¢…í•©), (ì˜ìƒ)
        r'\{.*?\}',  # ì˜ˆ: {ë‰´ìŠ¤ì´ˆì }
        r'<[^>]+>',  # HTML íƒœê·¸ ì”ì—¬ë¬¼
        r'\[\s*\w+\s*\]',  # ê³µë°± í¬í•¨ ëŒ€ê´„í˜¸ íƒœê·¸
    ]

    symbols_to_remove = [
        r'\|', r'\:', r'\_', r'\-', r'\+', r'=', r'/', r'\\'  # |, :, _, -, +, =, /, \ ë“±
    ]

    cleaned_title = title
    
    for pattern in patterns_to_remove:
        cleaned_title = re.sub(re.escape(pattern), '', cleaned_title, flags=re.IGNORECASE).strip()
    
    for pattern_regex in patterns_to_remove_regex + symbols_to_remove:
        cleaned_title = re.sub(pattern_regex, '', cleaned_title).strip()
    
    cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
    
    return cleaned_title

async def search_news_google_cs(query):
    logging.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    """Google Custom Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")

    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": google_api_key,
        "cx": google_cse_id,
        "q": query,
        "num": 10,
        "hl": "ko",
        "gl": "kr"
    }
    async with aiohttp.ClientSession() as session:
        async with session.get(url, params=params) as resp:
            data = await resp.json()
            if "error" in data:
                logging.error(f"Google CSE API ì˜¤ë¥˜: {data['error']['message']}")
                if "quotaExceeded" in data['error']['message']:
                    logging.warning("ê²½ê³ : Google CSE API í• ë‹¹ëŸ‰(Quota)ì´ ì´ˆê³¼ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Google Cloud Consoleì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return []
            return data.get("items", [])

def calculate_fact_check_confidence(criteria_scores):
    """
    íŒ©íŠ¸ì²´í¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ 0%ì—ì„œ 100%ê¹Œì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if not criteria_scores:
        return 0

    total_possible_score = 0
    total_actual_score = 0

    for criterion, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: '{criterion}'ì˜ ì ìˆ˜ '{score}'ê°€ ìœ íš¨í•œ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        total_possible_score += 5
        total_actual_score += score

    if total_actual_score == 0 and total_possible_score > 0:
        return 0

    if total_possible_score == 0:
        return 0

    confidence_percentage = (total_actual_score / total_possible_score) * 100

    return max(0, min(100, round(confidence_percentage)))

def calculate_source_diversity_score(evidence):
    """
    ì œê³µëœ ê·¼ê±°(evidence)ì˜ ì¶œì²˜ ë‹¤ì–‘ì„±ì„ ê³„ì‚°í•˜ì—¬ 0-5ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not evidence:
        return 0
    
    unique_sources = set()
    for item in evidence:
        # source_titleì´ ìˆë‹¤ë©´ ì´ë¥¼ ì‚¬ìš© (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì²˜ë¦¬)
        if item.get("source_title"):
            unique_sources.add(item["source_title"].lower())
        # source_titleì´ ì—†ìœ¼ë©´ URLì˜ ë„ë©”ì¸ì„ ì‚¬ìš©
        elif item.get("url"):
            try:
                domain = urlparse(item["url"]).netloc
                if domain:
                    unique_sources.add(domain.lower())
            except Exception:
                # URL íŒŒì‹± ì˜¤ë¥˜ ì‹œ ë¬´ì‹œ
                pass

    num_unique_sources = len(unique_sources)

    if num_unique_sources >= 4:
        return 5
    elif num_unique_sources == 3:
        return 4
    elif num_unique_sources == 2:
        return 3
    elif num_unique_sources == 1:
        return 1  # ë‹¨ì¼ ì¶œì²˜ë¼ë„ ë‹¤ì–‘ì„± ì ìˆ˜ëŠ” ë‚®ê²Œ ì±…ì •
    else:
        return 0  # ì¶œì²˜ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
