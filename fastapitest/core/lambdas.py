import os
import re
import asyncio
import logging
from urllib.parse import urlparse, urlunparse, urljoin

import aiohttp
import requests
from bs4 import BeautifulSoup
from newspaper import Article

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from openai import OpenAI
import yt_dlp

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# -----------------------------
# í…ìŠ¤íŠ¸ ì •ë¦¬ & ìœ í‹¸
# -----------------------------
def _clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'(\n){3,}', '\n\n', text)
    text = re.sub(r'Copyright\s*.*ë¬´ë‹¨ì „ì¬.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Â©\s*.*All rights reserved.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'ì €ì‘ê¶Œì\s*.*ë¬´ë‹¨ë³µì œ.*', '', text, flags=re.IGNORECASE)
    return text.strip()


_MEDIA = (
    r"(ë¬¸í™”ì¼ë³´|ì¤‘ì•™ì¼ë³´|ê²½í–¥ì‹ ë¬¸|ë¨¸ë‹ˆíˆ¬ë°ì´|MBN|ì—°í•©ë‰´ìŠ¤|SBS ë‰´ìŠ¤|MBC ë‰´ìŠ¤|KBS ë‰´ìŠ¤|ë™ì•„ì¼ë³´|"
    r"ì¡°ì„ ì¼ë³´|í•œê²¨ë ˆ|êµ­ë¯¼ì¼ë³´|ì„œìš¸ì‹ ë¬¸|ì„¸ê³„ì¼ë³´|ë…¸ì»·ë‰´ìŠ¤|í—¤ëŸ´ë“œê²½ì œ|ë§¤ì¼ê²½ì œ|í•œêµ­ê²½ì œ|ì•„ì‹œì•„ê²½ì œ|"
    r"YTN|JTBC|TVì¡°ì„ |ì±„ë„A|ë°ì¼ë¦¬ì•ˆ|ë‰´ì‹œìŠ¤|ë‰´ìŠ¤1|ì—°í•©ë‰´ìŠ¤TV|ë‰´ìŠ¤í•Œ|ì´ë°ì¼ë¦¬|íŒŒì´ë‚¸ì…œë‰´ìŠ¤|"
    r"ì•„ì£¼ê²½ì œ|UPIë‰´ìŠ¤|ZUM ë‰´ìŠ¤|ë„¤ì´íŠ¸ ë‰´ìŠ¤|ë‹¤ìŒ ë‰´ìŠ¤)"
)

def clean_news_title(title: str) -> str:
    if not title:
        return ""
    raw = title
    t = re.sub(r"<[^>]+>", " ", raw)
    t = re.sub(r"^\s*\[[^\]]{1,12}\]\s*", "", t)
    t = re.sub(rf"^\s*{_MEDIA}\s*[\|\-]\s*", "", t)
    t = re.sub(rf"\s*[\|\-]\s*{_MEDIA}\s*$", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    if len(t) < 2:
        return raw.strip()
    return t


def extract_video_id(url: str) -> str | None:
    try:
        m = re.search(r"(?:v=|/|youtu\.be/|shorts/|embed/)([0-9A-Za-z_-]{11})", url)
        if m:
            vid = m.group(1)
            logging.info(f"[ë””ë²„ê¹…] URLì—ì„œ ì¶”ì¶œëœ video_id: {vid}")
            return vid
    except Exception as e:
        logging.error(f"ìœ íŠœë¸Œ video_id ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


# -----------------------------
# Google CSE ë‰´ìŠ¤ ê²€ìƒ‰
# -----------------------------
async def search_news_google_cs(query: str):
    logging.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")

    if not google_api_key or not google_cse_id:
        logging.error("Google CSE í‚¤/ì—”ì§„ ID ëˆ„ë½")
        return []

    def _mk_params(q: str, start: int | None = None):
        p = {
            "key": google_api_key,
            "cx": google_cse_id,
            "q": q,
            "num": 10,
            "hl": "ko",
            "gl": "kr",
            "fields": "items(title,htmlTitle,link,displayLink,snippet),searchInformation(totalResults)"
        }
        if start:
            p["start"] = start
        return p

    url = "https://www.googleapis.com/customsearch/v1"
    async with aiohttp.ClientSession(headers={"Accept": "application/json"}) as session:
        # ì²« í˜ì´ì§€
        async with session.get(url, params=_mk_params(query), timeout=15) as resp:
            try:
                data = await resp.json()
            except Exception:
                txt = await resp.text()
                logging.error(f"CSE JSON íŒŒì‹± ì‹¤íŒ¨(status={resp.status}): {txt[:200]}")
                return []
            if "error" in data:
                msg = data["error"].get("message")
                logging.error(f"Google CSE API ì˜¤ë¥˜: {msg}")
                return []
            items = data.get("items") or []
            if items:
                return items
        # ë‘ ë²ˆì§¸ í˜ì´ì§€ ì‹œë„
        async with session.get(url, params=_mk_params(query, start=11), timeout=15) as resp2:
            data2 = await resp2.json()
            return data2.get("items") or []

    # ì´ê³³ì— ë„ë‹¬í•  ì¼ì€ ê±°ì˜ ì—†ìŒ
    return []


# -----------------------------
# ìƒëŒ€ê²½ë¡œ â†’ ì ˆëŒ€ê²½ë¡œ ë³´ì • (í•µì‹¬ íŒ¨ì¹˜ 1)
# -----------------------------
RELATIVE_HOST_MAP = {
    "/news/newsView.php": "www.seoul.co.kr",  # ì„œìš¸ì‹ ë¬¸ ìƒëŒ€ URL ë³´ì •
    # í•„ìš”ì‹œ ì¶”ê°€ ë§¤í•‘
}

def absolutize_url(url: str, base_domain: str | None = None) -> str:
    """
    ìƒëŒ€/ìŠ¤í‚´ì—†ëŠ” URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜í•œë‹¤.
    base_domain(ì˜ˆ: 'www.seoul.co.kr')ì´ ì£¼ì–´ì§€ë©´ ìš°ì„  ì‚¬ìš©.
    """
    if not url:
        return ""
    p = urlparse(url)

    # ì´ë¯¸ ì ˆëŒ€ URL
    if p.scheme:
        return url

    # //example.com/.. í˜•íƒœ
    if url.startswith("//"):
        return "https:" + url

    # ê²€ìƒ‰ê²°ê³¼ì˜ displayLink ê°™ì€ ë„ë©”ì¸ì´ ìˆì„ ë•Œ
    if base_domain:
        if not base_domain.startswith("http"):
            base_domain = "https://" + base_domain
        return urljoin(base_domain, url)

    # ìì£¼ ë‚˜ì˜¤ëŠ” ìƒëŒ€ê²½ë¡œ íŒ¨í„´
    for prefix, host in RELATIVE_HOST_MAP.items():
        if url.startswith(prefix):
            return urljoin(f"https://{host}", url)

    # ì—¬ê¸°ê¹Œì§€ë„ ë§¤ì¹­ ì•ˆ ë˜ë©´ ì‹¤íŒ¨
    return ""


# -----------------------------
# ì–¸ë¡ ì‚¬ë³„ ì„ íƒì ì¶”ì¶œ
# -----------------------------
def _extract_article_content_with_selectors(html_content: str, url: str) -> str:
    # ê²½í–¥ì‹ ë¬¸ êµ¬í˜• URL ë³´ì •
    if "news.khan.co.kr/kh_news/khan_art_view.html" in url:
        m = re.search(r'artid=(\d+)', url)
        if m:
            art_id = m.group(1)
            url = f"https://www.khan.co.kr/article/{art_id}"

    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    selectors = {
        "hani.co.kr": "div.article-text p.text",
        "khan.co.kr": "#articleBody p, div#articleBody p, #articleBody p.content_text",
        "segye.com": "article.viewBox2",
        "hankookilbo.com": "div.col-main p.read",
        "asiatoday.co.kr": "div.news_bm",
        "seoul.co.kr": "div.viewContent",
        "donga.com": "section.news_view",
        "naeil.com": "div.article-view p",
    }

    selector = None
    for dom, sel in selectors.items():
        if dom in domain:
            selector = sel
            break
    if not selector:
        return ""

    soup = BeautifulSoup(html_content, 'html.parser')
    article_elements = []

    if any(d in domain for d in ["hani.co.kr", "khan.co.kr", "hankookilbo.com", "naeil.com"]):
        elements = soup.select(selector)
        for p_tag in elements:
            for br in p_tag.find_all('br'):
                br.replace_with('\n')
            text = p_tag.get_text(separator=' ').strip()
            if text:
                article_elements.append(text)
    elif any(d in domain for d in ["segye.com", "asiatoday.co.kr", "seoul.co.kr", "donga.com"]):
        main_content_div = soup.select_one(selector)
        if main_content_div:
            for tag in main_content_div.find_all([
                'script','style','img','table','figure','figcaption','aside','nav','footer','header',
                'iframe','video','audio','meta','link','form','input','button','select','textarea','svg',
                'canvas','map','area','object','param','embed','source','track','picture','portal','slot',
                'template','noscript','ins','del','bdo','bdi','rp','rt','rtc','ruby','data','time','mark',
                'small','sub','sup','abbr','acronym','address','b','big','blockquote','center','cite','code',
                'dd','dfn','dir','dl','dt','em','font','i','kbd','li','menu','ol','pre','q','s','samp',
                'strike','strong','tt','u','var','ul','h1','h2','h3','h4','h5','h6'
            ]):
                tag.decompose()

            for br in main_content_div.find_all('br'):
                br.replace_with('\n')

            paragraphs = []
            for content in main_content_div.contents:
                if getattr(content, "name", None) == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())

            article_elements = paragraphs

    full_text = '\n\n'.join(filter(None, article_elements))
    return full_text


# -----------------------------
# Selenium í¬ë¡¤ë§ (ì¡°ì„ /ì œë„¤ë¦­)
# -----------------------------
def extract_chosun_with_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = None
    try:
        logging.info(f"ğŸ“° Seleniumìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        article_selector = "article#article-view-content-div, article.layout__article-main section.article-body"
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, article_selector)))

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        container = soup.select_one('article.layout__article-main section.article-body') or \
                    soup.select_one('article#article-view-content-div')

        if not container:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""

        article_content = []
        for p in container.find_all("p"):
            text = p.get_text(strip=True)
            if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                article_content.append(text)
        full_text = '\n'.join(article_content)

        if full_text and len(full_text) > 100:
            logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            return full_text

        logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return ""
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()


def _extract_generic_with_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = None
    try:
        logging.info(f"ğŸ“° Selenium (Generic)ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        selectors = [
            "div.article_content","div#articleBodyContents","div#article_body",
            "div.news_content","article.article_view","div.view_content",
            "div.article-text","div.article-body","div.entry-content",
            "div.contents_area","div.news_view","div.viewContent",
            "article.viewBox2","div.col-main","div.news_bm","section.news_view",
            "div.article-view"
        ]

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ", ".join(selectors))))
        except TimeoutException:
            logging.warning("âš ï¸ Selenium (Generic): ë³¸ë¬¸ ìš”ì†Œê°€ 10ì´ˆ ë‚´ ë¡œë“œë˜ì§€ ì•ŠìŒ. ì „ì²´ ì†ŒìŠ¤ ì‚¬ìš©.")

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        container = None
        for sel in selectors:
            container = soup.select_one(sel)
            if container:
                break

        if container:
            for tag in container.find_all([
                'script','style','img','table','figure','figcaption','aside','nav','footer','header',
                'iframe','video','audio','meta','link','form','input','button','select','textarea','svg',
                'canvas','map','area','object','param','embed','source','track','picture','portal','slot',
                'template','noscript','ins','del','bdo','bdi','rp','rt','rtc','ruby','data','time','mark',
                'small','sub','sup','abbr','acronym','address','b','big','blockquote','center','cite','code',
                'dd','dfn','dir','dl','dt','em','font','i','kbd','li','menu','ol','pre','q','s','s','samp',
                'strike','strong','tt','u','var','ul','h1','h2','h3','h4','h5','h6'
            ]):
                tag.decompose()

            for br in container.find_all('br'):
                br.replace_with('\n')

            paragraphs = []
            for content in container.contents:
                if getattr(content, "name", None) == 'p':
                    tx = content.get_text(separator=' ').strip()
                    if tx:
                        paragraphs.append(tx)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())

            full_text = '\n\n'.join(filter(None, paragraphs))
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            logging.warning("Selenium (Generic): ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ.")
            return ""
        else:
            logging.warning("Selenium (Generic): ë³¸ë¬¸ ìš”ì†Œ ë¯¸ë°œê²¬. ì „ì²´ í…ìŠ¤íŠ¸ ì‹œë„.")
            full_text = soup.get_text(separator='\n', strip=True)
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            logging.warning("Selenium (Generic): ì „ì²´ í…ìŠ¤íŠ¸ë„ ë¶ˆì¶©ë¶„.")
            return ""
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium (Generic) ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨/íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium (Generic) ì˜¤ë¥˜: {e}")
        return ""
    finally:
        if driver:
            driver.quit()


# -----------------------------
# ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
# -----------------------------
async def get_article_text(url: str, base_domain: str | None = None) -> str:
    """
    (í•µì‹¬ íŒ¨ì¹˜) urlì´ ìƒëŒ€ê²½ë¡œì—¬ë„ base_domainê³¼ ë§¤í•‘ì„ ì´ìš©í•´ ì ˆëŒ€ê²½ë¡œë¡œ ë§Œë“  ë’¤ ì§„í–‰.
    """
    fixed = absolutize_url(url, base_domain=base_domain)
    if not fixed:
        logging.warning(f"âš ï¸ ìƒëŒ€ ê²½ë¡œ URL(ë² ì´ìŠ¤ ì—†ìŒ) ìŠ¤í‚µ: {url}")
        return ""

    url = fixed
    logging.info(f"ğŸ“° ë¹„ë™ê¸°ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {url}")

    parsed_url = urlparse(url)
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

    # ì¡°ì„ ì¼ë³´ëŠ” ë°”ë¡œ Selenium
    if "chosun.com" in parsed_url.netloc:
        logging.info("â­ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ê°ì§€. Selenium í¬ë¡¤ë§ ìš°ì„ .")
        try:
            text = await asyncio.to_thread(extract_chosun_with_selenium, url)
            if text and len(text) > 100:
                return _clean_text(text)
            logging.warning("âš ï¸ Selenium(ì¡°ì„ ) ì¶”ì¶œ ì‹¤íŒ¨/ë¶ˆì¶©ë¶„. ì¢…ë£Œ.")
            return ""
        except Exception as e:
            logging.error(f"âŒ Selenium(ì¡°ì„ ) ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return ""

    # aiohttp + newspaper
    try:
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(clean_url, timeout=30) as response:
                response.raise_for_status()
                html_content = await response.text()

                article = Article(clean_url, language='ko')
                article.download(input_html=html_content)
                article.parse()

                if article.text and len(article.text) > 300:
                    logging.info(f"âœ… newspaperë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(article.text)}ì): {url}")
                    return _clean_text(article.text)
                else:
                    logging.warning(f"âš ï¸ newspaper ê²°ê³¼ ë¶ˆì¶©ë¶„. BeautifulSoup ì„ íƒì í´ë°±: {url}")
    except aiohttp.ClientError as e:
        logging.warning(f"âš ï¸ aiohttp ì˜¤ë¥˜. ë‹¤ìŒ ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ì‹¤íŒ¨. ë‹¤ìŒ ì‹œë„: {url} -> {e}")

    # requests + ì„ íƒì
    try:
        logging.warning(f"âš ï¸ requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„ ì„ íƒì) ì¬ì‹œë„: {url}")
        resp = requests.get(clean_url, headers=headers, timeout=30)
        resp.raise_for_status()
        html = resp.text

        extracted = _extract_article_content_with_selectors(html, url)
        if extracted and len(extracted) > 100:  # <-- 'and' (ì˜¤íƒ€ ë°©ì§€)
            cleaned = _clean_text(extracted)
            logging.info(f"âœ… requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„) ì„±ê³µ ({len(cleaned)}ì): {url}")
            return cleaned
        else:
            logging.warning("âš ï¸ ì–¸ë¡ ì‚¬ë³„ ì„ íƒì ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŒ. Selenium í´ë°±.")
    except requests.exceptions.RequestException as e:
        logging.warning(f"âš ï¸ requests ë‹¨ê³„ ì‹¤íŒ¨. Selenium í´ë°±: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ BeautifulSoup íŒŒì‹± ì‹¤íŒ¨. Selenium í´ë°±: {url} -> {e}")

    # Selenium (Generic) ìµœì¢… í´ë°±
    logging.warning(f"âš ï¸ ìµœì¢… í´ë°±: Selenium (Generic) ì‹œë„: {url}")
    try:
        text = await asyncio.to_thread(_extract_generic_with_selenium, url)
        if text and len(text) > 100:
            logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ìµœì¢… ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            return _clean_text(text)
        logging.warning("âš ï¸ Selenium (Generic)ë„ ë¶ˆì¶©ë¶„/ì‹¤íŒ¨.")
        return ""
    except Exception as e:
        logging.error(f"âŒ Selenium (Generic) ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return ""


# -----------------------------
# YouTube ìë§‰ (yt-dlp + Whisper)
# -----------------------------
def fetch_youtube_transcript(video_url: str) -> str:
    vid = extract_video_id(video_url)
    logging.info(f"[ë””ë²„ê¹…] ì¶”ì¶œëœ video_id: {vid}")
    if not vid:
        logging.error("ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return ""

    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        logging.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return ""

    cookies_path = os.getenv("YTDLP_COOKIES", "/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt")
    outtmpl = f"{vid}.%(ext)s"
    downloaded_paths: list[str] = []

    try:
        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': outtmpl,
            'cookiefile': cookies_path if os.path.exists(cookies_path) else None,
            'quiet': True,
        }

        logging.info(f"ğŸ¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            sinfo = ydl.sanitize_info(info)
            if 'requested_downloads' in sinfo:
                downloaded_paths = [d['filepath'] for d in sinfo['requested_downloads']]
            elif '_filename' in sinfo:
                downloaded_paths.append(sinfo['_filename'])

            if not downloaded_paths:
                raise RuntimeError("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

            audio_file = downloaded_paths[0]

        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {os.path.abspath(audio_file)}")

        with open(audio_file, "rb") as f:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language="ko"
            )
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
        return transcript.text or ""
    except Exception as e:
        logging.exception(f"yt-dlp ë˜ëŠ” Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        for p in downloaded_paths:
            try:
                if os.path.exists(p):
                    os.remove(p)
                    logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {p}")
            except Exception:
                pass


# -----------------------------
# ì ìˆ˜ ê³„ì‚° ìœ í‹¸(ë³µêµ¬ í¬í•¨)
# -----------------------------
def calculate_fact_check_confidence(criteria_scores: dict) -> int:
    """
    ê°œë³„ ê¸°ì¤€(0~5ì )ì˜ í•©ìœ¼ë¡œ 0~100% í™˜ì‚°.
    """
    if not criteria_scores:
        return 0
    total_possible = 0
    total_actual = 0
    for _, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: ì ìˆ˜ '{score}'ê°€ ìœ íš¨ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¨")
            return 0
        total_possible += 5
        total_actual += score
    if total_possible == 0:
        return 0
    pct = (total_actual / total_possible) * 100
    return max(0, min(100, round(pct)))


def calculate_source_diversity_score(evidence: list[dict]) -> int:
    """
    ì œê³µëœ ê·¼ê±°(evidence)ì˜ ì¶œì²˜ ë‹¤ì–‘ì„±ì„ 0~5ì ìœ¼ë¡œ í™˜ì‚°.
    - ì„œë¡œ ë‹¤ë¥¸ source_title(ë˜ëŠ” ë„ë©”ì¸) ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
    - 1ê°œ=1ì , 2ê°œ=3ì , 3ê°œ=4ì , 4ê°œ ì´ìƒ=5ì , ì—†ìŒ=0ì 
    """
    if not evidence:
        return 0
    unique = set()
    for item in evidence:
        st = item.get("source_title")
        if st:
            unique.add(st.lower())
            continue
        url = item.get("url")
        if url:
            try:
                dom = urlparse(url).netloc
                if dom:
                    unique.add(dom.lower())
            except Exception:
                pass
    n = len(unique)
    if n >= 4:
        return 5
    if n == 3:
        return 4
    if n == 2:
        return 3
    if n == 1:
        return 1
    return 0
