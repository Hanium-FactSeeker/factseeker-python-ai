import os
import re
import time
import json
import asyncio
import logging
import hashlib
from urllib.parse import urlparse, urlunparse

import aiohttp
import requests
from bs4 import BeautifulSoup
from newspaper import Article

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from openai import OpenAI
import yt_dlp


logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# -----------------------------
# Utilities
# -----------------------------
def _clean_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'(\n){3,}', '\n\n', text)
    text = re.sub(r'Copyright\s*.*ë¬´ë‹¨ì „ì¬.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Â©\s*.*All rights reserved.*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'ì €ì‘ê¶Œì\s*.*ë¬´ë‹¨ë³µì œ.*', '', text, flags=re.IGNORECASE)
    return text.strip()


def extract_video_id(url: str):
    try:
        m = re.search(r"(?:v=|/|youtu\.be/|shorts/|embed/)([0-9A-Za-z_-]{11})", url)
        if m:
            vid = m.group(1)
            logging.info(f"[ë””ë²„ê¹…] URLì—ì„œ ì¶”ì¶œëœ video_id: {vid}")
            return vid
    except Exception as e:
        logging.error(f"ìœ íŠœë¸Œ video_id ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


# -----------------------------
# Title cleaner (ì™„í™” + ì„¸ì´í”„ê°€ë“œ)
# -----------------------------
_MEDIA = (
    r"(ë¬¸í™”ì¼ë³´|ì¤‘ì•™ì¼ë³´|ê²½í–¥ì‹ ë¬¸|ë¨¸ë‹ˆíˆ¬ë°ì´|MBN|ì—°í•©ë‰´ìŠ¤|SBS ë‰´ìŠ¤|MBC ë‰´ìŠ¤|KBS ë‰´ìŠ¤|ë™ì•„ì¼ë³´|"
    r"ì¡°ì„ ì¼ë³´|í•œê²¨ë ˆ|êµ­ë¯¼ì¼ë³´|ì„œìš¸ì‹ ë¬¸|ì„¸ê³„ì¼ë³´|ë…¸ì»·ë‰´ìŠ¤|í—¤ëŸ´ë“œê²½ì œ|ë§¤ì¼ê²½ì œ|í•œêµ­ê²½ì œ|ì•„ì‹œì•„ê²½ì œ|"
    r"YTN|JTBC|TVì¡°ì„ |ì±„ë„A|ë°ì¼ë¦¬ì•ˆ|ë‰´ì‹œìŠ¤|ë‰´ìŠ¤1|ì—°í•©ë‰´ìŠ¤TV|ë‰´ìŠ¤í•Œ|ì´ë°ì¼ë¦¬|íŒŒì´ë‚¸ì…œë‰´ìŠ¤|"
    r"ì•„ì£¼ê²½ì œ|UPIë‰´ìŠ¤|ZUM ë‰´ìŠ¤|ë„¤ì´íŠ¸ ë‰´ìŠ¤|ë‹¤ìŒ ë‰´ìŠ¤)"
)

def clean_news_title(title: str) -> str:
    if not title:
        return ""
    raw = title

    # HTML íƒœê·¸ ì œê±°
    t = re.sub(r"<[^>]+>", " ", raw)

    # ì‹œì‘ë¶€ì˜ ì§§ì€ ëŒ€ê´„í˜¸ íƒœê·¸ ì œê±° (ì˜ˆ: [ë‹¨ë…], [ì†ë³´])
    t = re.sub(r"^\s*\[[^\]]{1,12}\]\s*", "", t)

    # ì–‘ëì˜ ì–¸ë¡ ì‚¬ í‘œê¸° ì œê±° (| ë˜ëŠ” - ë¡œ êµ¬ë¶„ëœ ê²½ìš°)
    t = re.sub(rf"^\s*{_MEDIA}\s*[\|\-]\s*", "", t)
    t = re.sub(rf"\s*[\|\-]\s*{_MEDIA}\s*$", "", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip()

    # ì„¸ì´í”„ê°€ë“œ: ë„ˆë¬´ ì§§ì•„ì§€ë©´ ì›ì œëª© ìœ ì§€
    if len(t) < 2:
        return raw.strip()
    return t


# -----------------------------
# CSE ê²€ìƒ‰ (ë‹¤ìš´ì‹œí”„íŠ¸ í¬í•¨)
# -----------------------------
async def search_news_google_cs(query: str):
    logging.info(f"Google CSEë¡œ ë‰´ìŠ¤ ê²€ìƒ‰: {query}")
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")

    if not google_api_key or not google_cse_id:
        logging.error("Google CSE í‚¤/ì—”ì§„ ID ëˆ„ë½")
        return []

    def _simplify_ko(q: str) -> str:
        stop = ["ë‚´ë¶€", "ê¸°ë¥˜", "ì¦ì–¸", "ë‚˜ì˜¤ê³ ", "ìˆë‹¤", "ìˆë‹¤ëŠ”", "í•˜ë ¤", "ì›€ì§ì„ì´", "ìœ„ì›ì¥", "ë°œì–¸ì„", "ì œì§€í–ˆë‹¤"]
        tokens = re.split(r"\s+", q)
        tokens = [t for t in tokens if t and t not in stop]
        return " ".join(tokens) or q

    def _mk_params(q: str, or_terms: str | None = None, start: int | None = None):
        p = {
            "key": google_api_key,
            "cx": google_cse_id,
            "q": q,
            "num": 10,
            "hl": "ko",
            "gl": "kr",
            "fields": "items(title,htmlTitle,link,displayLink,snippet),searchInformation(totalResults)"
        }
        if or_terms:
            p["orTerms"] = or_terms
        if start:
            p["start"] = start
        return p

    url = "https://www.googleapis.com/customsearch/v1"

    async def _cse_fetch(session: aiohttp.ClientSession, params: dict, max_retries: int = 2):
        delay = 0.5
        for attempt in range(max_retries + 1):
            try:
                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=25)) as resp:
                    data = await resp.json()
                    if "error" in data:
                        msg = data["error"].get("message")
                        logging.error(f"Google CSE API ì˜¤ë¥˜: {msg}")
                        if msg and "quota" in msg.lower():
                            logging.warning("ê²½ê³ : CSE ì¿¼í„° ì´ˆê³¼ ê°€ëŠ¥ì„±")
                        return []
                    return data.get("items") or []
            except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                logging.warning(f"CSE ìš”ì²­ ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{max_retries}): {e}")
                if attempt < max_retries:
                    await asyncio.sleep(delay)
                    delay *= 2
                else:
                    return []

    async with aiohttp.ClientSession(headers={"Accept": "application/json"}) as session:
        # 1ì°¨: ì› ì¿¼ë¦¬ (+2í˜ì´ì§€ê¹Œì§€)
        for attempt in range(1, 3):
            params = _mk_params(query, start=1 + (attempt - 1) * 10 if attempt > 1 else None)
            items = await _cse_fetch(session, params)
            if items:
                for it in items[:5]:
                    logging.debug(f"[CSE] raw_title={it.get('title')!r}")
                return items

        # 2ì°¨: ë°©ì†¡3ì‚¬ OR í™•ì¥
        if re.search(r"\b(KBS|MBC|EBS)\b", query, flags=re.IGNORECASE):
            params = _mk_params(_simplify_ko(query), or_terms="KBS MBC EBS")
            items = await _cse_fetch(session, params)
            if items:
                for it in items[:5]:
                    logging.debug(f"[CSE] raw_title={it.get('title')!r}")
                return items

        # 3ì°¨: ê°•ì œ ì¶•ì•½
        simplified = _simplify_ko(query)
        if simplified != query:
            params = _mk_params(simplified)
            items = await _cse_fetch(session, params)
            if items:
                for it in items[:5]:
                    logging.debug(f"[CSE] raw_title={it.get('title')!r}")
                return items

    logging.warning("ğŸ“­ CSE ê²°ê³¼ 0ê±´ (ëª¨ë“  ë‹¤ìš´ì‹œí”„íŠ¸ ì‹¤íŒ¨)")
    return []


# -----------------------------
# Article extraction (ì–¸ë¡ ì‚¬ ì„ íƒì + Selenium)
# -----------------------------
def _extract_article_content_with_selectors(html_content: str, url: str) -> str:
    # ê²½í–¥ì‹ ë¬¸ êµ¬í˜• URL â†’ ì‹ í˜• ì „í™˜
    if "news.khan.co.kr/kh_news/khan_art_view.html" in url:
        m = re.search(r'artid=(\d+)', url)
        if m:
            art_id = m.group(1)
            url = f"https://www.khan.co.kr/article/{art_id}"

    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    selectors = {
        "hani.co.kr": "div.article-text p.text",
        "khan.co.kr": "#articleBody p, div#articleBody p, #articleBody p.content_text",
        "segye.com": "article.viewBox2",
        "hankookilbo.com": "div.col-main p.read",
        "asiatoday.co.kr": "div.news_bm",
        "seoul.co.kr": "div.viewContent",
        "donga.com": "section.news_view",
        "naeil.com": "div.article-view p",
    }

    selector = None
    for dom, sel in selectors.items():
        if dom in domain:
            selector = sel
            break
    if not selector:
        return ""

    soup = BeautifulSoup(html_content, 'html.parser')
    article_elements = []

    if any(d in domain for d in ["hani.co.kr", "khan.co.kr", "hankookilbo.com", "naeil.com"]):
        elements = soup.select(selector)
        for p_tag in elements:
            for br in p_tag.find_all('br'):
                br.replace_with('\n')
            text = p_tag.get_text(separator=' ').strip()
            if text:
                article_elements.append(text)
    elif any(d in domain for d in ["segye.com", "asiatoday.co.kr", "seoul.co.kr", "donga.com"]):
        main_content_div = soup.select_one(selector)
        if main_content_div:
            for tag in main_content_div.find_all([
                'script', 'style', 'img', 'table', 'figure', 'figcaption', 'aside', 'nav', 'footer', 'header',
                'iframe', 'video', 'audio', 'meta', 'link', 'form', 'input', 'button', 'select', 'textarea', 'svg',
                'canvas', 'map', 'area', 'object', 'param', 'embed', 'source', 'track', 'picture', 'portal', 'slot',
                'template', 'noscript', 'ins', 'del', 'bdo', 'bdi', 'rp', 'rt', 'rtc', 'ruby', 'data', 'time', 'mark',
                'small', 'sub', 'sup', 'abbr', 'acronym', 'address', 'b', 'big', 'blockquote', 'center', 'cite', 'code',
                'dd', 'dfn', 'dir', 'dl', 'dt', 'em', 'font', 'i', 'kbd', 'li', 'menu', 'ol', 'pre', 'q', 's', 'samp',
                'strike', 'strong', 'tt', 'u', 'var', 'ul', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'
            ]):
                tag.decompose()

            for br in main_content_div.find_all('br'):
                br.replace_with('\n')

            paragraphs = []
            for content in main_content_div.contents:
                if getattr(content, "name", None) == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())

            article_elements = paragraphs

    full_text = '\n\n'.join(filter(None, article_elements))
    return full_text


def extract_chosun_with_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = None
    try:
        logging.info(f"ğŸ“° Seleniumìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        article_selector = "article#article-view-content-div, article.layout__article-main section.article-body"
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, article_selector)))

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        article_content = []
        container = soup.select_one('article.layout__article-main section.article-body') or \
                    soup.select_one('article#article-view-content-div')

        if container:
            paragraphs = container.find_all("p")
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and not any(k in text for k in ["chosun.com", "ê¸°ì", "Copyright", "ë¬´ë‹¨ì „ì¬"]):
                    article_content.append(text)
            full_text = '\n'.join(article_content)

            if full_text and len(full_text) > 100:
                logging.info("âœ… Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Seleniumìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Seleniumì—ì„œë„ ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()


def _extract_generic_with_selenium(url: str) -> str:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = None
    try:
        logging.info(f"ğŸ“° Selenium (Generic)ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„: {url}")
        service = Service("/usr/local/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)

        generic_article_selectors = [
            "div.article_content", "div#articleBodyContents", "div#article_body",
            "div.news_content", "article.article_view", "div.view_content",
            "div.article-text", "div.article-body", "div.entry-content",
            "div.contents_area", "div.news_view", "div.viewContent",
            "article.viewBox2", "div.col-main", "div.news_bm", "section.news_view",
            "div.article-view"
        ]

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ", ".join(generic_article_selectors))))
        except TimeoutException:
            logging.warning("âš ï¸ Selenium (Generic): ë³¸ë¬¸ ìš”ì†Œê°€ 10ì´ˆ ë‚´ì— ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ í˜ì´ì§€ ì†ŒìŠ¤ ì‚¬ìš©.")

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        container = None
        for selector in generic_article_selectors:
            container = soup.select_one(selector)
            if container:
                break

        if container:
            for tag in container.find_all([
                'script', 'style', 'img', 'table', 'figure', 'figcaption', 'aside', 'nav', 'footer', 'header',
                'iframe', 'video', 'audio', 'meta', 'link', 'form', 'input', 'button', 'select', 'textarea', 'svg',
                'canvas', 'map', 'area', 'object', 'param', 'embed', 'source', 'track', 'picture', 'portal', 'slot',
                'template', 'noscript', 'ins', 'del', 'bdo', 'bdi', 'rp', 'rt', 'rtc', 'ruby', 'data', 'time', 'mark',
                'small', 'sub', 'sup', 'abbr', 'acronym', 'address', 'b', 'big', 'blockquote', 'center', 'cite', 'code',
                'dd', 'dfn', 'dir', 'dl', 'dt', 'em', 'font', 'i', 'kbd', 'li', 'menu', 'ol', 'pre', 'q', 's', 'samp',
                'strike', 'strong', 'tt', 'u', 'var', 'ul', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'
            ]):
                tag.decompose()

            for br in container.find_all('br'):
                br.replace_with('\n')

            paragraphs = []
            for content in container.contents:
                if getattr(content, "name", None) == 'p':
                    text = content.get_text(separator=' ').strip()
                    if text:
                        paragraphs.append(text)
                elif isinstance(content, str) and content.strip():
                    paragraphs.append(content.strip())

            full_text = '\n\n'.join(filter(None, paragraphs))
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Selenium (Generic)ìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìœ¼ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
        else:
            logging.warning("Selenium (Generic): íŠ¹ì • ë³¸ë¬¸ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            full_text = soup.get_text(separator='\n', strip=True)
            if full_text and len(full_text) > 100:
                logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                return full_text
            else:
                logging.warning("Selenium (Generic)ìœ¼ë¡œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ë„ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ""
    except (TimeoutException, NoSuchElementException) as e:
        logging.error(f"âŒ Selenium (Generic) í¬ë¡¤ë§ ì¤‘ ìš”ì†Œ íƒìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ: {e}")
        return ""
    except Exception as e:
        logging.exception(f"âŒ Selenium (Generic) í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""
    finally:
        if driver:
            driver.quit()


# -----------------------------
# Async article fetch orchestrator
# -----------------------------
async def get_article_text(url: str) -> str:
    logging.info(f"ğŸ“° ë¹„ë™ê¸°ë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {url}")
    parsed_url = urlparse(url)
    clean_url = urlunparse(parsed_url._replace(query='', fragment=''))
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

    # ì¡°ì„ ì¼ë³´ ìš°ì„  ì‹œë„
    if "chosun.com" in parsed_url.netloc:
        logging.info("â­ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ê°ì§€. Selenium í¬ë¡¤ë§ì„ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            text = await asyncio.to_thread(extract_chosun_with_selenium, url)
            if text and len(text) > 100:
                return _clean_text(text)
            else:
                logging.warning("âš ï¸ Seleniumìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ì„ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return ""
        except Exception as e:
            logging.error(f"âŒ asyncio.to_thread Selenium ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""

    # aiohttp + newspaper
    try:
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.get(clean_url, timeout=30) as response:
                response.raise_for_status()
                html_content = await response.text()

                article = Article(clean_url, language='ko')
                article.download(input_html=html_content)
                article.parse()

                if article.text and len(article.text) > 300:
                    logging.info(f"âœ… newspaperë¡œ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(article.text)}ì): {url}")
                    return _clean_text(article.text)
                else:
                    logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•¨. BeautifulSoup í´ë°± ì‹œë„: {url}")
    except aiohttp.ClientError as e:
        logging.warning(f"âš ï¸ aiohttp í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ ë°œìƒ. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ newspaper í¬ë¡¤ë§ ì‹¤íŒ¨. ë‹¤ìŒ ë°©ë²• ì‹œë„: {url} -> {e}")

    # requests + BeautifulSoup (ì–¸ë¡ ì‚¬ë³„ ì„ íƒì)
    try:
        logging.warning(f"âš ï¸ requests+BeautifulSoupìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì¬ì‹œë„ (ì–¸ë¡ ì‚¬ë³„ ì„ íƒì ì ìš©): {url}")
        response = requests.get(clean_url, headers=headers, timeout=30)
        response.raise_for_status()
        html_content = response.text

        extracted = _extract_article_content_with_selectors(html_content, url)
        if extracted and len(extracted) > 100:
            cleaned_final_text = _clean_text(extracted)
            logging.info(f"âœ… requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„)ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(cleaned_final_text)}ì): {url}")
            return cleaned_final_text
        else:
            logging.warning("âš ï¸ requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„)ë¡œ ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ. ìµœì¢… í´ë°± ì‹œë„.")
    except requests.exceptions.RequestException as e:
        logging.warning(f"âš ï¸ requests+BeautifulSoup (ì–¸ë¡ ì‚¬ë³„) ì‹¤íŒ¨. ìµœì¢… í´ë°± ì‹œë„: {url} -> {e}")
    except Exception as e:
        logging.warning(f"âš ï¸ BeautifulSoup íŒŒì‹± ì‹¤íŒ¨ (ì–¸ë¡ ì‚¬ë³„). ìµœì¢… í´ë°± ì‹œë„: {url} -> {e}")

    # ìµœì¢… í´ë°±: Selenium (Generic)
    logging.warning(f"âš ï¸ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨. ìµœì¢… í´ë°±: Selenium (Generic) ì‹œë„: {url}")
    try:
        text = await asyncio.to_thread(_extract_generic_with_selenium, url)
        if text and len(text) > 100:
            logging.info("âœ… Selenium (Generic)ìœ¼ë¡œ ìµœì¢… ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ")
            return _clean_text(text)
        else:
            logging.warning("âš ï¸ Selenium (Generic)ìœ¼ë¡œë„ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš©ì´ ë¶ˆì¶©ë¶„í•©ë‹ˆë‹¤.")
            return ""
    except Exception as e:
        logging.error(f"âŒ asyncio.to_thread Selenium (Generic) ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


# -----------------------------
# YouTube transcript (yt-dlp + Whisper)
# -----------------------------
def fetch_youtube_transcript(video_url: str) -> str:
    vid = extract_video_id(video_url)
    logging.info(f"[ë””ë²„ê¹…] ì¶”ì¶œëœ video_id: {vid}")
    if not vid:
        logging.error("ìœ íš¨í•œ YouTube URLì´ ì•„ë‹™ë‹ˆë‹¤.")
        return ""

    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        logging.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return ""

    cookies_path = "/home/ubuntu/factseeker-python-ai/fastapitest/cookies.txt"
    outtmpl = f"{vid}.%(ext)s"
    downloaded_paths: list[str] = []

    try:
        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': outtmpl,
            'cookiefile': cookies_path,
            'quiet': True,
        }

        logging.info(f"ğŸ¬ yt-dlpë¡œ ìŒì› ë‹¤ìš´ë¡œë“œ ì‹œì‘: {video_url}")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(video_url, download=True)
            sinfo = ydl.sanitize_info(info)
            if 'requested_downloads' in sinfo:
                downloaded_paths = [d['filepath'] for d in sinfo['requested_downloads']]
            elif '_filename' in sinfo:
                downloaded_paths.append(sinfo['_filename'])

            if not downloaded_paths:
                raise RuntimeError("yt-dlp ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

            audio_file = downloaded_paths[0]

        logging.info(f"âœ… ìŒì› ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {audio_file}")

        with open(audio_file, "rb") as f:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                language="ko"
            )
        logging.info("âœ… Whisper APIë¡œ ìë§‰ ì¶”ì¶œ ì™„ë£Œ")
        return transcript.text or ""

    except Exception as e:
        logging.exception(f"yt-dlp ë˜ëŠ” Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""
    finally:
        for p in downloaded_paths:
            try:
                if os.path.exists(p):
                    os.remove(p)
                    logging.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {p}")
            except Exception:
                pass


# -----------------------------
# Scoring helpers
# -----------------------------
def calculate_fact_check_confidence(criteria_scores: dict) -> int:
    if not criteria_scores:
        return 0
    total_possible = 0
    total_actual = 0
    for _, score in criteria_scores.items():
        if not (0 <= score <= 5):
            logging.error(f"ì˜¤ë¥˜: ì ìˆ˜ '{score}'ê°€ ìœ íš¨ ë²”ìœ„(0-5)ë¥¼ ë²—ì–´ë‚¨")
            return 0
        total_possible += 5
        total_actual += score
    if total_possible == 0:
        return 0
    pct = (total_actual / total_possible) * 100
    return max(0, min(100, round(pct)))


def calculate_source_diversity_score(evidence: list[dict]) -> int:
    if not evidence:
        return 0
    unique = set()
    for item in evidence:
        st = item.get("source_title")
        if st:
            unique.add(st.lower())
            continue
        url = item.get("url")
        if url:
            try:
                dom = urlparse(url).netloc
                if dom:
                    unique.add(dom.lower())
            except Exception:
                pass
    n = len(unique)
    if n >= 4:
        return 5
    if n == 3:
        return 4
    if n == 2:
        return 3
    if n == 1:
        return 1
    return 0
