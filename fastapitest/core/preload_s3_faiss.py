import os
import time
import logging
import boto3
from core.faiss_manager import download_from_s3_if_exists, CHUNK_CACHE_DIR

# S3 ì„¤ì •
S3_BUCKET_NAME = "factseeker-faiss-db"
s3 = boto3.client("s3")

def list_faiss_keys_from_s3(s3_prefix):
    """ì§€ì •ëœ prefixì—ì„œ .faiss íŒŒì¼ í‚¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=s3_prefix)
    keys = []
    for page in page_iterator:
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".faiss"):
                keys.append(obj["Key"])
    return keys

def map_query_to_partition(query_vector, num_partitions=10):
    """ì„ë² ë”© ë²¡í„° ê¸°ë°˜ íŒŒí‹°ì…˜ ê²°ì • (ë‹¨ìˆœ í•´ì‹œ ê¸°ë°˜)"""
    return int(sum(query_vector) * 1000) % num_partitions

def get_partition_prefix_from_query(query_vector):
    partition_num = map_query_to_partition(query_vector)
    return f"feature_faiss_db_openai_partition/partition_{partition_num}/"

async def preload_faiss_from_existing_s3(s3_prefix):
    """ì§€ì •ëœ S3 prefix í•˜ìœ„ì— ì¡´ì¬í•˜ëŠ” ì¸ë±ìŠ¤ë“¤ë§Œ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ"""
    if s3_prefix.startswith("article_faiss_cache"):
        logging.info("ğŸ›‘ ë³¸ë¬¸ ì¸ë±ìŠ¤ëŠ” í”„ë¦¬ë¡œë“œí•˜ì§€ ì•ŠìŒ")
        return

    os.makedirs(CHUNK_CACHE_DIR, exist_ok=True)
    logging.info(f"ğŸš€ S3 FAISS ì¸ë±ìŠ¤ í”„ë¦¬ë¡œë“œ ì‹œì‘ (prefix={s3_prefix})")

    faiss_keys = list_faiss_keys_from_s3(s3_prefix)
    logging.info(f"ğŸ”¢ í”„ë¦¬ë¡œë“œ ëŒ€ìƒ ì¸ë±ìŠ¤ ê°œìˆ˜: {len(faiss_keys)}ê°œ")

    for faiss_key in faiss_keys:
        # S3 í‚¤ì—ì„œ í´ë” ì´ë¦„ (ì˜ˆ: 'partition_0')ì„ ì¶”ì¶œí•˜ì—¬ ë¡œì»¬ íŒŒì¼ ì´ë¦„ì˜ ì ‘ë‘ì‚¬ë¡œ ì‚¬ìš©
        dir_name = os.path.basename(os.path.dirname(faiss_key))
        
        # S3ì—ì„œ pkl íŒŒì¼ì˜ í‚¤ë¥¼ ì˜¬ë°”ë¥´ê²Œ ìƒì„±
        pkl_key = os.path.join(os.path.dirname(faiss_key), "index.pkl")
        
        # ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ìƒì„± (ì˜ˆ: '.../cache/partition_0.faiss')
        faiss_path = os.path.join(CHUNK_CACHE_DIR, f"{dir_name}.faiss")
        pkl_path = os.path.join(CHUNK_CACHE_DIR, f"{dir_name}.pkl")

        start = time.time()
        # S3 í‚¤ë¥¼ faiss_keyë¡œ, ë¡œì»¬ ê²½ë¡œë¥¼ faiss_pathë¡œ ì§€ì •
        faiss_ok = download_from_s3_if_exists(faiss_key, faiss_path)
        # S3 í‚¤ë¥¼ pkl_keyë¡œ, ë¡œì»¬ ê²½ë¡œë¥¼ pkl_pathë¡œ ì§€ì •
        pkl_ok = download_from_s3_if_exists(pkl_key, pkl_path)
        elapsed = time.time() - start

        if faiss_ok and pkl_ok:
            logging.info(f"âœ… í”„ë¦¬ë¡œë“œ ì™„ë£Œ: {dir_name} â±ï¸ {elapsed:.2f}ì´ˆ")
        else:
            logging.warning(f"âš ï¸ í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨: {dir_name}")