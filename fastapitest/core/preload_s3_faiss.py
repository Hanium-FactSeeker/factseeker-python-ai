import os
import time
import logging
import boto3
from core.faiss_manager import download_from_s3_if_exists, CHUNK_CACHE_DIR

# S3 ì„¤ì •
S3_BUCKET_NAME = "factseeker-faiss-db"
S3_PREFIX = "article_faiss_cache/"
s3 = boto3.client("s3")

def list_faiss_keys_from_s3():
    """S3ì—ì„œ .faiss íŒŒì¼ í‚¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=S3_PREFIX)
    keys = []
    for page in page_iterator:
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".faiss"):
                keys.append(obj["Key"])
    return keys


async def preload_faiss_from_existing_s3():
    """S3ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¸ë±ìŠ¤ë“¤ë§Œ ë¡œì»¬ë¡œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ"""
    os.makedirs(CHUNK_CACHE_DIR, exist_ok=True)
    logging.info("ğŸš€ S3ì— ì¡´ì¬í•˜ëŠ” FAISS ì¸ë±ìŠ¤ í”„ë¦¬ë¡œë“œ ì‹œì‘")

    faiss_keys = list_faiss_keys_from_s3()
    for faiss_key in faiss_keys:
        if not faiss_key.endswith(".faiss"):
            continue

        # ê²½ë¡œì—ì„œ í´ë” ì´ë¦„ ì¶”ì¶œ (í•´ì‹œê°’)
        dir_name = os.path.dirname(faiss_key).replace(S3_PREFIX, "").strip("/")
        pkl_key = f"{S3_PREFIX}{dir_name}/index.pkl"

        faiss_path = os.path.join(CHUNK_CACHE_DIR, f"{dir_name}.faiss")
        pkl_path = os.path.join(CHUNK_CACHE_DIR, f"{dir_name}.pkl")

        start = time.time()
        faiss_ok = download_from_s3_if_exists(faiss_key, faiss_path)
        pkl_ok = download_from_s3_if_exists(pkl_key, pkl_path)
        elapsed = time.time() - start

        if faiss_ok and pkl_ok:
            logging.info(f"âœ… í”„ë¦¬ë¡œë“œ ì™„ë£Œ: {dir_name} â±ï¸ {elapsed:.2f}ì´ˆ")
        else:
            logging.warning(f"âš ï¸ í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨: {dir_name}")