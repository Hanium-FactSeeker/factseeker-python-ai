import os
import time
import logging
import boto3
from botocore.exceptions import ClientError

# --- ì„¤ì • ---
# CHUNK_CACHE_DIRëŠ” main.pyì—ì„œë„ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
CHUNK_CACHE_DIR = "article_faiss_cache" 
S3_BUCKET_NAME = "factseeker-faiss-db"
s3 = boto3.client("s3")
# --- ì„¤ì • ë ---

def _download_s3_file(s3_key, local_path):
    """S3ì—ì„œ ë‹¨ì¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜"""
    try:
        if not os.path.exists(os.path.dirname(local_path)):
            os.makedirs(os.path.dirname(local_path))
        s3.download_file(S3_BUCKET_NAME, s3_key, local_path)
        return True
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            logging.warning(f"S3ì— íŒŒì¼ ì—†ìŒ: {s3_key}")
        else:
            logging.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {s3_key} -> {local_path} / error: {e}")
        return False

def _list_faiss_keys_from_s3(s3_prefix):
    """ì§€ì •ëœ prefixì—ì„œ .faiss íŒŒì¼ í‚¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    paginator = s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=s3_prefix)
    keys = []
    for page in page_iterator:
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".faiss"):
                keys.append(obj["Key"])
    return keys

def preload_faiss_from_existing_s3(s3_prefix):
    """
    ì§€ì •ëœ S3 prefix í•˜ìœ„ì˜ ëª¨ë“  FAISS íŒŒí‹°ì…˜ì„ ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ë¡œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # ë³¸ë¬¸ ìºì‹œëŠ” URL í•´ì‹œ ê¸°ë°˜ì´ë¼ í”„ë¦¬ë¡œë“œ ëŒ€ìƒì´ ì•„ë‹ˆë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
    if "article_faiss_cache" in s3_prefix:
        logging.info("ë³¸ë¬¸ ì¸ë±ìŠ¤(article_faiss_cache)ëŠ” í”„ë¦¬ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    os.makedirs(CHUNK_CACHE_DIR, exist_ok=True)
    logging.info(f"ğŸš€ S3 FAISS ì¸ë±ìŠ¤ í”„ë¦¬ë¡œë“œ ì‹œì‘ (prefix={s3_prefix})")

    faiss_keys = _list_faiss_keys_from_s3(s3_prefix)
    logging.info(f"ğŸ”¢ í”„ë¦¬ë¡œë“œ ëŒ€ìƒ FAISS íŒŒí‹°ì…˜ ê°œìˆ˜: {len(faiss_keys)}ê°œ")

    for faiss_key in faiss_keys:
        # S3 í‚¤ ì˜ˆì‹œ: 'feature_faiss_db_openai_partition/partition_0/index.faiss'
        # dir_nameì€ 'partition_0'ê³¼ ê°™ì€ íŒŒí‹°ì…˜ í´ë” ì´ë¦„ì´ ë©ë‹ˆë‹¤.
        dir_name = os.path.basename(os.path.dirname(faiss_key))
        pkl_key = os.path.join(os.path.dirname(faiss_key), "index.pkl")
        
        local_dir = os.path.join(CHUNK_CACHE_DIR, dir_name)
        
        # ì´ë¯¸ ë¡œì»¬ì— ìˆìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
        if os.path.exists(local_dir):
            logging.info(f"ì´ë¯¸ ì¡´ì¬í•¨, ê±´ë„ˆë›°ê¸°: {local_dir}")
            continue

        os.makedirs(local_dir, exist_ok=True)
        faiss_path = os.path.join(local_dir, "index.faiss")
        pkl_path = os.path.join(local_dir, "index.pkl")

        start = time.time()
        faiss_ok = _download_s3_file(faiss_key, faiss_path)
        pkl_ok = _download_s3_file(pkl_key, pkl_path)
        elapsed = time.time() - start

        if faiss_ok and pkl_ok:
            logging.info(f"âœ… í”„ë¦¬ë¡œë“œ ì™„ë£Œ: {dir_name} â±ï¸ {elapsed:.2f}ì´ˆ")
        else:
            logging.warning(f"âš ï¸ í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨: {dir_name}")

# ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°ë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    # ì‹¤ì œ ì œëª© FAISS íŒŒí‹°ì…˜ì´ ì €ì¥ëœ S3 ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    TITLE_FAISS_S3_PREFIX = "feature_faiss_db_openai_partition/"
    preload_faiss_from_existing_s3(TITLE_FAISS_S3_PREFIX)