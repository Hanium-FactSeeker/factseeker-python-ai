import os
import re
import asyncio
import json
import logging
import shutil
import hashlib
import numpy as np
import boto3
from botocore.exceptions import ClientError
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document

from core.lambdas import (
    extract_video_id,
    fetch_youtube_transcript,
    search_news_google_cs,
    get_article_text,
    clean_news_title,
    calculate_fact_check_confidence,
    calculate_source_diversity_score
)
from core.llm_chains import (
    build_claim_extractor,
    build_claim_summarizer,
    build_factcheck_chain,
    build_reduce_similar_claims_chain,
    build_channel_type_classifier,
    get_chat_llm
)
from core.faiss_manager import CHUNK_CACHE_DIR

# --- ì„¤ì •ê°’ ---
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
MAX_CLAIMS_TO_FACT_CHECK = 10
MAX_ARTICLES_PER_CLAIM = 10  # âœ¨ ì£¼ì¥ë‹¹ ìµœëŒ€ ê²€ìƒ‰ ê¸°ì‚¬ ìˆ˜ (ì´ ê°’ì„ ì¡°ì ˆí•˜ì„¸ìš”) âœ¨
DISTANCE_THRESHOLD = 0.8

# ë™ì‹œ ì²˜ë¦¬ ì œí•œ (ë³‘ëª© ì™„í™”)
MAX_CONCURRENT_CLAIMS = int(os.environ.get("MAX_CONCURRENT_CLAIMS", "3"))
MAX_CONCURRENT_FACTCHECKS = int(os.environ.get("MAX_CONCURRENT_FACTCHECKS", "3"))
MAX_EVIDENCES_PER_CLAIM = int(os.environ.get("MAX_EVIDENCES_PER_CLAIM", "10"))
# --- ì„¤ì •ê°’ ë ---

try:
    s3 = boto3.client('s3')
except Exception as e:
    s3 = None
    logging.error(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

embed_model = OpenAIEmbeddings(
    model="text-embedding-3-small", request_timeout=60, max_retries=5, chunk_size=500
)

# URLë³„ ë™ì‹œ ì²˜ë¦¬ë¥¼ ë§‰ê¸° ìœ„í•œ ì ê¸ˆ(Lock) ê°ì²´
url_locks = {}

# --- URL ê¸°ë°˜ ìºì‹œ ê²½ë¡œ ë° S3 ë™ê¸°í™” ---
def url_to_cache_key(url):
    return hashlib.md5(url.encode()).hexdigest()

def get_article_faiss_path(url):
    return os.path.join(CHUNK_CACHE_DIR, url_to_cache_key(url))

def upload_to_s3(local_dir, s3_key):
    if not s3:
        logging.warning("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    for root, _, files in os.walk(local_dir):
        for file in files:
            local_path = os.path.join(root, file)
            s3_path = os.path.join(s3_key, file)
            try:
                s3.upload_file(local_path, S3_BUCKET_NAME, s3_path)
            except ClientError as e:
                logging.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {local_path} -> s3://{S3_BUCKET_NAME}/{s3_path} - {e}")
                raise

def download_from_s3(local_dir, s3_key):
    if not s3:
        logging.warning("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    try:
        response = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=s3_key)
        if 'Contents' not in response:
            return False
        os.makedirs(local_dir, exist_ok=True)
        for obj in response['Contents']:
            s3_path = obj['Key']
            relative_path = os.path.relpath(s3_path, s3_key)
            local_path = os.path.join(local_dir, relative_path)
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            s3.download_file(S3_BUCKET_NAME, s3_path, local_path)
        return True
    except ClientError as e:
        if e.response['Error']['Code'] == '404':
            return False
        logging.error(f"S3 ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: s3://{S3_BUCKET_NAME}/{s3_key} - {e}")
        return False

# --- ê¸°ì‚¬ URL ê¸°ì¤€ FAISS ìƒì„±/ë¡œë“œ (ì ê¸ˆ ë¡œì§ ì ìš©) ---
async def ensure_article_faiss(url):
    """(ì ê¸ˆ ê¸°ëŠ¥ ì¶”ê°€) ê¸°ì‚¬ ë³¸ë¬¸ì„ ë²¡í„°í™”í•˜ê³  ìºì‹œ(S3/ë¡œì»¬)ì— ì €ì¥/ë¡œë“œ"""
    lock = url_locks.setdefault(url, asyncio.Lock())
    async with lock:
        cache_key = url_to_cache_key(url)
        local_path = get_article_faiss_path(url)
        faiss_path = os.path.join(local_path, "index.faiss")
        pkl_path = os.path.join(local_path, "index.pkl")
        s3_key = f"article_faiss_cache/{cache_key}"

        if os.path.exists(faiss_path) and os.path.exists(pkl_path):
            try:
                logging.info(f"ìºì‹œ ì¬ì‚¬ìš© (ì ê¸ˆ í›„ í™•ì¸): {url}")
                return FAISS.load_local(local_path, embed_model, allow_dangerous_deserialization=True)
            except Exception as e:
                shutil.rmtree(local_path, ignore_errors=True)
                logging.warning(f"ë¡œì»¬ ìºì‹œ ì†ìƒ, ì¬ìƒì„± ì‹œë„: {e}")
        
        if s3 is not None and download_from_s3(local_path, s3_key):
            try:
                logging.info(f"S3 ìºì‹œ ì¬ì‚¬ìš© (ì ê¸ˆ í›„ í™•ì¸): {url}")
                return FAISS.load_local(local_path, embed_model, allow_dangerous_deserialization=True)
            except Exception as e:
                shutil.rmtree(local_path, ignore_errors=True)
                logging.warning(f"S3 ìºì‹œ ì†ìƒ, ì¬ìƒì„± ì‹œë„: {e}")
        
        logging.info(f"ìºì‹œ ì—†ìŒ, ì‹ ê·œ í¬ë¡¤ë§ ì‹œì‘: {url}")
        text = await get_article_text(url)
        if not text or len(text) < 200:
            return None
            
        doc = Document(page_content=text, metadata={"url": url})
        faiss_db = FAISS.from_documents([doc], embed_model)
        faiss_db.save_local(local_path)
        
        if s3 is not None:
            try:
                upload_to_s3(local_path, s3_key)
                logging.info(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_key}")
            except Exception as e:
                logging.warning(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        return faiss_db

# --- CSE â†’ FAISSì—ì„œ ì—¬ëŸ¬ ê¸°ì‚¬, url ê¸°ì¤€ ì¤‘ë³µ ì—†ëŠ” ë¬¸ì„œë§Œ ìˆ˜ì§‘ (í•œë„ ì¦‰ì‹œ ì¤‘ë‹¨) ---
async def search_and_retrieve_docs_once(claim, faiss_partition_dirs, seen_urls):
    summarizer = build_claim_summarizer()
    try:
        summary_result = await summarizer.ainvoke({"claim": claim})
        summarized_query = summary_result.content.strip()
        logging.info(f"ğŸ” ìš”ì•½ëœ ê²€ìƒ‰ì–´: '{summarized_query}'")
    except Exception as e:
        logging.error(f"Claim ìš”ì•½ ì‹¤íŒ¨: {e}, ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰ ì§„í–‰")
        summarized_query = claim

    search_results = await search_news_google_cs(summarized_query)
    # CSE ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©
    cse_titles = [clean_news_title(item.get('title', '')) for item in search_results[:10]]
    cse_raw_titles = [item.get('title', '') for item in search_results[:10]]
    cse_urls = [item.get('link') for item in search_results[:10]]

    if not cse_titles:
        logging.warning("Google ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íƒìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return []

    # ì„ë² ë”© í˜¸ì¶œ ì•ˆì •í™”: ì†Œê·œëª¨ ì¬ì‹œë„/ì˜ˆì™¸ ë³´í˜¸
    def _embed_docs_with_retry(texts, retries=1):
        delay = 0.5
        for attempt in range(retries + 1):
            try:
                return embed_model.embed_documents(texts)
            except Exception as e:
                logging.warning(f"ì„ë² ë”© ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{retries}): {e}")
                if attempt < retries:
                    import time as _t
                    _t.sleep(delay)
                    delay *= 2
                else:
                    return []

    cse_title_embs = _embed_docs_with_retry(cse_titles, retries=1)
    search_vectors = np.array(cse_title_embs, dtype=np.float32)
    # CSE ê²°ê³¼ ìˆœì„œë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ CSE ì¸ë±ìŠ¤ë³„ í›„ë³´ë“¤ì„ ìˆ˜ì§‘
    candidates_by_cse = {}

    # íŒŒí‹°ì…˜ ë””ë ‰í„°ë¦¬ì˜ ìˆ«ìê°€ í´ìˆ˜ë¡ ìš°ì„  (ìµœì‹ )
    def partition_num(path: str) -> int:
        import os, re
        base = os.path.basename(path)
        m = re.search(r'(\d+)', base)
        return int(m.group(1)) if m else -1

    for faiss_dir in sorted(faiss_partition_dirs, key=partition_num, reverse=True):
        try:
            title_faiss_db = FAISS.load_local(
                faiss_dir, embeddings=embed_model, allow_dangerous_deserialization=True
            )
            if title_faiss_db.index.ntotal == 0:
                continue

            D, I = title_faiss_db.index.search(search_vectors, k=3)
            for j in range(len(cse_title_embs)):
                for i, dist in enumerate(D[j]):
                    if dist < DISTANCE_THRESHOLD:
                        faiss_idx = I[j][i]
                        docstore_id = title_faiss_db.index_to_docstore_id[faiss_idx]
                        doc = title_faiss_db.docstore._dict[docstore_id]
                        url = doc.metadata.get("url")
                        if url and url not in seen_urls:
                            lst = candidates_by_cse.setdefault(j, [])
                            lst.append({
                                "url": url,
                                "dist": float(dist),
                                "cse_idx": j,
                                "matched_cse_title": cse_titles[j],
                                "raw_cse_title": cse_raw_titles[j]
                            })
        except Exception as e:
            logging.error(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {faiss_dir} â†’ {e}")

    # CSE ê²€ìƒ‰ ê²°ê³¼ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¥´ë˜, ê° CSE ì¸ë±ìŠ¤ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ URL í•˜ë‚˜ë§Œ ì„ íƒ
    article_urls = []
    matched_meta = {}
    seen_tmp = set()
    for j in range(len(cse_title_embs)):
        lst = candidates_by_cse.get(j, [])
        if not lst:
            continue
        # ê°™ì€ URLì´ ì—¬ëŸ¬ íŒŒí‹°ì…˜ì—ì„œ ë°œê²¬ë  ìˆ˜ ìˆìœ¼ë‹ˆ URLë³„ ìµœì†Ÿê°’ë§Œ ìœ ì§€í•˜ê³ , ì´ë¯¸ ì„ íƒëœ URLì€ ì œì™¸
        best_per_url = {}
        for cand in lst:
            u = cand["url"]
            if u in seen_tmp:
                continue
            prev = best_per_url.get(u)
            if (prev is None) or (cand["dist"] < prev["dist"]):
                best_per_url[u] = cand
        if not best_per_url:
            continue
        chosen = min(best_per_url.values(), key=lambda c: c["dist"])
        u = chosen["url"]
        article_urls.append(u)
        matched_meta[u] = {
            "matched_cse_title": chosen["matched_cse_title"],
            "raw_cse_title": chosen["raw_cse_title"]
        }
        seen_tmp.add(u)

    # Fallback: CSE ê¸°ë°˜ ë§¤ì¹­ì´ í•œ ê±´ë„ ì—†ìœ¼ë©´, ìš”ì•½ í‚¤ì›Œë“œ ìì²´ë¡œ ì œëª© FAISSë¥¼ ì§ì ‘ ê²€ìƒ‰
    if not article_urls:
        try:
            logging.info("CSE ê¸°ë°˜ ë§¤ì¹­ ê²°ê³¼ ì—†ìŒ â†’ í‚¤ì›Œë“œ ì§ì ‘ FAISS ê²€ìƒ‰ ì‹œë„")
            # ì„ë² ë”© ì¿¼ë¦¬ë„ ì¬ì‹œë„/ì˜ˆì™¸ ë³´í˜¸
            def _embed_query_with_retry(q, retries=1):
                delay = 0.5
                for attempt in range(retries + 1):
                    try:
                        return embed_model.embed_query(q)
                    except Exception as e:
                        logging.warning(f"ì„ë² ë”© ì¿¼ë¦¬ ì‹¤íŒ¨(ì¬ì‹œë„ {attempt}/{retries}): {e}")
                        if attempt < retries:
                            import time as _t
                            _t.sleep(delay)
                            delay *= 2
                        else:
                            return None

            emb = _embed_query_with_retry(summarized_query, retries=1)
            if emb is None:
                return []
            query_vec = emb
            query_np = np.array([query_vec], dtype=np.float32)
            fallback = {}

            # ìµœì‹  íŒŒí‹°ì…˜ ìš°ì„ 
            for faiss_dir in sorted(faiss_partition_dirs, key=partition_num, reverse=True):
                try:
                    title_faiss_db = FAISS.load_local(
                        faiss_dir, embeddings=embed_model, allow_dangerous_deserialization=True
                    )
                    if title_faiss_db.index.ntotal == 0:
                        continue
                    D, I = title_faiss_db.index.search(query_np, k=5)
                    for i, dist in enumerate(D[0]):
                        if dist < DISTANCE_THRESHOLD:
                            faiss_idx = I[0][i]
                            docstore_id = title_faiss_db.index_to_docstore_id[faiss_idx]
                            doc = title_faiss_db.docstore._dict[docstore_id]
                            url = doc.metadata.get("url")
                            if not url:
                                continue
                            cur = fallback.get(url)
                            if (cur is None) or (dist < cur["dist"]):
                                fallback[url] = {"dist": float(dist)}
                except Exception as e:
                    logging.error(f"FAISS í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {faiss_dir} â†’ {e}")

            if fallback:
                # ê±°ë¦¬ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒí•œ ì ìš©
                article_urls = [u for u, _ in sorted(fallback.items(), key=lambda kv: kv[1]["dist"])][:MAX_ARTICLES_PER_CLAIM]
                # ë©”íƒ€ë°ì´í„°ì—ëŠ” ìš”ì•½ ì§ˆì˜ë¥¼ ê¸°ë¡
                for u in article_urls:
                    matched_meta[u] = {
                        "matched_cse_title": summarized_query,
                        "raw_cse_title": summarized_query,
                    }
        except Exception as e:
            logging.error(f"í‚¤ì›Œë“œ ì§ì ‘ FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    docs = []

    for url in article_urls:
        faiss_db = await ensure_article_faiss(url)
        if faiss_db:
            for doc in faiss_db.docstore._dict.values():
                actual_url = doc.metadata.get("url")
                if actual_url and actual_url not in seen_urls:
                    meta = matched_meta.get(actual_url, {"matched_cse_title": "", "raw_cse_title": ""})
                    docs.append(Document(
                        page_content=doc.page_content,
                        metadata={
                            "url": actual_url,
                            "matched_cse_title": meta["matched_cse_title"],
                            "raw_cse_title": meta["raw_cse_title"]
                        }
                    ))
                    break
    return docs

# --- ë³¸ë¬¸ í™•ë³´ëœ ë‰´ìŠ¤ 15ê°œê°€ ë  ë•Œê¹Œì§€ ë°˜ë³µ í™•ë³´ ---
async def search_and_retrieve_docs(claim, faiss_partition_dirs):
    collected_docs = []
    seen_urls = set()
    attempt_count = 0

    while len(collected_docs) < MAX_ARTICLES_PER_CLAIM:
        attempt_count += 1
        logging.info(f"ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œë„ {attempt_count}íšŒ - í™•ë³´ëœ ê¸°ì‚¬ ìˆ˜: {len(collected_docs)}")

        new_docs = await search_and_retrieve_docs_once(claim, faiss_partition_dirs, seen_urls)

        if not new_docs:
            logging.warning(f"ğŸ“­ ìˆ˜ì§‘ëœ ë¬¸ì„œ ì—†ìŒ. ë°˜ë³µ ì§„í–‰ ì¤‘... (í˜„ì¬ í™•ë³´: {len(collected_docs)})")

        for doc in new_docs:
            url = doc.metadata.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                collected_docs.append(doc)
                logging.info(f"âœ… ê¸°ì‚¬ í™•ë³´: {url} ({len(collected_docs)}/{MAX_ARTICLES_PER_CLAIM})")

        if attempt_count > 30 and len(collected_docs) < MAX_ARTICLES_PER_CLAIM:
            logging.error("ğŸš¨ 30íšŒ ì´ìƒ ë°˜ë³µí–ˆì§€ë§Œ 15ê°œ í™•ë³´ ì‹¤íŒ¨. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

    logging.info(f"ğŸ“° ìµœì¢… í™•ë³´ëœ ë¬¸ì„œ ìˆ˜: {len(collected_docs)}ê°œ")
    return collected_docs[:MAX_ARTICLES_PER_CLAIM]

    article_urls = list(matched_urls.keys())
    coros = [ensure_article_faiss(url) for url in article_urls]
    faiss_dbs = await asyncio.gather(*coros)
    docs = []
    for faiss_db in faiss_dbs:
        if faiss_db:
            doc_list = [doc for doc in faiss_db.docstore._dict.values()]
            for doc in doc_list:
                url = doc.metadata.get("url")
                if url and url in matched_urls:
                    docs.append(Document(
                        page_content=doc.page_content,
                        metadata={
                            "url": url,
                            "matched_cse_title": matched_urls[url]["matched_cse_title"],
                            "raw_cse_title": matched_urls[url]["raw_cse_title"]
                        }
                    ))
    logging.info(f"ğŸ“° ìµœì¢… í¬ë¡¤ë§ ë° ìºì‹± ì„±ê³µ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    return docs

async def run_fact_check(youtube_url, faiss_partition_dirs):
    video_id = extract_video_id(youtube_url)
    if not video_id:
        return {"error": "Invalid YouTube URL"}

    logging.info(f"ìœ íŠœë¸Œ ë¶„ì„ ì‹œì‘: {youtube_url}")
    try:
        transcript = fetch_youtube_transcript(youtube_url)
        if not transcript:
            return {"error": "Failed to load transcript"}

        extractor = build_claim_extractor()
        result = await extractor.ainvoke({"transcript": transcript})
        claims = [line.strip() for line in result.content.strip().split('\n') if line.strip()]

        if not claims:
            return {
                "video_id": video_id, "video_url": youtube_url,
                "video_total_confidence_score": 0, "claims": []
            }

        reducer = build_reduce_similar_claims_chain()
        claims_json = json.dumps(claims, ensure_ascii=False, indent=2)
        reduced_result = await reducer.ainvoke({"claims_json": claims_json})

        claims_to_check = []
        try:
            json_match = re.search(r"```json\s*(\[.*?\])\s*```", reduced_result.content, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
                claims_to_check = json.loads(json_content)
            else:
                claims_to_check = [line.strip() for line in reduced_result.content.strip().split('\n') if line.strip()]
        except json.JSONDecodeError:
            claims_to_check = [
                line.strip() for line in reduced_result.content.strip().split('\n')
                if line.strip() and not line.strip().startswith(('```json', '```', '[', ']'))
            ]

        claims_to_check = claims_to_check[:MAX_CLAIMS_TO_FACT_CHECK]
        logging.info(f"âœ‚ï¸ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ì£¼ì¥ {len(claims_to_check)}ê°œ: {claims_to_check}")

        if not claims_to_check:
            return {
                "video_id": video_id, "video_url": youtube_url,
                "video_total_confidence_score": 0, "claims": []
            }

    except Exception as e:
        logging.exception(f"ì£¼ì¥ ì¶”ì¶œ/ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"Failed to extract claims: {e}"}

    # ì£¼ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë‹¨ê³„ë¥¼ ì •ì˜í•˜ê³ , ì£¼ì¥ ë‹¨ìœ„ ë™ì‹œì„±ì€ ì™¸ë¶€ ì„¸ë§ˆí¬ì–´ë¡œ ì œì–´í•©ë‹ˆë‹¤.
    async def process_claim_step(idx, claim):
        logging.info(f"--- íŒ©íŠ¸ì²´í¬ ì‹œì‘: ({idx + 1}/{len(claims_to_check)}) '{claim}'")

        url_set = set()  # ê°™ì€ URLì—ì„œ ì¤‘ë³µ ì¦ê±° ë°©ì§€
        validated_evidence = []
        fact_checker = build_factcheck_chain()

        async def factcheck_doc(doc):
            try:
                check_result = await fact_checker.ainvoke({"claim": claim, "context": doc.page_content})
                result_content = check_result.content
                relevance = re.search(r"ê´€ë ¨ì„±: (.+)", result_content)
                fact_check_result_match = re.search(r"ì‚¬ì‹¤ ì„¤ëª… ì—¬ë¶€: (.+)", result_content)
                justification = re.search(r"ê°„ë‹¨í•œ ì„¤ëª…: (.+)", result_content)
                snippet = re.search(r"í•µì‹¬ ê·¼ê±° ë¬¸ì¥: (.+)", result_content)
                url = doc.metadata.get("url")

                if (
                    relevance and fact_check_result_match and justification
                    and "ì˜ˆ" in relevance.group(1)
                    and url and url not in url_set
                ):
                    url_set.add(url)
                    return {
                        "url": url, "relevance": "yes",
                        "fact_check_result": fact_check_result_match.group(1).strip(),
                        "justification": justification.group(1).strip(),
                        "snippet": snippet.group(1).strip() if snippet else ""
                    }
            except Exception as e:
                logging.error(f"    - LLM íŒ©íŠ¸ì²´í¬ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

        # per-claim ë¬¸ì„œ íŒ©íŠ¸ì²´í¬ ë™ì‹œì„± ì œí•œ
        factcheck_semaphore = asyncio.Semaphore(MAX_CONCURRENT_FACTCHECKS)

        async def limited_factcheck_doc(doc):
            async with factcheck_semaphore:
                return await factcheck_doc(doc)

        # CSE ê²€ìƒ‰ì€ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ê³ , í•´ë‹¹ ê²°ê³¼ì—ì„œ ë§¤ì¹­ëœ ë¬¸ì„œë§Œ ë°°ì¹˜ë¡œ íŒ©íŠ¸ì²´í¬
        new_docs = await search_and_retrieve_docs_once(claim, faiss_partition_dirs, set())
        if not new_docs:
            logging.info(f"ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨: '{claim}'")
            return {
                "claim": claim, "result": "insufficient_evidence",
                "confidence_score": 0, "evidence": []
            }

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì¡°ê¸° ì¢…ë£Œ ê°€ëŠ¥í•˜ê²Œ í•¨
        for i in range(0, len(new_docs), MAX_CONCURRENT_FACTCHECKS):
            if len(validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                break
            batch = new_docs[i:i+MAX_CONCURRENT_FACTCHECKS]
            factcheck_tasks = [limited_factcheck_doc(doc) for doc in batch]
            factcheck_results = await asyncio.gather(*factcheck_tasks)
            for res in factcheck_results:
                if res:
                    validated_evidence.append(res)
                    if len(validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                        break

        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        diversity_score = calculate_source_diversity_score(validated_evidence)
        confidence_score = calculate_fact_check_confidence({
            "source_diversity": diversity_score,
            "evidence_count": min(len(validated_evidence), 5)
        })

        logging.info(f"--- íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: '{claim}' -> ì‹ ë¢°ë„: {confidence_score}% (ì¦ê±° {len(validated_evidence)}ê°œ)")

        return {
            "claim": claim,
            "result": "likely_true" if validated_evidence else "insufficient_evidence",
            "confidence_score": confidence_score,
            "evidence": validated_evidence[:3]
        }

    # ì£¼ì¥ ë‹¨ìœ„ ë™ì‹œì„± ì œí•œ (ìµœëŒ€ 3ê°œ ë™ì‹œ ì²˜ë¦¬)
    claim_semaphore = asyncio.Semaphore(MAX_CONCURRENT_CLAIMS)

    async def limited_process_claim(idx, claim):
        async with claim_semaphore:
            return await process_claim_step(idx, claim)

    claim_tasks = [limited_process_claim(idx, claim) for idx, claim in enumerate(claims_to_check)]
    gathered = await asyncio.gather(*claim_tasks, return_exceptions=True)

    # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì£¼ì¥ì„ ëˆ„ë½í•˜ì§€ ì•Šë„ë¡ ì—ëŸ¬ í•­ëª©ìœ¼ë¡œ ê¸°ë¡
    outputs = []
    for i, result in enumerate(gathered):
        if isinstance(result, Exception):
            claim_text = claims_to_check[i] if i < len(claims_to_check) else ""
            # ì˜ˆì™¸ íƒ€ì…ê³¼ ìƒì„¸ ë©”ì‹œì§€ë¥¼ í•¨ê»˜ ê¸°ë¡í•´ ê³µë°± ë©”ì‹œì§€ ë¬¸ì œ ë°©ì§€
            err_type = type(result).__name__
            err_msg = str(result) or repr(result) or err_type
            logging.error(f"ğŸ›‘ ì£¼ì¥ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: '{claim_text}' -> {err_type}: {err_msg}")
            outputs.append({
                "claim": claim_text,
                "result": "error",
                "confidence_score": 0,
                "evidence": [],
                "error": f"{err_type}: {err_msg}",
                "error_type": err_type,
                "error_stage": "process_claim_step"
            })
        else:
            outputs.append(result)

    if outputs:
        avg_score = round(sum(o['confidence_score'] for o in outputs) / len(outputs))
        evidence_ratio = sum(1 for o in outputs if o["result"] == "likely_true") / len(outputs)
        summary = f"ì¦ê±° í™•ë³´ëœ ì£¼ì¥ ë¹„ìœ¨: {evidence_ratio*100:.1f}%" if len(outputs) >= 3 else f"ì‹ ë¢°ë„ í‰ê°€ ë¶ˆê°€ (íŒ©íŠ¸ì²´í¬ ì£¼ì¥ ìˆ˜ ë¶€ì¡±: {len(outputs)}ê°œ)"
    else:
        avg_score = 0
        summary = "ê²°ê³¼ ì—†ìŒ"
    
    classifier = build_channel_type_classifier()
    classification = await classifier.ainvoke({"transcript": transcript})
    channel_type, reason = parse_channel_type(classification.content)

    return {
        "video_id": video_id,
        "video_url": youtube_url,
        "video_total_confidence_score": avg_score,
        "claims": outputs,
        "summary": summary,
        "channel_type": channel_type,
        "channel_type_reason": reason
    }

def parse_channel_type(llm_output: str):
    channel_type_match = re.search(r"ì±„ë„ ìœ í˜•:\s*(.+)", llm_output)
    reason_match = re.search(r"ë¶„ë¥˜ ê·¼ê±°:\s*(.+)", llm_output)
    channel_type = channel_type_match.group(1).strip() if channel_type_match else "ì•Œ ìˆ˜ ì—†ìŒ"
    reason = reason_match.group(1).strip() if reason_match else "LLM ì‘ë‹µì—ì„œ íŒë‹¨ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return channel_type, reason
