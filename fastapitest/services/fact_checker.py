# core/fact_checker.py
import os
import re
import json
import asyncio
import logging
import shutil
import hashlib
import numpy as np
from typing import List, Dict, Any, Optional

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document

# í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ì¶˜ ì„í¬íŠ¸
from core.lambdas import (
    extract_video_id,
    fetch_youtube_transcript,
    search_news_google_cs,
    get_article_text,
    clean_news_title,
    calculate_fact_check_confidence,
    calculate_source_diversity_score,
)
from core.llm_chains import (
    build_claim_extractor,
    build_claim_summarizer,
    build_factcheck_chain,
    build_reduce_similar_claims_chain,
    build_channel_type_classifier,
)
from core.faiss_manager import CHUNK_CACHE_DIR

logger = logging.getLogger(__name__)

# --- ì„¤ì •ê°’ ---
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
MAX_CLAIMS_TO_FACT_CHECK = int(os.environ.get("MAX_CLAIMS_TO_FACT_CHECK", 10))
MAX_ARTICLES_PER_CLAIM = int(os.environ.get("MAX_ARTICLES_PER_CLAIM", 10))
DISTANCE_THRESHOLD = float(os.environ.get("DISTANCE_THRESHOLD", 0.8))
# --- ë ---

# ì„ë² ë”© ëª¨ë¸
embed_model = OpenAIEmbeddings(
    model="text-embedding-3-small", request_timeout=60, max_retries=5, chunk_size=500
)

# URLë³„ ë™ì‹œ ì²˜ë¦¬ ë°©ì§€
_url_locks: Dict[str, asyncio.Lock] = {}

# --- URL ê¸°ë°˜ ìºì‹œ ê²½ë¡œ ---
def _url_to_cache_key(url: str) -> str:
    return hashlib.md5(url.encode()).hexdigest()

def _article_faiss_path(url: str) -> str:
    return os.path.join(CHUNK_CACHE_DIR, _url_to_cache_key(url))

# S3ëŠ” ì„ íƒ(ì—¬ê¸°ì„  ë¡œì»¬ë§Œ)
def _upload_to_s3(local_dir: str, s3_key: str) -> None:
    # ì™¸ë¶€ í™˜ê²½ ì˜ì¡´ ì œê±°: ì‹¤ì œ ìš´ì˜ì—ì„œë§Œ ì‚¬ìš©í•˜ë„ë¡ ì£¼ì„/No-op
    return

def _download_from_s3(local_dir: str, s3_key: str) -> bool:
    return False

# --- ê¸°ì‚¬ URL ê¸°ì¤€ FAISS ìƒì„±/ë¡œë“œ (Lock) ---
async def _ensure_article_faiss(url: str) -> Optional[FAISS]:
    lock = _url_locks.setdefault(url, asyncio.Lock())
    async with lock:
        local_path = _article_faiss_path(url)
        faiss_path = os.path.join(local_path, "index.faiss")
        pkl_path = os.path.join(local_path, "index.pkl")
        s3_key = f"article_faiss_cache/{_url_to_cache_key(url)}"

        if os.path.exists(faiss_path) and os.path.exists(pkl_path):
            try:
                logger.info(f"ìºì‹œ ì¬ì‚¬ìš© (ì ê¸ˆ í›„ í™•ì¸): {url}")
                return FAISS.load_local(local_path, embed_model, allow_dangerous_deserialization=True)
            except Exception as e:
                shutil.rmtree(local_path, ignore_errors=True)
                logger.warning(f"ë¡œì»¬ ìºì‹œ ì†ìƒ, ì¬ìƒì„± ì‹œë„: {e}")

        if _download_from_s3(local_path, s3_key):
            try:
                logger.info(f"S3 ìºì‹œ ì¬ì‚¬ìš© (ì ê¸ˆ í›„ í™•ì¸): {url}")
                return FAISS.load_local(local_path, embed_model, allow_dangerous_deserialization=True)
            except Exception as e:
                shutil.rmtree(local_path, ignore_errors=True)
                logger.warning(f"S3 ìºì‹œ ì†ìƒ, ì¬ìƒì„± ì‹œë„: {e}")

        logger.info(f"ìºì‹œ ì—†ìŒ, ì‹ ê·œ í¬ë¡¤ë§ ì‹œì‘: {url}")
        text = await get_article_text(url)
        if not text or len(text) < 200:
            return None

        doc = Document(page_content=text, metadata={"url": url})
        faiss_db = FAISS.from_documents([doc], embed_model)
        os.makedirs(local_path, exist_ok=True)
        faiss_db.save_local(local_path)

        try:
            _upload_to_s3(local_path, s3_key)
            logger.info(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {s3_key}")
        except Exception as e:
            logger.warning(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")

        return faiss_db

# --- CSE ê²€ìƒ‰ â†’ FAISSì—ì„œ ì œëª© ê·¼ì ‘ê²€ìƒ‰ìœ¼ë¡œ URL ë§¤ì¹­ ---
async def _search_and_retrieve_docs_once(
    claim: str,
    faiss_partition_dirs: List[str],
    seen_urls: set
) -> List[Document]:
    summarizer = build_claim_summarizer()
    try:
        summary_result = await summarizer.ainvoke({"claim": claim})
        summarized_query = (summary_result.content or "").strip() or claim
        logger.info(f"ğŸ” ìš”ì•½ëœ ê²€ìƒ‰ì–´: '{summarized_query}'")
    except Exception as e:
        logger.error(f"Claim ìš”ì•½ ì‹¤íŒ¨: {e}, ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰ ì§„í–‰")
        summarized_query = claim

    search_results = await search_news_google_cs(summarized_query)
    cse_titles = [clean_news_title(item.get('title', '') or "") for item in search_results[:20]]
    cse_raw_titles = [item.get('title', '') or "" for item in search_results[:20]]
    cse_urls = [item.get('link') for item in search_results[:20]]

    if not cse_titles:
        logger.warning("Google ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íƒìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return []

    cse_title_embs = embed_model.embed_documents(cse_titles)
    search_vectors = np.array(cse_title_embs, dtype=np.float32)
    matched_urls: Dict[str, Dict[str, str]] = {}

    for faiss_dir in faiss_partition_dirs:
        try:
            title_faiss_db = FAISS.load_local(
                faiss_dir, embeddings=embed_model, allow_dangerous_deserialization=True
            )
            if getattr(title_faiss_db.index, "ntotal", 0) == 0:
                continue

            D, I = title_faiss_db.index.search(search_vectors, k=3)
            for j in range(len(cse_title_embs)):
                for i, dist in enumerate(D[j]):
                    if dist < DISTANCE_THRESHOLD:
                        faiss_idx = I[j][i]
                        docstore_id = title_faiss_db.index_to_docstore_id[faiss_idx]
                        doc = title_faiss_db.docstore._dict[docstore_id]
                        url = doc.metadata.get("url")
                        if url and url not in seen_urls and url not in matched_urls:
                            matched_urls[url] = {
                                "matched_cse_title": cse_titles[j],
                                "raw_cse_title": cse_raw_titles[j]
                            }
        except Exception as e:
            logger.error(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {faiss_dir} â†’ {e}")

    docs: List[Document] = []
    for url in list(matched_urls.keys()):
        faiss_db = await _ensure_article_faiss(url)
        if faiss_db:
            # ë‹¨ì¼ ë¬¸ì„œë§Œ ì‚¬ìš©
            for d in faiss_db.docstore._dict.values():
                actual_url = d.metadata.get("url")
                if actual_url and actual_url not in seen_urls:
                    docs.append(Document(
                        page_content=d.page_content,
                        metadata={
                            "url": actual_url,
                            "matched_cse_title": matched_urls[actual_url]["matched_cse_title"],
                            "raw_cse_title": matched_urls[actual_url]["raw_cse_title"]
                        }
                    ))
                    break
    return docs


async def _search_and_retrieve_docs(claim: str, faiss_partition_dirs: List[str]) -> List[Document]:
    collected_docs: List[Document] = []
    seen_urls: set = set()
    attempt = 0

    while len(collected_docs) < MAX_ARTICLES_PER_CLAIM:
        attempt += 1
        logger.info(f"ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œë„ {attempt}íšŒ - í™•ë³´ëœ ê¸°ì‚¬ ìˆ˜: {len(collected_docs)}")
        new_docs = await _search_and_retrieve_docs_once(claim, faiss_partition_dirs, seen_urls)
        if not new_docs:
            logger.warning(f"ğŸ“­ ìˆ˜ì§‘ëœ ë¬¸ì„œ ì—†ìŒ. ë°˜ë³µ ì§„í–‰ ì¤‘... (í˜„ì¬ í™•ë³´: {len(collected_docs)})")

        for doc in new_docs:
            url = doc.metadata.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                collected_docs.append(doc)
                logger.info(f"âœ… ê¸°ì‚¬ í™•ë³´: {url} ({len(collected_docs)}/{MAX_ARTICLES_PER_CLAIM})")

        if attempt > 30:
            logger.error("ğŸš¨ 30íšŒ ì´ìƒ ë°˜ë³µí–ˆì§€ë§Œ ëª©í‘œ ê°œìˆ˜ í™•ë³´ ì‹¤íŒ¨. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

    logger.info(f"ğŸ“° ìµœì¢… í™•ë³´ëœ ë¬¸ì„œ ìˆ˜: {len(collected_docs)}ê°œ")
    return collected_docs[:MAX_ARTICLES_PER_CLAIM]


def _parse_channel_type(llm_output: str):
    channel_type_match = re.search(r"ì±„ë„ ìœ í˜•:\s*(.+)", llm_output or "")
    reason_match = re.search(r"ë¶„ë¥˜ ê·¼ê±°:\s*(.+)", llm_output or "")
    channel_type = channel_type_match.group(1).strip() if channel_type_match else "ì•Œ ìˆ˜ ì—†ìŒ"
    reason = reason_match.group(1).strip() if reason_match else "LLM ì‘ë‹µì—ì„œ íŒë‹¨ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return channel_type, reason


# ------------------------------------------------------------------------------
# ì™¸ë¶€ ì§„ì…ì 
# ------------------------------------------------------------------------------
async def run_fact_check(youtube_url: str, faiss_partition_dirs: List[str]) -> Dict[str, Any]:
    video_id = extract_video_id(youtube_url)
    if not video_id:
        return {"error": "Invalid YouTube URL"}

    logger.info(f"ìœ íŠœë¸Œ ë¶„ì„ ì‹œì‘: {youtube_url}")
    try:
        transcript = fetch_youtube_transcript(youtube_url)
        if not transcript:
            return {"error": "Failed to load transcript"}

        extractor = build_claim_extractor()
        result = await extractor.ainvoke({"transcript": transcript})
        claims = [line.strip() for line in (result.content or "").strip().split("\n") if line.strip()]

        if not claims:
            return {
                "video_id": video_id, "video_url": youtube_url,
                "video_total_confidence_score": 0, "claims": []
            }

        reducer = build_reduce_similar_claims_chain()
        claims_json = json.dumps(claims, ensure_ascii=False, indent=2)
        reduced_result = await reducer.ainvoke({"claims_json": claims_json})

        claims_to_check: List[str] = []
        try:
            json_match = re.search(r"```json\s*(\[.*?\])\s*```", reduced_result.content or "", re.DOTALL)
            if json_match:
                claims_to_check = json.loads(json_match.group(1))
            else:
                claims_to_check = [line.strip() for line in (reduced_result.content or "").strip().split("\n") if line.strip()]
        except json.JSONDecodeError:
            claims_to_check = [
                line.strip() for line in (reduced_result.content or "").strip().split("\n")
                if line.strip() and not line.strip().startswith(("```json", "```", "[", "]"))
            ]

        claims_to_check = claims_to_check[:MAX_CLAIMS_TO_FACT_CHECK]
        logger.info(f"âœ‚ï¸ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ì£¼ì¥ {len(claims_to_check)}ê°œ: {claims_to_check}")

        if not claims_to_check:
            return {
                "video_id": video_id, "video_url": youtube_url,
                "video_total_confidence_score": 0, "claims": []
            }

    except Exception as e:
        logger.exception(f"ì£¼ì¥ ì¶”ì¶œ/ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"Failed to extract claims: {e}"}

    fact_checker = build_factcheck_chain()

    async def process_claim_step(idx: int, claim: str) -> Dict[str, Any]:
        try:
            logger.info(f"--- íŒ©íŠ¸ì²´í¬ ì‹œì‘: ({idx + 1}/{len(claims_to_check)}) '{claim}'")
            docs = await _search_and_retrieve_docs(claim, faiss_partition_dirs)
            if not docs:
                logger.info(f"ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨: '{claim}'")
                return {
                    "claim": claim, "result": "insufficient_evidence",
                    "confidence_score": 0, "evidence": []
                }

            url_set = set()

            async def factcheck_doc(doc: Document) -> Optional[Dict[str, Any]]:
                try:
                    check_result = await fact_checker.ainvoke({"claim": claim, "context": doc.page_content})
                    result_content = check_result.content or ""
                    relevance = re.search(r"ê´€ë ¨ì„±:\s*(.+)", result_content)
                    fact_check_result_match = re.search(r"ì‚¬ì‹¤ ì„¤ëª… ì—¬ë¶€:\s*(.+)", result_content)
                    justification = re.search(r"ê°„ë‹¨í•œ ì„¤ëª…:\s*(.+)", result_content)
                    snippet = re.search(r"í•µì‹¬ ê·¼ê±° ë¬¸ì¥:\s*(.+)", result_content)
                    url = doc.metadata.get("url")

                    # "ê´€ë ¨ì„±: ì˜ˆ" + URL ì¤‘ë³µ ì œê±°ë§Œ í†µê³¼
                    if (
                        relevance and "ì˜ˆ" in relevance.group(1)
                        and fact_check_result_match and justification
                        and url and url not in url_set
                    ):
                        url_set.add(url)
                        return {
                            "url": url,
                            "relevance": "yes",
                            "fact_check_result": fact_check_result_match.group(1).strip(),
                            "justification": justification.group(1).strip(),
                            "snippet": (snippet.group(1).strip() if snippet else "")
                        }
                except Exception as e:
                    logger.error(f"    - LLM íŒ©íŠ¸ì²´í¬ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                return None

            tasks = [factcheck_doc(doc) for doc in docs]
            factcheck_results = await asyncio.gather(*tasks, return_exceptions=True)

            validated: List[Dict[str, Any]] = []
            for r in factcheck_results:
                if isinstance(r, Exception):
                    logger.error(f"íŒ©íŠ¸ì²´í¬ task ì˜ˆì™¸: {r}")
                    continue
                if r:
                    validated.append(r)

            # ì•ˆì „í•œ ì ìˆ˜ ê³„ì‚° (ì˜ˆì™¸ ë°©ì§€)
            try:
                diversity_score = calculate_source_diversity_score(validated)
            except Exception as e:
                logger.exception("diversity score ê³„ì‚° ì‹¤íŒ¨")
                diversity_score = 0

            try:
                confidence_score = calculate_fact_check_confidence({
                    "source_diversity": diversity_score,
                    "evidence_count": min(len(validated), 5)
                })
            except Exception as e:
                logger.exception("confidence ê³„ì‚° ì‹¤íŒ¨")
                confidence_score = 0

            logger.info(f"--- íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: '{claim}' -> ì‹ ë¢°ë„: {confidence_score}%")

            return {
                "claim": claim,
                "result": "likely_true" if validated else "insufficient_evidence",
                "confidence_score": confidence_score,
                "evidence": validated[:3]
            }
        except Exception as e:
            logger.exception(f"process_claim_step ì˜ˆì™¸: {e}")
            # ì ˆëŒ€ ê²°ê³¼ê°€ ì¦ë°œí•˜ì§€ ì•Šë„ë¡ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "claim": claim,
                "result": "insufficient_evidence",
                "confidence_score": 0,
                "evidence": []
            }

    claim_tasks = [process_claim_step(i, c) for i, c in enumerate(claims_to_check)]
    raw_outputs = await asyncio.gather(*claim_tasks, return_exceptions=True)

    outputs: List[Dict[str, Any]] = []
    for o in raw_outputs:
        if isinstance(o, Exception):
            logger.exception(f"claim task ì˜ˆì™¸(ê¸°ë³¸ê°’ ëŒ€ì²´): {o}")
            outputs.append({
                "claim": "(unknown)",
                "result": "insufficient_evidence",
                "confidence_score": 0,
                "evidence": []
            })
        else:
            outputs.append(o)

    if outputs:
        try:
            avg_score = round(sum(o.get('confidence_score', 0) for o in outputs) / len(outputs))
        except Exception:
            avg_score = 0
        try:
            evidence_ratio = sum(1 for o in outputs if o.get("result") == "likely_true") / len(outputs)
            summary = f"ì¦ê±° í™•ë³´ëœ ì£¼ì¥ ë¹„ìœ¨: {evidence_ratio*100:.1f}%" if len(outputs) >= 3 else f"ì‹ ë¢°ë„ í‰ê°€ ë¶ˆê°€ (íŒ©íŠ¸ì²´í¬ ì£¼ì¥ ìˆ˜ ë¶€ì¡±: {len(outputs)}ê°œ)"
        except Exception:
            summary = "ê²°ê³¼ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
    else:
        avg_score = 0
        summary = "ê²°ê³¼ ì—†ìŒ"

    # ì±„ë„ ìœ í˜• ë¶„ë¥˜(ì˜ˆì™¸ ë³´í˜¸)
    channel_type = "ì•Œ ìˆ˜ ì—†ìŒ"
    reason = "LLM ì‘ë‹µì—ì„œ íŒë‹¨ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    try:
        classifier = build_channel_type_classifier()
        classification = await classifier.ainvoke({"transcript": transcript})
        channel_type, reason = _parse_channel_type(classification.content or "")
    except Exception as e:
        logger.warning(f"ì±„ë„ ìœ í˜• ë¶„ë¥˜ ì‹¤íŒ¨: {e}")

    return {
        "video_id": video_id,
        "video_url": youtube_url,
        "video_total_confidence_score": avg_score,
        "claims": outputs,
        "summary": summary,
        "channel_type": channel_type,
        "channel_type_reason": reason
    }
