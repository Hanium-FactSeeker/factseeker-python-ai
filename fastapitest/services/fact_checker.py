import os
import re
import asyncio
import json
import logging
import shutil
import hashlib
import numpy as np
import boto3
from botocore.exceptions import ClientError
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document

from core.lambdas import (
    extract_video_id,
    fetch_youtube_transcript,
    search_news_google_cs,
    get_article_text,
    clean_news_title,
    calculate_fact_check_confidence,
    calculate_source_diversity_score
)
from core.llm_chains import (
    build_claim_extractor,
    build_claim_summarizer,
    build_factcheck_chain,
    build_reduce_similar_claims_chain,
    build_channel_type_classifier,
    get_chat_llm
)
from core.faiss_manager import CHUNK_CACHE_DIR

# --- âœ¨âœ¨âœ¨ S3 ì„¤ì • ì¶”ê°€ âœ¨âœ¨âœ¨ ---
S3_BUCKET_NAME = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
try:
    s3 = boto3.client('s3')
except Exception as e:
    s3 = None
    logging.error(f"S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
# --- âœ¨âœ¨âœ¨ S3 ì„¤ì • ì¢…ë£Œ âœ¨âœ¨âœ¨ ---


MAX_CLAIMS_TO_FACT_CHECK = 10
DISTANCE_THRESHOLD = 0.8  # L2 ê±°ë¦¬ ì„ê³„ê°’

embed_model = OpenAIEmbeddings(model="text-embedding-3-small", request_timeout=60, max_retries=5, chunk_size=500)

# --- âœ¨âœ¨âœ¨ S3 ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ì¶”ê°€ âœ¨âœ¨âœ¨ ---
def upload_to_s3(local_dir, s3_key):
    if not s3:
        logging.warning("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    for root, _, files in os.walk(local_dir):
        for file in files:
            local_path = os.path.join(root, file)
            # s3_keyëŠ” ë””ë ‰í† ë¦¬ì²˜ëŸ¼ ì‘ë™í•˜ë¯€ë¡œ íŒŒì¼ ì´ë¦„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            s3_path = os.path.join(s3_key, file)
            try:
                s3.upload_file(local_path, S3_BUCKET_NAME, s3_path)
            except ClientError as e:
                logging.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {local_path} -> s3://{S3_BUCKET_NAME}/{s3_path} - {e}")
                raise

def download_from_s3(local_dir, s3_key):
    if not s3:
        logging.warning("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    try:
        # S3 í‚¤(í´ë”) ì•„ë˜ì˜ ëª¨ë“  ê°ì²´ë¥¼ ë‚˜ì—´í•©ë‹ˆë‹¤.
        response = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=s3_key)
        if 'Contents' not in response:
            return False # S3ì— íŒŒì¼ì´ ì—†ìŒ

        os.makedirs(local_dir, exist_ok=True)
        for obj in response['Contents']:
            s3_path = obj['Key']
            # s3_key prefixë¥¼ ì œê±°í•˜ì—¬ ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ë§Œë“­ë‹ˆë‹¤.
            relative_path = os.path.relpath(s3_path, s3_key)
            local_path = os.path.join(local_dir, relative_path)
            
            # íŒŒì¼ì´ ìœ„ì¹˜í•  ë¡œì»¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            s3.download_file(S3_BUCKET_NAME, s3_path, local_path)
        return True
    except ClientError as e:
        # 404 (Not Found)ëŠ” íŒŒì¼ì´ ì—†ëŠ” ê²ƒì´ë¯€ë¡œ ì˜¤ë¥˜ê°€ ì•„ë‹™ë‹ˆë‹¤.
        if e.response['Error']['Code'] == '404':
            return False
        logging.error(f"S3 ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: s3://{S3_BUCKET_NAME}/{s3_key} - {e}")
        return False
# --- âœ¨âœ¨âœ¨ S3 í•¨ìˆ˜ ì¢…ë£Œ âœ¨âœ¨âœ¨ ---


async def get_article_text_safe(url):
    """ê¸°ì‚¬ ë³¸ë¬¸ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œ ì˜ˆì™¸ ì•ˆì „ ë˜í¼"""
    try:
        text = await get_article_text(url)
        return url, text
    except Exception as e:
        logging.warning(f"âŒ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
        return url, None

async def search_and_retrieve_docs(claim, faiss_partition_dirs):
    summarizer = build_claim_summarizer()
    try:
        summary_result = await summarizer.ainvoke({"claim": claim})
        summarized_query = summary_result.content.strip()
        logging.info(f"ğŸ” ìš”ì•½ëœ ê²€ìƒ‰ì–´: '{summarized_query}'")
    except Exception as e:
        logging.error(f"Claim ìš”ì•½ ì‹¤íŒ¨: {e}, ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰ ì§„í–‰")
        summarized_query = claim

    search_results = await search_news_google_cs(summarized_query)

    cse_titles = [clean_news_title(item.get('title', '')) for item in search_results[:10]]
    cse_raw_titles = [item.get('title', '') for item in search_results[:10]]
    cse_urls = [item.get('link') for item in search_results[:10]]
    cse_title_embs = embed_model.embed_documents(cse_titles)

    matched_urls = {}
    for idx, faiss_dir in enumerate(faiss_partition_dirs):
        faiss_index_path = os.path.join(faiss_dir, "index.faiss")
        faiss_pkl_path = os.path.join(faiss_dir, "index.pkl")
        if not (os.path.exists(faiss_index_path) and os.path.exists(faiss_pkl_path)):
            continue
        try:
            title_faiss_db = FAISS.load_local(
                faiss_dir,
                embeddings=embed_model,
                allow_dangerous_deserialization=True
            )
            D, I = title_faiss_db.index.search(np.array(cse_title_embs, dtype=np.float32), k=1)
            for j, dist in enumerate(D.flatten()):
                if dist < DISTANCE_THRESHOLD:
                    faiss_idx = I.flatten()[j]
                    docstore_id = title_faiss_db.index_to_docstore_id[faiss_idx]
                    doc = title_faiss_db.docstore._dict[docstore_id]
                    url = doc.metadata.get("url")
                    if url and url not in matched_urls:
                        matched_urls[url] = {
                            "matched_cse_title": cse_titles[j],
                            "raw_cse_title": cse_raw_titles[j]
                        }
        except Exception as e:
            logging.error(f"FAISS íŒŒí‹°ì…˜ {faiss_dir} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    article_urls = list(matched_urls.keys())
    coros = [get_article_text_safe(url) for url in article_urls]
    article_results = await asyncio.gather(*coros)
    docs = []
    for url, article_text in article_results:
        meta = matched_urls[url]
        if article_text and len(article_text) > 200:
            docs.append(Document(
                page_content=article_text,
                metadata={
                    "url": url,
                    "matched_cse_title": meta["matched_cse_title"],
                    "raw_cse_title": meta["raw_cse_title"]
                }
            ))
    logging.info(f"ğŸ“° ìµœì¢… í¬ë¡¤ë§ ì„±ê³µ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    return docs

async def run_fact_check(youtube_url, faiss_partition_dirs):
    video_id = extract_video_id(youtube_url)
    if not video_id:
        return {"error": "Invalid YouTube URL"}

    logging.info(f"ìœ íŠœë¸Œ ë¶„ì„ ì‹œì‘: {youtube_url}")
    try:
        transcript = fetch_youtube_transcript(youtube_url)
        if not transcript:
            return {"error": "Failed to load transcript"}

        extractor = build_claim_extractor()
        result = await extractor.ainvoke({"transcript": transcript})
        claims = [line.strip() for line in result.content.strip().split('\n') if line.strip()]

        if not claims:
            return {
                "video_id": video_id,
                "video_url": youtube_url,
                "video_total_confidence_score": 0,
                "claims": []
            }

        reducer = build_reduce_similar_claims_chain()
        claims_json = json.dumps(claims, ensure_ascii=False, indent=2)
        reduced_result = await reducer.ainvoke({"claims_json": claims_json})

        claims_to_check = []
        try:
            json_match = re.search(r"```json\s*(\[.*?\])\s*```", reduced_result.content, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
                claims_to_check = json.loads(json_content)
            else:
                claims_to_check = [
                    line.strip() for line in reduced_result.content.strip().split('\n')
                    if line.strip()
                ]
        except json.JSONDecodeError:
            claims_to_check = [
                line.strip() for line in reduced_result.content.strip().split('\n')
                if line.strip() and not line.strip().startswith(('```json', '```', '[', ']'))
            ]

        claims_to_check = claims_to_check[:MAX_CLAIMS_TO_FACT_CHECK]
        logging.info(f"âœ‚ï¸ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ì£¼ì¥ {len(claims_to_check)}ê°œ: {claims_to_check}")

        if not claims_to_check:
             return {
                "video_id": video_id,
                "video_url": youtube_url,
                "video_total_confidence_score": 0,
                "claims": []
            }

    except Exception as e:
        logging.exception(f"ì£¼ì¥ ì¶”ì¶œ/ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"Failed to extract claims: {e}"}

    async def process_claim_step(idx, claim):
        logging.info(f"--- íŒ©íŠ¸ì²´í¬ ì‹œì‘: ({idx + 1}/{len(claims_to_check)}) '{claim}'")

        # --- âœ¨âœ¨âœ¨ FAISS ìƒì„±/ë¡œë“œ ë¡œì§ ìˆ˜ì • âœ¨âœ¨âœ¨ ---
        claim_hash = hashlib.md5(claim.encode()).hexdigest()
        s3_key = f"claim_faiss_cache/{claim_hash}"
        local_faiss_path = os.path.join(CHUNK_CACHE_DIR, s3_key)
        
        faiss_db = None
        docs = None

        # 1. ë¡œì»¬ ìºì‹œ í™•ì¸
        if os.path.exists(local_faiss_path):
            try:
                faiss_db = FAISS.load_local(local_faiss_path, embed_model, allow_dangerous_deserialization=True)
                docs = [doc for doc in faiss_db.docstore._dict.values()]
                logging.info(f"âœ… ë¡œì»¬ ìºì‹œì—ì„œ FAISS DB ë¡œë“œ ì„±ê³µ: {local_faiss_path}")
            except Exception as e:
                logging.warning(f"âš ï¸ ë¡œì»¬ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬ìƒì„± ì‹œë„: {e}")
                shutil.rmtree(local_faiss_path)

        # 2. S3 ìºì‹œ í™•ì¸
        if not faiss_db:
            logging.info(f"S3 ìºì‹œ í™•ì¸ ì¤‘: s3://{S3_BUCKET_NAME}/{s3_key}")
            if download_from_s3(local_faiss_path, s3_key):
                try:
                    faiss_db = FAISS.load_local(local_faiss_path, embed_model, allow_dangerous_deserialization=True)
                    docs = [doc for doc in faiss_db.docstore._dict.values()]
                    logging.info(f"âœ… S3 ìºì‹œì—ì„œ FAISS DB ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì„±ê³µ")
                except Exception as e:
                    logging.warning(f"âš ï¸ S3 ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì¬ìƒì„± ì‹œë„: {e}")
                    shutil.rmtree(local_faiss_path)
            else:
                logging.info(f"S3ì— ìºì‹œ ì—†ìŒ. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

        # 3. ìƒˆë¡œ ìƒì„± ë° S3ì— ì—…ë¡œë“œ
        if not faiss_db:
            docs = await search_and_retrieve_docs(claim, faiss_partition_dirs)
            if not docs:
                logging.info(f"ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨: '{claim}'")
                return {
                    "claim": claim, "result": "insufficient_evidence",
                    "confidence_score": 0, "evidence": []
                }
            
            os.makedirs(local_faiss_path, exist_ok=True)
            faiss_db = FAISS.from_documents(docs, embed_model)
            faiss_db.save_local(local_faiss_path)
            logging.info(f"âœ… FAISS DB ìƒˆë¡œ ìƒì„± ë° ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {local_faiss_path}")
            
            try:
                logging.info(f"ğŸš€ S3ì— FAISS ì¸ë±ìŠ¤ ì—…ë¡œë“œ ì‹œë„: s3://{S3_BUCKET_NAME}/{s3_key}")
                upload_to_s3(local_faiss_path, s3_key)
                logging.info(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ.")
            except Exception as e:
                logging.error(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        if not docs: # ìºì‹œì—ì„œ ë¡œë“œí•œ ê²½ìš° docsê°€ Noneì´ë¯€ë¡œ ìƒˆë¡œ ì±„ì›Œì•¼ í•¨
            docs = [doc for doc_id, doc in faiss_db.docstore._dict.items()]
        # --- âœ¨âœ¨âœ¨ ë¡œì§ ìˆ˜ì • ì¢…ë£Œ âœ¨âœ¨âœ¨ ---

        validated_evidence = []
        fact_checker = build_factcheck_chain()

        async def factcheck_doc(doc):
            try:
                check_result = await fact_checker.ainvoke({
                    "claim": claim, "context": doc.page_content
                })
                result_content = check_result.content
                relevance = re.search(r"ê´€ë ¨ì„±: (.+)", result_content)
                fact_check_result_match = re.search(r"ì‚¬ì‹¤ ì„¤ëª… ì—¬ë¶€: (.+)", result_content)
                justification = re.search(r"ê°„ë‹¨í•œ ì„¤ëª…: (.+)", result_content)
                snippet = re.search(r"í•µì‹¬ ê·¼ê±° ë¬¸ì¥: (.+)", result_content)

                if relevance and fact_check_result_match and justification:
                    fact_check_result_text = fact_check_result_match.group(1).strip()
                    if "ì˜ˆ" in relevance.group(1) and "ì•„ë‹ˆì˜¤" not in fact_check_result_text:
                        return {
                            "url": doc.metadata.get("url"),
                            "relevance": "yes",
                            "fact_check_result": fact_check_result_text,
                            "justification": justification.group(1).strip(),
                            "snippet": snippet.group(1).strip() if snippet else ""
                        }
            except Exception as e:
                logging.error(f"    - LLM íŒ©íŠ¸ì²´í¬ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

        factcheck_tasks = [factcheck_doc(doc) for doc in docs]
        factcheck_results = await asyncio.gather(*factcheck_tasks)
        validated_evidence = [res for res in factcheck_results if res]

        diversity_score = calculate_source_diversity_score(validated_evidence)
        confidence_score = calculate_fact_check_confidence({
            "source_diversity": diversity_score,
            "evidence_count": min(len(validated_evidence), 5)
        })

        logging.info(f"--- íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: '{claim}' -> ì‹ ë¢°ë„: {confidence_score}%")

        return {
            "claim": claim,
            "result": "likely_true" if validated_evidence else "insufficient_evidence",
            "confidence_score": confidence_score,
            "evidence": validated_evidence[:3]
        }

    claim_tasks = [
        process_claim_step(idx, claim)
        for idx, claim in enumerate(claims_to_check)
    ]
    outputs = await asyncio.gather(*claim_tasks, return_exceptions=True)
    outputs = [output for output in outputs if not isinstance(output, Exception)]

    avg_score = round(sum(o['confidence_score'] for o in outputs) / len(outputs)) if outputs else 0
    evidence_ratio = sum(1 for o in outputs if o["result"] == "likely_true") / len(outputs) if outputs else 0
    summary = f"ì¦ê±° í™•ë³´ëœ ì£¼ì¥ ë¹„ìœ¨: {evidence_ratio*100:.1f}%" if len(outputs) >= 3 else f"ì‹ ë¢°ë„ í‰ê°€ ë¶ˆê°€ (íŒ©íŠ¸ì²´í¬ ì£¼ì¥ ìˆ˜ ë¶€ì¡±: {len(outputs)}ê°œ)"
    
    classifier = build_channel_type_classifier()
    classification = await classifier.ainvoke({"transcript": transcript})
    channel_type, reason = parse_channel_type(classification.content)

    return {
        "video_id": video_id,
        "video_url": youtube_url,
        "video_total_confidence_score": avg_score,
        "claims": outputs,
        "summary": summary,
        "channel_type": channel_type,
        "channel_type_reason": reason
    }


def parse_channel_type(llm_output: str):
    channel_type_match = re.search(r"ì±„ë„ ìœ í˜•:\s*(.+)", llm_output)
    reason_match = re.search(r"ë¶„ë¥˜ ê·¼ê±°:\s*(.+)", llm_output)
    channel_type = channel_type_match.group(1).strip() if channel_type_match else "ì•Œ ìˆ˜ ì—†ìŒ"
    reason = reason_match.group(1).strip() if reason_match else "LLM ì‘ë‹µì—ì„œ íŒë‹¨ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return channel_type, reason