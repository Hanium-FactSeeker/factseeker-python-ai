import os
import re
import asyncio
import json
import logging
import shutil
from urllib.parse import urlparse, urlunparse
import boto3 # boto3 ì„í¬íŠ¸ ì¶”ê°€

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document

# core.lambdas ë° core.llm_chains, core.faiss_managerëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ë‹¤ê³  ê°€ì •
from core.lambdas import (
    extract_video_id,
    fetch_youtube_transcript,
    search_news_google_cs,
    get_article_text,
    clean_news_title,
    calculate_fact_check_confidence,
    calculate_source_diversity_score
)
from core.llm_chains import (
    build_claim_extractor,
    build_claim_summarizer,
    build_factcheck_chain,
    build_reduce_similar_claims_chain,
    build_channel_type_classifier,
    get_chat_llm
)
# FAISS_DB_PATH, CHUNK_CACHE_DIR ë“±ì€ main.pyë‚˜ ê°œë³„ ê¸°ì‚¬ FAISSì™€ ê´€ë ¨
from core.faiss_manager import get_or_build_faiss, CHUNK_CACHE_DIR 

MAX_CLAIMS_TO_FACT_CHECK = 10
embed_model = OpenAIEmbeddings(model="text-embedding-3-small", request_timeout=60, max_retries=5, chunk_size=500)

async def search_and_retrieve_docs(claim):
    """
    ì£¼ì¥(claim)ì— ëŒ€í•´ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê´€ë ¨ì„± ìˆëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì•„ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    search_results = await search_news_google_cs(claim)
    
    docs = []
    retrieved_urls = set()
    
    for item in search_results:
        url = item.get("link")
        source_title = item.get("title")
        snippet = item.get("snippet")
        
        if not url or url in retrieved_urls:
            continue
        
        try:
            # ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìºì‹œ í¬í•¨)
            article_text = await get_or_build_faiss(
                url=url, 
                embed_model=embed_model,
            )
            
            if article_text and len(article_text) > 200:
                # ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                doc = Document(
                    page_content=article_text,
                    metadata={
                        "source_title": source_title,
                        "url": url,
                        "snippet": snippet
                    }
                )
                docs.append(doc)
                retrieved_urls.add(url)
        except Exception as e:
            logging.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")
            continue

    return docs


async def run_fact_check(youtube_url):
    """
    ì£¼ì–´ì§„ ìœ íŠœë¸Œ URLì— ëŒ€í•œ íŒ©íŠ¸ì²´í¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    video_id = extract_video_id(youtube_url)
    if not video_id:
        return {"error": "Invalid YouTube URL"}

    logging.info(f"ìœ íŠœë¸Œ ë¶„ì„ ì‹œì‘: {youtube_url}")

    try:
        # 0. ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
        transcript = fetch_youtube_transcript(youtube_url)
        if not transcript:
            return {"error": "Failed to load transcript"}
        
        # 1. íŒ©íŠ¸ì²´í¬ ê°€ëŠ¥í•œ ì£¼ì¥ ì¶”ì¶œ
        extractor = build_claim_extractor()
        result = await extractor.ainvoke({"transcript": transcript})
        # LangChain AIMessage ê°ì²´ì—ì„œ content ì†ì„±ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
        claims = [line.strip() for line in result.content.strip().split('\n') if line.strip()]
        
        # ì£¼ì¥ì´ ì—†ëŠ” ê²½ìš°
        if not claims:
            return {
                "video_id": video_id,
                "video_total_confidence_score": 0,
                "claims": []
            }
        
        logging.info(f"ğŸ” íŒ©íŠ¸ì²´í¬ë¥¼ ìœ„í•œ ì£¼ì¥ {len(claims)}ê°œ ì¶”ì¶œ ì™„ë£Œ: {claims}")
        
        # 2. ìœ ì‚¬í•˜ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” ì£¼ì¥ ì œê±°
        reducer = build_reduce_similar_claims_chain()
        reduced_result = await reducer.ainvoke({"claims": claims})
        # LangChain AIMessage ê°ì²´ì—ì„œ content ì†ì„±ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
        claims_to_check = [
            line.strip() for line in reduced_result.content.strip().split('\n')
            if line.strip()
        ][:MAX_CLAIMS_TO_FACT_CHECK]
        
        logging.info(f"âœ‚ï¸ ì¤‘ë³µ ì œê±° í›„ ìµœì¢… íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ì£¼ì¥ {len(claims_to_check)}ê°œ: {claims_to_check}")

        if not claims_to_check:
            return {
                "video_id": video_id,
                "video_total_confidence_score": 0,
                "claims": []
            }
        
    except Exception as e:
        logging.exception(f"ì£¼ì¥ ì¶”ì¶œ/ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"Failed to extract claims: {e}"}

    # 3. ê° ì£¼ì¥ì— ëŒ€í•´ íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
    async def process_claim_step(claim_tuple):
        idx, claim = claim_tuple
        logging.info(f"--- íŒ©íŠ¸ì²´í¬ ì‹œì‘: ({idx + 1}/{len(claims_to_check)}) '{claim}'")
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¬¸ì„œ ê²€ìƒ‰
        docs = await search_and_retrieve_docs(claim)
        if not docs:
            logging.info(f"ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í•¨: '{claim}'")
            return {
                "claim": claim,
                "result": "insufficient_evidence",
                "confidence_score": 0,
                "evidence": []
            }

        faiss_db_temp_path = os.path.join(CHUNK_CACHE_DIR, f"temp_faiss_{video_id}_{idx}")
        if os.path.exists(faiss_db_temp_path):
            shutil.rmtree(faiss_db_temp_path)

        # ë¬¸ì„œë“¤ì„ FAISS ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥
        faiss_db = FAISS.from_documents(docs, embed_model)
        faiss_db.save_local(faiss_db_temp_path)
        logging.info(f"âœ… ê¸°ì‚¬ ë¬¸ì„œ {len(docs)}ê°œë¡œ ì„ì‹œ FAISS DB ìƒì„± ì™„ë£Œ")
        
        # ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
        retriever = faiss_db.as_retriever(search_kwargs={"k": 5})
        validated_evidence = []
        
        # íŒ©íŠ¸ì²´í¬ ì²´ì¸ ì‹¤í–‰
        fact_checker = build_factcheck_chain()
        for i, doc in enumerate(docs):
            try:
                check_result = await fact_checker.ainvoke({
                    "claim": claim,
                    "context": doc.page_content
                })
                # LangChain AIMessage ê°ì²´ì—ì„œ content ì†ì„±ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
                result_content = check_result.content

                # LLM ê²°ê³¼ íŒŒì‹±
                relevance = re.search(r"ê´€ë ¨ì„±: (.+)", result_content)
                fact_check_result = re.search(r"ì‚¬ì‹¤ ì—¬ë¶€: (.+)", result_content)
                justification = re.search(r"íŒë‹¨ ê·¼ê±°: (.+)", result_content)
                
                if relevance and fact_check_result and justification:
                    if "ì˜ˆ" in relevance.group(1):
                        validated_evidence.append({
                            "source_title": doc.metadata.get("source_title"),
                            "url": doc.metadata.get("url"),
                            "snippet": doc.metadata.get("snippet"),
                            "relevance": "yes",
                            "fact_check_result": fact_check_result.group(1).strip(),
                            "justification": justification.group(1).strip()
                        })
                        logging.info(f"    - ê·¼ê±° í™•ë³´ ({i+1}): {doc.metadata.get('source_title')}")
                else:
                    logging.warning(f"    - LLM ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ ({i+1}): {result_content}")
            except Exception as e:
                logging.error(f"    - LLM íŒ©íŠ¸ì²´í¬ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

        # ì„ì‹œ FAISS DB ì‚­ì œ
        if os.path.exists(faiss_db_temp_path):
            shutil.rmtree(faiss_db_temp_path)

        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        diversity_score = calculate_source_diversity_score(validated_evidence)
        confidence_score = calculate_fact_check_confidence({
            "source_diversity": diversity_score,
            "evidence_count": len(validated_evidence) if len(validated_evidence) <= 5 else 5
        })

        logging.info(f"--- íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: '{claim}' -> ì‹ ë¢°ë„: {confidence_score}%")

        return {
            "claim": claim,
            "result": "likely_true" if validated_evidence else "insufficient_evidence",
            "confidence_score": confidence_score,
            "evidence": validated_evidence[:3]
        }

    claim_tasks = [
        process_claim_step((idx, claim))
        for idx, claim in enumerate(claims_to_check)
    ]
    outputs = await asyncio.gather(*claim_tasks, return_exceptions=True)
    
    outputs = [output for output in outputs if not isinstance(output, Exception)]

    avg_score = round(sum(o['confidence_score'] for o in outputs) / len(outputs)) if outputs else 0
    evidence_ratio = sum(1 for o in outputs if o["result"] == "likely_true") / len(outputs) if outputs else 0
    # confidence_summary ë¬¸ìì—´ ìƒì„± ë¡œì§ ë³€ê²½
    if len(outputs) < 3:
        summary = f"ì‹ ë¢°ë„ í‰ê°€ ë¶ˆê°€ (íŒ©íŠ¸ì²´í¬ ì£¼ì¥ ìˆ˜ ë¶€ì¡±: {len(outputs)}ê°œ)"
    else:
        # ë°±ë¶„ìœ¨ í¬ë§·íŒ…
        evidence_ratio_percent = evidence_ratio * 100
        summary = f"ì¦ê±° í™•ë³´ëœ ì£¼ì¥ ë¹„ìœ¨: {evidence_ratio_percent:.1f}%"

    classifier = build_channel_type_classifier()
    classification = await classifier.ainvoke({"transcript": transcript})
    channel_type, reason = parse_channel_type(classification.content)

    return {
        "video_id": video_id,
        "video_total_confidence_score": avg_score,
        "claims": outputs,
        "summary": summary,
        "channel_type": channel_type,
        "channel_type_reason": reason
    }

def parse_channel_type(llm_output: str):
    """LLM ì¶œë ¥ì—ì„œ ì±„ë„ ìœ í˜•ê³¼ ì´ìœ ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    # ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì±„ë„ ìœ í˜•ê³¼ ì´ìœ ë¥¼ ì¶”ì¶œ
    channel_type_match = re.search(r"ì±„ë„ ìœ í˜•:\s*(.+)", llm_output)
    reason_match = re.search(r"íŒë‹¨ ê·¼ê±°:\s*(.+)", llm_output)

    channel_type = channel_type_match.group(1).strip() if channel_type_match else "ì•Œ ìˆ˜ ì—†ìŒ"
    reason = reason_match.group(1).strip() if reason_match else "LLM ì‘ë‹µì—ì„œ íŒë‹¨ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return channel_type, reason
