import os
import asyncio
import logging
from datetime import datetime
from zoneinfo import ZoneInfo
import boto3
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager
import contextlib
from dotenv import load_dotenv

# core í´ë”ì˜ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from services.fact_checker import run_fact_check
from core.preload_s3_faiss import preload_faiss_from_existing_s3, CHUNK_CACHE_DIR
from article_checker.router import create_router as create_article_router

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# --- ë°ì´í„° ëª¨ë¸ ì •ì˜ ---
class FactCheckRequest(BaseModel):
    youtube_url: str

# --- FAISS ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•œ ë³€ìˆ˜ ---
faiss_partition_dirs = []


def _kst_now() -> datetime:
    return datetime.now(ZoneInfo("Asia/Seoul"))


def _compute_month_kst() -> str:
    return _kst_now().strftime("%Y%m")


def _refresh_faiss_partition_dirs():
    """Rescan local cache for partition_* folders and refresh the global list."""
    if not os.path.exists(CHUNK_CACHE_DIR):
        return
    faiss_partition_dirs.clear()

    def partition_num(name: str) -> int:
        try:
            base = os.path.basename(name)
            num = int(''.join(ch for ch in base if ch.isdigit()))
            return num
        except Exception:
            return -1

    items = [
        os.path.join(CHUNK_CACHE_DIR, item)
        for item in os.listdir(CHUNK_CACHE_DIR)
        if os.path.isdir(os.path.join(CHUNK_CACHE_DIR, item)) and item.startswith("partition_")
    ]
    for item_path in sorted(items, key=partition_num, reverse=True):
        faiss_partition_dirs.append(item_path)


def _remove_local_partition(prefix: str):
    import shutil
    part = os.path.basename(os.path.dirname(prefix.rstrip('/')))
    local_dir = os.path.join(CHUNK_CACHE_DIR, part)
    if os.path.isdir(local_dir):
        shutil.rmtree(local_dir, ignore_errors=True)
        logging.info(f"ğŸ§¹ ë¡œì»¬ íŒŒí‹°ì…˜ ì‚­ì œ: {local_dir}")


async def _watch_titles_preload_task(base_prefix: str, poll_interval_sec: float = 120.0, include_partition_10: bool = True):
    """Background watcher: monitors S3 title partitions and re-preloads on change.

    - Watches monthly partition (Asia/Seoul). Optionally also partition_10.
    - On index.faiss change (with index.pkl present), removes local partition and re-preloads just that prefix.
    - Refreshes global faiss_partition_dirs after each reload.
    """
    s3 = boto3.client("s3")
    bucket = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
    seen: dict[str, str] = {}

    def prefixes_now() -> list[str]:
        ym = _compute_month_kst()
        monthly = f"{base_prefix.rstrip('/')}/partition_{ym}/"
        res = [monthly]
        if include_partition_10:
            res.append(f"{base_prefix.rstrip('/')}/partition_10/")
        return res

    def head(key: str):
        try:
            return s3.head_object(Bucket=bucket, Key=key)
        except Exception:
            return None

    while True:
        try:
            for prefix in prefixes_now():
                faiss_key = f"{prefix}index.faiss"
                pkl_key = f"{prefix}index.pkl"
                faiss_head = head(faiss_key)
                pkl_head = head(pkl_key)
                if not faiss_head or not pkl_head:
                    continue
                tag = f"{faiss_head.get('ETag')}_{faiss_head.get('LastModified').timestamp()}"
                if seen.get(prefix) != tag:
                    logging.info(f"ğŸ”” S3 ë³€ê²½ ê°ì§€: {faiss_key} â†’ ì œëª© í”„ë¦¬ë¡œë“œ ì¬ì‹¤í–‰")
                    _remove_local_partition(prefix)
                    # ê°•ì œ ì¬ë‹¤ìš´ë¡œë“œë¥¼ í†µí•´ ë¡œì»¬ ìºì‹œê°€ ë‚¨ì•„ ìˆì–´ë„ ìµœì‹ ìœ¼ë¡œ êµì²´
                    preload_faiss_from_existing_s3(prefix, force_reload=True)
                    _refresh_faiss_partition_dirs()
                    seen[prefix] = tag
        except Exception as e:
            logging.warning(f"ì œëª© í”„ë¦¬ë¡œë“œ ì›Œì²˜ ì˜¤ë¥˜(ê³„ì† ì§„í–‰): {e}")

        await asyncio.sleep(poll_interval_sec)

# --- FastAPI Lifespan Manager ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ì½”ë“œ
    logging.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘...")
    
    # ì œëª© FAISSê°€ ì €ì¥ëœ ì •í™•í•œ S3 ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    TITLE_FAISS_S3_PREFIX = "feature_faiss_db_openai_partition/"

    # ìµœì´ˆ 1íšŒ í”„ë¦¬ë¡œë“œ
    preload_faiss_from_existing_s3(TITLE_FAISS_S3_PREFIX)
    _refresh_faiss_partition_dirs()
    
    if faiss_partition_dirs:
        logging.info(f"âœ… {len(faiss_partition_dirs)}ê°œì˜ ì œëª© FAISS íŒŒí‹°ì…˜ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        logging.warning("âš ï¸ ë¡œë“œëœ ì œëª© FAISS íŒŒí‹°ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì œëª© ê¸°ë°˜ ê²€ìƒ‰ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    # ë°±ê·¸ë¼ìš´ë“œ ì›Œì²˜ ì‹œì‘ (S3 ë³€ê²½ ì‹œ ì œëª© í”„ë¦¬ë¡œë“œ ì¬ì‹¤í–‰)
    watch_enabled = os.environ.get("TITLE_PRELOAD_WATCH", "1") in ("1", "true", "TRUE", "yes", "YES")
    watch_interval = float(os.environ.get("TITLE_PRELOAD_WATCH_INTERVAL", "120"))
    include_p10 = os.environ.get("TITLE_PRELOAD_INCLUDE_P10", "1") in ("1", "true", "TRUE", "yes", "YES")
    watch_task = None
    if watch_enabled:
        logging.info(f"ğŸ•’ ì œëª© í”„ë¦¬ë¡œë“œ ê°ì‹œ ì‹œì‘(interval={watch_interval}s, include_p10={include_p10})")
        watch_task = asyncio.create_task(_watch_titles_preload_task(TITLE_FAISS_S3_PREFIX, watch_interval, include_p10))

    try:
        yield
    finally:
        if watch_task:
            watch_task.cancel()
            with contextlib.suppress(Exception):
                await watch_task
    # ì„œë²„ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë  ì½”ë“œ
    logging.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ...")

# --- FastAPI ì•± ìƒì„± ---
app = FastAPI(lifespan=lifespan)

# ê¸°ì‚¬ íŒ©íŠ¸ì²´í¬ ë¼ìš°í„° ì¶”ê°€ (ê¸°ì¡´ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
app.include_router(create_article_router(lambda: faiss_partition_dirs))

# --- API ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/fact-check")
async def fact_check_endpoint(request: FactCheckRequest):
    """
    ìœ íŠœë¸Œ URLì„ ë°›ì•„ íŒ©íŠ¸ì²´í¬ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not faiss_partition_dirs:
        raise HTTPException(
            status_code=503, 
            detail="ì„œë²„ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. FAISS íŒŒí‹°ì…˜ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
        
    try:
        result = await run_fact_check(
            youtube_url=request.youtube_url,
            faiss_partition_dirs=faiss_partition_dirs
        )
        if "error" in result:
            raise HTTPException(status_code=400, detail=result["error"])
        return result
    except Exception as e:
        logging.error(f"íŒ©íŠ¸ì²´í¬ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@app.get("/")
def read_root():
    return {"message": "FactSeeker AI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}
