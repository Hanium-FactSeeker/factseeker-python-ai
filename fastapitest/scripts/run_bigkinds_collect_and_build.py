import os
import re
import glob
import time
import shutil
import logging
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Tuple

import boto3
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.docstore.document import Document

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains


def _kst_now() -> datetime:
    return datetime.now(ZoneInfo("Asia/Seoul"))


def compute_partition_month_kst() -> str:
    return _kst_now().strftime("%Y%m")


def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def wait_for_download_complete(download_dir: str, timeout_sec: int = 600) -> str:
    """Wait until there is no .crdownload left and return the latest file path."""
    ensure_dir(download_dir)
    start = time.time()
    last_file = None
    while time.time() - start < timeout_sec:
        partials = glob.glob(os.path.join(download_dir, "*.crdownload"))
        if not partials:
            candidates = [p for p in glob.glob(os.path.join(download_dir, "*")) if os.path.isfile(p)]
            if candidates:
                candidates.sort(key=os.path.getmtime, reverse=True)
                last_file = candidates[0]
                if os.path.splitext(last_file)[1].lower() in (".xlsx", ".xls", ".csv"):
                    return last_file
        time.sleep(1)
    raise TimeoutError(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {download_dir} (ë§ˆì§€ë§‰ íŒŒì¼: {last_file})")


def move_to_data_folder(src_path: str) -> str:
    day = _kst_now()
    ym = day.strftime("%Y%m")
    ymd = day.strftime("%Y%m%d")
    dst_dir = os.path.join("data", "bigkinds", ym)
    ensure_dir(dst_dir)
    ext = os.path.splitext(src_path)[1].lower() or ".xlsx"
    dst_path = os.path.join(dst_dir, f"bigkinds_{ymd}{ext}")
    shutil.move(src_path, dst_path)
    return dst_path


def build_and_upload_month_partition(
    df: pd.DataFrame,
    embedding_model: OpenAIEmbeddings,
    bucket: str,
    s3_prefix_base: str,
    partition_month: str,
) -> str:
    s3_client = boto3.client("s3")
    part_name = f"partition_{partition_month}"
    s3_part_prefix = f"{s3_prefix_base.rstrip('/')}/{part_name}"

    # ì¤€ë¹„: ë¡œì»¬ ì‘ì—… ë””ë ‰í„°ë¦¬
    work_dir = os.path.abspath(os.path.join(".tmp", part_name))
    if os.path.isdir(work_dir):
        shutil.rmtree(work_dir, ignore_errors=True)
    ensure_dir(work_dir)

    # 1) ê¸°ì¡´ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ(ìˆìœ¼ë©´)
    resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=f"{s3_part_prefix}/")
    if resp.get("Contents"):
        for obj in resp["Contents"]:
            key = obj["Key"]
            if key.endswith("index.faiss") or key.endswith("index.pkl"):
                dst = os.path.join(work_dir, os.path.basename(key))
                s3_client.download_file(bucket, key, dst)

    # 2) ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ì‹ ê·œ
    try:
        db = FAISS.load_local(work_dir, embeddings=embedding_model, allow_dangerous_deserialization=True)
    except Exception:
        db = None

    # 3) ì»¬ëŸ¼ ê°ì§€ ë° ë¬¸ì„œ ìƒì„±(ì¤‘ë³µ URL ìŠ¤í‚µ)
    def _find_cols(df: pd.DataFrame) -> Tuple[str, str]:
        title_candidates = ["ì œëª©", "ê¸°ì‚¬ì œëª©", "title", "Title"]
        url_candidates = ["URL", "ì›ë¬¸URL", "url", "ë§í¬", "link", "Link"]
        t = next((c for c in df.columns if c in title_candidates), None)
        u = next((c for c in df.columns if c in url_candidates), None)
        if not t or not u:
            raise ValueError(f"ì œëª©/URL ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
        return t, u

    df = df.copy()
    df.columns = [str(c).strip().replace("\n", "") for c in df.columns]
    tcol, ucol = _find_cols(df)

    existing_urls = set()
    if db:
        for d in db.docstore._dict.values():
            u = (d.metadata or {}).get("url")
            if isinstance(u, str):
                existing_urls.add(u.strip())

    docs: List[Document] = []
    seen = set()
    for _, row in df.iterrows():
        title = str(row.get(tcol, "")).strip()
        url = str(row.get(ucol, "")).strip()
        if not title or not re.match(r"^https?://", url):
            continue
        if url in existing_urls or url in seen:
            continue
        seen.add(url)
        docs.append(Document(page_content=title, metadata={"url": url}))

    if db and docs:
        new_db = FAISS.from_documents(docs, embedding_model)
        db.merge_from(new_db)
        db.save_local(work_dir)
    elif not db and docs:
        db = FAISS.from_documents(docs, embedding_model)
        db.save_local(work_dir)
    elif not db and not docs:
        raise ValueError("ê¸°ì¡´ ì¸ë±ìŠ¤ë„ ì—†ê³  ì¶”ê°€í•  ë¬¸ì„œë„ ì—†ìŠµë‹ˆë‹¤.")

    # 4) ì—…ë¡œë“œ ìˆœì„œ: pkl -> faiss
    for name in ("index.pkl", "index.faiss"):
        s3_client.upload_file(os.path.join(work_dir, name), bucket, f"{s3_part_prefix}/{name}")

    shutil.rmtree(work_dir, ignore_errors=True)
    return part_name


def setup_driver(download_dir: str, headless: bool) -> tuple[webdriver.Chrome, WebDriverWait]:
    opts = Options()
    prefs = {
        "download.default_directory": os.path.abspath(download_dir),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
        "profile.default_content_settings.popups": 0,
    }
    opts.add_experimental_option("prefs", prefs)
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(service=Service(), options=opts)
    driver.maximize_window()
    wait = WebDriverWait(driver, 15)
    return driver, wait


def bigkinds_login_and_download(
    user_id: str,
    user_pw: str,
    download_dir: str,
    headless: bool = True,
) -> str:
    driver, wait = setup_driver(download_dir, headless=headless)
    try:
        driver.get("https://www.bigkinds.or.kr")
        time.sleep(2)
        membership = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "li.topMembership")))
        ActionChains(driver).move_to_element(membership).perform()
        time.sleep(0.6)
        login_link = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[text()="ë¡œê·¸ì¸"]')))
        driver.execute_script("arguments[0].click();", login_link)

        wait.until(EC.visibility_of_element_located((By.ID, "login-user-id"))).send_keys(user_id)
        driver.find_element(By.ID, "login-user-password").send_keys(user_pw)
        wait.until(EC.element_to_be_clickable((By.ID, "login-btn"))).click()
        time.sleep(3)

        # ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ ì´ë™
        driver.get("https://www.bigkinds.or.kr/v2/news/index.do")
        time.sleep(2)

        # ì–¸ë¡ ì‚¬: ì „êµ­ì¼ê°„ì§€
        tab2 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a[href='#srch-tab2']")))
        tab2.click()
        time.sleep(0.5)
        driver.execute_script("document.getElementById('ì „êµ­ì¼ê°„ì§€').click();")
        time.sleep(0.5)

        # í†µí•©ë¶„ë¥˜: ì •ì¹˜
        tab3 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a[href='#srch-tab3']")))
        tab3.click()
        time.sleep(0.5)
        wait.until(EC.element_to_be_clickable((By.XPATH, '//span[@data-role="display" and text()="ì •ì¹˜"]'))).click()
        time.sleep(0.5)

        # ê¸°ê°„: 1ì¼
        tab1 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a[href='#srch-tab1']")))
        tab1.click()
        time.sleep(0.3)
        one_day_label = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'label[for="date1-7"]')))
        driver.execute_script("arguments[0].click();", one_day_label)
        time.sleep(0.5)

        # ê²€ìƒ‰ ì ìš©
        search_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.btn.btn-search.news-search-btn.news-report-search-btn')))
        driver.execute_script("arguments[0].scrollIntoView(true);", search_btn)
        ActionChains(driver).move_to_element(search_btn).click().perform()
        time.sleep(3)

        # ë¶„ì„ê¸°ì‚¬ ì²´í¬
        label_tm_use = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'label[for="filter-tm-use"]')))
        driver.execute_script("arguments[0].click();", label_tm_use)
        time.sleep(2)

        # ë‹¤ìš´ë¡œë“œ
        step3_btn = wait.until(EC.element_to_be_clickable((By.ID, "collapse-step-3")))
        driver.execute_script("arguments[0].click();", step3_btn)
        time.sleep(0.5)
        excel_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.news-download-btn.mobile-excel-download')))
        ActionChains(driver).move_to_element(excel_btn).click().perform()
        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°ëŠ” í˜¸ì¶œìì—ì„œ ìˆ˜í–‰
        time.sleep(2)
        return wait_for_download_complete(download_dir)
    finally:
        try:
            driver.quit()
        except Exception:
            pass


def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # í™˜ê²½ ë³€ìˆ˜
    user_id = os.environ.get("BIGKINDS_USER_ID")
    user_pw = os.environ.get("BIGKINDS_USER_PW")
    if not user_id or not user_pw:
        raise SystemExit("í™˜ê²½ë³€ìˆ˜ BIGKINDS_USER_ID/BIGKINDS_USER_PWê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    openai_api_key = os.environ.get("OPENAI_API_KEY")
    if not openai_api_key:
        raise SystemExit("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    bucket = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
    s3_prefix = os.environ.get("S3_INDEX_PREFIX", "feature_faiss_db_openai_partition/")
    download_dir = os.environ.get("DOWNLOAD_DIR", os.path.abspath("./downloads"))
    headless = os.environ.get("HEADLESS", "1") in ("1", "true", "TRUE", "yes", "YES")

    # íŒŒí‹°ì…˜ ë°©ì‹: ê¸°ë³¸ì€ ë‹¹ì›”(ì„œìš¸ì‹œê°„)
    partition_month = os.environ.get("PARTITION_MONTH") or compute_partition_month_kst()

    # 1) ìˆ˜ì§‘: BigKindsì—ì„œ 1ì¼ ë°ì´í„° ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
    logging.info("ğŸ“¥ BigKinds ë‹¤ìš´ë¡œë“œ ì‹œì‘ (1ì¼)")
    downloaded = bigkinds_login_and_download(user_id, user_pw, download_dir, headless=headless)
    moved = move_to_data_folder(downloaded)
    logging.info(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬: {moved}")

    # 2) ë¹Œë“œ/ì—…ë¡œë“œ: íƒ€ì´í‹€ ì¸ë±ìŠ¤ ì¦ë¶„ ë³‘í•© â†’ S3 pklâ†’faiss ì—…ë¡œë“œ
    logging.info("ğŸ§± íƒ€ì´í‹€ ì¸ë±ìŠ¤ ë³‘í•©/ì—…ë¡œë“œ ì‹œì‘")
    df = pd.read_excel(moved)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    part_name = build_and_upload_month_partition(df, embeddings, bucket, s3_prefix, partition_month)
    logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket}/{s3_prefix}{part_name}/ (pklâ†’faiss)")


if __name__ == "__main__":
    main()
