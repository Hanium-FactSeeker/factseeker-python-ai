import os
import sys
import re
import glob
import time
import shutil
import logging
import subprocess
import csv
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import List, Tuple, Optional

import boto3
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.docstore.document import Document

from webdriver_manager.chrome import ChromeDriverManager
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.keys import Keys


def _kst_now() -> datetime:
    return datetime.now(ZoneInfo("Asia/Seoul"))


def compute_partition_month_kst() -> str:
    return _kst_now().strftime("%Y%m")


def _parse_hhmm(value: str) -> Optional[tuple[int, int]]:
    m = re.match(r"^(\d{1,2}):(\d{2})$", value.strip())
    if not m:
        return None
    h, mi = int(m.group(1)), int(m.group(2))
    if 0 <= h <= 23 and 0 <= mi <= 59:
        return h, mi
    return None


def _next_kst_datetime(hour: int, minute: int) -> datetime:
    now = _kst_now()
    target = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
    if target <= now:
        target = target + timedelta(days=1)
    return target


def _sleep_until_kst(hour: int, minute: int) -> None:
    target = _next_kst_datetime(hour, minute)
    now = _kst_now()
    secs = (target - now).total_seconds()
    mins = int(secs // 60)
    logging.info(f"â° ì˜ˆì•½ ì‹¤í–‰ ëŒ€ê¸°: {target.strftime('%Y-%m-%d %H:%M KST')} (~{mins}ë¶„)")
    # ê¸´ ëŒ€ê¸°ëŠ” ì¤‘ê°„ì¤‘ê°„ ë¡œê·¸ë¥¼ ì°ìœ¼ë©° ê¸°ë‹¤ë¦°ë‹¤
    remaining = secs
    while remaining > 0:
        chunk = 300 if remaining > 600 else 60 if remaining > 120 else 10 if remaining > 30 else remaining
        time.sleep(chunk)
        remaining -= chunk
        if remaining > 0:
            left_m = int(max(0, remaining) // 60)
            logging.info(f"... ëŒ€ê¸° ì¤‘ (~{left_m}ë¶„ ë‚¨ìŒ)")


def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def wait_for_download_complete(download_dir: str, timeout_sec: int = 600) -> str:
    """Wait until there is no .crdownload left and return the latest file path."""
    ensure_dir(download_dir)
    start = time.time()
    last_file = None
    while time.time() - start < timeout_sec:
        partials = glob.glob(os.path.join(download_dir, "*.crdownload"))
        if not partials:
            candidates = [p for p in glob.glob(os.path.join(download_dir, "*")) if os.path.isfile(p)]
            if candidates:
                candidates.sort(key=os.path.getmtime, reverse=True)
                last_file = candidates[0]
                if os.path.splitext(last_file)[1].lower() in (".xlsx", ".xls", ".csv"):
                    return last_file
        time.sleep(1)
    raise TimeoutError(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {download_dir} (ë§ˆì§€ë§‰ íŒŒì¼: {last_file})")


def move_to_data_folder(src_path: str) -> str:
    day = _kst_now()
    ym = day.strftime("%Y%m")
    ymd = day.strftime("%Y%m%d")
    dst_dir = os.path.join("data", "bigkinds", ym)
    ensure_dir(dst_dir)
    ext = os.path.splitext(src_path)[1].lower() or ".xlsx"
    dst_path = os.path.join(dst_dir, f"bigkinds_{ymd}{ext}")
    shutil.move(src_path, dst_path)
    return dst_path


## CSVëŠ” ë¡œì»¬ ì €ì¥ë§Œ ìˆ˜í–‰(ìš”ì²­ì— ë”°ë¼ S3 ì—…ë¡œë“œëŠ” ì œê±°)


# Note: merge_excels_to_csv_and_build integration removed. This script always collects 1-day via Selenium.


def build_and_upload_month_partition(
    df: pd.DataFrame,
    embedding_model: OpenAIEmbeddings,
    bucket: str,
    s3_prefix_base: str,
    partition_month: str,
) -> Tuple[str, List[str]]:
    s3_client = boto3.client("s3")
    part_name = f"partition_{partition_month}"
    s3_part_prefix = f"{s3_prefix_base.rstrip('/')}/{part_name}"

    # ì¤€ë¹„: ë¡œì»¬ ì‘ì—… ë””ë ‰í„°ë¦¬
    work_dir = os.path.abspath(os.path.join(".tmp", part_name))
    if os.path.isdir(work_dir):
        shutil.rmtree(work_dir, ignore_errors=True)
    ensure_dir(work_dir)

    # 1) ê¸°ì¡´ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ(ìˆìœ¼ë©´)
    resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=f"{s3_part_prefix}/")
    if resp.get("Contents"):
        for obj in resp["Contents"]:
            key = obj["Key"]
            if key.endswith("index.faiss") or key.endswith("index.pkl"):
                dst = os.path.join(work_dir, os.path.basename(key))
                s3_client.download_file(bucket, key, dst)

    # 2) ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ì‹ ê·œ
    try:
        db = FAISS.load_local(work_dir, embeddings=embedding_model, allow_dangerous_deserialization=True)
    except Exception:
        db = None

    # 3) ì»¬ëŸ¼ ê°ì§€ ë° ë¬¸ì„œ ìƒì„±(ì¤‘ë³µ URL ìŠ¤í‚µ)
    def _find_cols(df: pd.DataFrame) -> Tuple[str, str]:
        title_candidates = ["ì œëª©", "ê¸°ì‚¬ì œëª©", "title", "Title"]
        url_candidates = ["URL", "ì›ë¬¸URL", "url", "ë§í¬", "link", "Link"]
        t = next((c for c in df.columns if c in title_candidates), None)
        u = next((c for c in df.columns if c in url_candidates), None)
        if not t or not u:
            raise ValueError(f"ì œëª©/URL ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
        return t, u

    df = df.copy()
    df.columns = [str(c).strip().replace("\n", "") for c in df.columns]
    tcol, ucol = _find_cols(df)

    existing_urls = set()
    if db:
        for d in db.docstore._dict.values():
            u = (d.metadata or {}).get("url")
            if isinstance(u, str):
                existing_urls.add(u.strip())

    docs: List[Document] = []
    used_urls: List[str] = []
    seen = set()
    for _, row in df.iterrows():
        title = str(row.get(tcol, "")).strip()
        url = str(row.get(ucol, "")).strip()
        if not title or not re.match(r"^https?://", url):
            continue
        if url in existing_urls or url in seen:
            continue
        seen.add(url)
        docs.append(Document(page_content=title, metadata={"url": url}))
        used_urls.append(url)

    if db and docs:
        new_db = FAISS.from_documents(docs, embedding_model)
        db.merge_from(new_db)
        db.save_local(work_dir)
    elif not db and docs:
        db = FAISS.from_documents(docs, embedding_model)
        db.save_local(work_dir)
    elif not db and not docs:
        raise ValueError("ê¸°ì¡´ ì¸ë±ìŠ¤ë„ ì—†ê³  ì¶”ê°€í•  ë¬¸ì„œë„ ì—†ìŠµë‹ˆë‹¤.")

    # 4) ì—…ë¡œë“œ ìˆœì„œ: pkl -> faiss
    for name in ("index.pkl", "index.faiss"):
        s3_client.upload_file(os.path.join(work_dir, name), bucket, f"{s3_part_prefix}/{name}")

    shutil.rmtree(work_dir, ignore_errors=True)
    return part_name, used_urls


def setup_driver(download_dir: str, headless: bool) -> tuple[webdriver.Chrome, WebDriverWait]:
    opts = Options()
    prefs = {
        "download.default_directory": os.path.abspath(download_dir),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
        "profile.default_content_settings.popups": 0,
    }
    opts.add_experimental_option("prefs", prefs)
    # Reduce noisy logs and headless quirks
    opts.add_experimental_option("excludeSwitches", ["enable-logging"])
    opts.add_argument("--log-level=3")
    opts.add_argument("--window-size=1400,900")
    opts.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36")
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=opts)
    wait = WebDriverWait(driver, 15)
    return driver, wait

# -------------------------------
# Selenium helpers (robust)
# -------------------------------
def _accept_unexpected_alerts(driver, wait_timeout: float = 1.0) -> Optional[str]:
    try:
        WebDriverWait(driver, wait_timeout).until(EC.alert_is_present())
        al = driver.switch_to.alert
        txt = al.text
        al.accept()
        return txt
    except TimeoutException:
        return None


def _scroll_into_view(driver, el):
    try:
        driver.execute_script("arguments[0].scrollIntoView({block:'center', inline:'center'});", el)
    except Exception:
        pass


def _open_step_panel(driver, panel_id: str):
    try:
        btn = driver.find_element(By.ID, panel_id)
        expanded = (btn.get_attribute("aria-expanded") or "").lower()
        if expanded in ("", "false"):
            driver.execute_script("arguments[0].click();", btn)
            time.sleep(0.3)
    except Exception:
        pass


def _open_tab(driver, css_selector: str):
    try:
        tab = WebDriverWait(driver, 7).until(EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector)))
        driver.execute_script("arguments[0].click();", tab)
        time.sleep(0.2)
    except Exception:
        pass


def select_national_dailies(driver, wait):
    _open_tab(driver, "a[href='#srch-tab2']")
    time.sleep(0.5)
    try:
        b = driver.find_element(By.ID, "ë°©ì†¡ì‚¬")
        _scroll_into_view(driver, b)
        b.click()
        time.sleep(0.2)
    except Exception:
        pass
    try:
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'ì „êµ­ì¼ê°„ì§€')))
        driver.execute_script("document.getElementById('ì „êµ­ì¼ê°„ì§€').click();")
        time.sleep(0.2)
        return
    except Exception:
        pass
    try:
        lbl = driver.find_element(By.CSS_SELECTOR, "label[for='ì „êµ­ì¼ê°„ì§€']")
        _scroll_into_view(driver, lbl)
        driver.execute_script("arguments[0].click();", lbl)
        time.sleep(0.2)
        return
    except Exception:
        pass
    try:
        node = driver.find_element(By.XPATH, "//*[contains(normalize-space(.), 'ì „êµ­ì¼ê°„ì§€')]")
        _scroll_into_view(driver, node)
        driver.execute_script("arguments[0].click();", node)
        time.sleep(0.2)
    except Exception:
        logging.warning("ì „êµ­ì¼ê°„ì§€ ì„ íƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")


def choose_politics_category(driver, wait):
    _open_tab(driver, "a[href='#srch-tab3']")
    time.sleep(0.3)
    try:
        node = wait.until(EC.element_to_be_clickable((By.XPATH, '//span[@data-role="display" and text()="ì •ì¹˜"]')))
        _scroll_into_view(driver, node)
        node.click()
        time.sleep(0.2)
    except Exception:
        logging.warning("í†µí•©ë¶„ë¥˜ 'ì •ì¹˜' ì„ íƒ ì‹¤íŒ¨. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")


def _ensure_date_direct_input_mode(driver) -> None:
    js = r'''
    return (function(){
      function clickNode(n){ try{ n.scrollIntoView({block:'center'}); n.click(); return true; }catch(e){ return false; } }
      const labels = ['ì§ì ‘ì…ë ¥','ì‚¬ìš©ì ì§€ì •','ìˆ˜ë™ì…ë ¥','ìˆ˜ë™','ì§ì ‘'];
      for (const t of labels){
        const nodes = Array.from(document.querySelectorAll('label,button,a,span,div'));
        for (const n of nodes){
          const s = (n.innerText||'').replace(/\s+/g,' ').trim();
          if (s && s.includes(t)) { if (clickNode(n)) return 'text'; }
        }
      }
      const radios = Array.from(document.querySelectorAll('input[type=radio],input[type=button]'));
      for (const r of radios){
        const s = ((r.id||'')+' '+(r.value||'')+' '+(r.name||'')).toLowerCase();
        if (/(direct|manual|custom|user|free)/.test(s)){
          if (clickNode(r)) return 'radio';
        }
      }
      return '';
    })();
    '''
    try:
        mode = driver.execute_script(js)
        if mode:
            time.sleep(0.2)
    except Exception:
        pass


def set_date_range_with_events(driver, start_str: str, end_str: str):
    js = r'''
    (function(){
      function setAndDispatch(id, val){
        var el = document.getElementById(id);
        if(!el){return;}
        try { el.removeAttribute('readonly'); } catch(_){ }
        el.value = val;
        if (window.jQuery){
          try { window.jQuery(el).val(val).trigger('input').trigger('change'); } catch(_){ }
        }
        el.dispatchEvent(new Event('input', {bubbles:true}));
        el.dispatchEvent(new Event('change', {bubbles:true}));
      }
      setAndDispatch('search-begin-date', arguments[0]);
      setAndDispatch('search-end-date', arguments[1]);
    })();
    '''
    driver.execute_script(js, start_str, end_str)


def set_date_range_robust(driver, start_str: str, end_str: str, retries: int = 3) -> None:
    def norm(d: str) -> str:
        m = re.match(r"^(\d{4})-(\d{1,2})-(\d{1,2})$", d)
        if not m: return d
        y, mo, da = m.groups()
        return f"{y}-{int(mo):02d}-{int(da):02d}"

    start_str = norm(start_str)
    end_str = norm(end_str)
    if not re.match(r"^\d{4}-\d{2}-\d{2}$", start_str) or not re.match(r"^\d{4}-\d{2}-\d{2}$", end_str):
        raise ValueError(f"ë‚ ì§œ í˜•ì‹ì€ YYYY-MM-DD ì´ì–´ì•¼ í•©ë‹ˆë‹¤. start={start_str}, end={end_str}")

    _open_tab(driver, "a[href='#srch-tab1']")
    _open_step_panel(driver, "collapse-step-1")
    _ensure_date_direct_input_mode(driver)

    for attempt in range(1, retries + 1):
        try:
            js = r'''
            (function(s,e){
              function setVal(id, val){
                var el = document.getElementById(id);
                if(!el) return false;
                try { el.removeAttribute('readonly'); } catch(_){ }
                el.value = val;
                if (window.jQuery){
                  try { window.jQuery(el).val(val).trigger('input').trigger('change'); } catch(_){ }
                }
                el.dispatchEvent(new Event('input', {bubbles:true}));
                el.dispatchEvent(new Event('change', {bubbles:true}));
                return true;
              }
              var ok1 = setVal('search-begin-date', s);
              var ok2 = setVal('search-end-date', e);
              document.activeElement && document.activeElement.blur && document.activeElement.blur();
              document.body && document.body.click && document.body.click();
              return ok1 && ok2;
            })(arguments[0], arguments[1]);
            '''
            driver.execute_script(js, start_str, end_str)
        except Exception:
            pass

        time.sleep(0.5)
        _accept_unexpected_alerts(driver, wait_timeout=0.7)

        begin_val, end_val = driver.execute_script(
            "return [document.getElementById('search-begin-date')?.value, document.getElementById('search-end-date')?.value];"
        )
        if begin_val == start_str and end_val == end_str:
            return

        try:
            b = driver.find_element(By.ID, 'search-begin-date')
            e = driver.find_element(By.ID, 'search-end-date')
            for el, val in ((b, start_str), (e, end_str)):
                driver.execute_script("arguments[0].removeAttribute('readonly');", el)
                _scroll_into_view(driver, el)
                el.click(); time.sleep(0.05)
                el.send_keys(Keys.CONTROL, 'a'); time.sleep(0.05)
                el.send_keys(val); time.sleep(0.05)
                el.send_keys(Keys.TAB)
            time.sleep(0.5)
            _accept_unexpected_alerts(driver, wait_timeout=0.7)
            begin_val, end_val = driver.execute_script(
                "return [document.getElementById('search-begin-date')?.value, document.getElementById('search-end-date')?.value];"
            )
            if begin_val == start_str and end_val == end_str:
                return
        except Exception:
            pass

        logging.info(f"ë‚ ì§œ ì¬ì‹œë„ í•„ìš”(ì‹œë„ {attempt}/{retries}): begin={begin_val}, end={end_val}")
        time.sleep(0.4)

    set_date_range_with_events(driver, start_str, end_str)
    time.sleep(0.4)
    _accept_unexpected_alerts(driver, wait_timeout=0.7)


def _dismiss_common_overlays(driver):
    try:
        texts = ["í™•ì¸", "ë™ì˜", "ë™ì˜í•©ë‹ˆë‹¤", "ë‹«ê¸°", "ì˜¤ëŠ˜ ê·¸ë§Œ ë³´ê¸°", "X", "ë‹«  ê¸°"]
        candidates = driver.find_elements(
            By.XPATH,
            "//*[contains(@class,'modal') or contains(@class,'layer') or contains(@class,'popup') or contains(@class,'overlay') or contains(@class,'dim')]//button|//a"
        )
        for btn in candidates:
            label = (btn.text or "").strip()
            if any(t in label for t in texts):
                try:
                    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                    driver.execute_script("arguments[0].click();", btn)
                    time.sleep(0.2)
                except Exception:
                    pass
    except Exception:
        pass


def click_search_button(driver, wait):
    _dismiss_common_overlays(driver)
    selectors = [
        (By.CSS_SELECTOR, "button.news-report-search-btn"),
        (By.CSS_SELECTOR, "button.news-search-btn"),
        (By.CSS_SELECTOR, "button.btn-search"),
        (By.XPATH, "//button[contains(normalize-space(.),'ê²€ìƒ‰í•˜ê¸°')]"),
        (By.XPATH, "//a[contains(normalize-space(.),'ê²€ìƒ‰í•˜ê¸°')]"),
        (By.XPATH, "//button[contains(.,'ê²€ìƒ‰')]")
    ]
    for by, sel in selectors:
        try:
            el = wait.until(EC.presence_of_element_located((by, sel)))
            _scroll_into_view(driver, el)
            try:
                el = wait.until(EC.element_to_be_clickable((by, sel)))
                ActionChains(driver).move_to_element(el).pause(0.05).click().perform()
            except Exception:
                driver.execute_script("arguments[0].click();", el)
            time.sleep(0.3)
            return True
        except Exception:
            continue
    try:
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.ENTER)
        time.sleep(0.3)
        return True
    except Exception:
        pass
    raise TimeoutException("ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨")


def apply_analysis_article_filter(driver, wait, max_retry: int = 3) -> bool:
    _open_step_panel(driver, "collapse-step-2")
    for i in range(1, max_retry + 1):
        try:
            cb = WebDriverWait(driver, 7).until(EC.presence_of_element_located((By.ID, "filter-tm-use")))
            _scroll_into_view(driver, cb)
            try:
                label = driver.find_element(By.CSS_SELECTOR, "label[for='filter-tm-use']")
                _scroll_into_view(driver, label)
                driver.execute_script("arguments[0].click();", label)
                time.sleep(0.2)
            except Exception:
                pass
            is_checked = driver.execute_script("return !!document.getElementById('filter-tm-use')?.checked;")
            if not is_checked:
                js_force = r'''
                (function(){
                  var el = document.getElementById('filter-tm-use');
                  if(!el) return false;
                  el.checked = true;
                  el.dispatchEvent(new Event('click', {bubbles:true}));
                  el.dispatchEvent(new Event('input', {bubbles:true}));
                  el.dispatchEvent(new Event('change', {bubbles:true}));
                  return el.checked === true;
                })();
                '''
                is_checked = driver.execute_script(js_force)
                time.sleep(0.2)
            if not is_checked:
                try:
                    _scroll_into_view(driver, cb)
                    cb.click()
                    time.sleep(0.2)
                    is_checked = driver.execute_script("return !!document.getElementById('filter-tm-use')?.checked;")
                except Exception:
                    pass
            if not is_checked:
                logging.warning(f"ë¶„ì„ê¸°ì‚¬ ì²´í¬ ì‹¤íŒ¨(ì‹œë„ {i}/{max_retry}) - ì¬ì‹œë„")
                _accept_unexpected_alerts(driver, wait_timeout=0.5)
                continue
            # ì ìš©í•˜ê¸° ë²„íŠ¼
            apply_candidates = [
                (By.XPATH, "//button[contains(normalize-space(.), 'ì ìš©í•˜ê¸°')]"),
                (By.XPATH, "//a[contains(normalize-space(.), 'ì ìš©í•˜ê¸°')]"),
                (By.CSS_SELECTOR, "button.btn-apply"),
                (By.CSS_SELECTOR, "button.apply"),
                (By.CSS_SELECTOR, "button.filter-apply"),
            ]
            applied = False
            for by, sel in apply_candidates:
                try:
                    btn = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((by, sel)))
                    _scroll_into_view(driver, btn)
                    driver.execute_script("arguments[0].click();", btn)
                    applied = True
                    break
                except Exception:
                    continue
            if not applied:
                try:
                    btn = driver.find_element(By.XPATH, "//button[contains(., 'ì ìš©í•˜ê¸°') or contains(., 'ì ìš©')]")
                    _scroll_into_view(driver, btn)
                    driver.execute_script("arguments[0].click();", btn)
                    applied = True
                except Exception:
                    pass
            if not applied:
                logging.warning(f"ì ìš©í•˜ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨(ì‹œë„ {i}/{max_retry})")
                _accept_unexpected_alerts(driver, wait_timeout=0.5)
                continue
            _accept_unexpected_alerts(driver, wait_timeout=1.0)
            time.sleep(1.2)
            still_checked = driver.execute_script("return !!document.getElementById('filter-tm-use')?.checked;")
            logging.info("âœ… ë¶„ì„ê¸°ì‚¬ ì²´í¬ ë° ì ìš©í•˜ê¸° ì™„ë£Œ (checked=%s)", still_checked)
            return True
        except Exception as e:
            logging.warning(f"ë¶„ì„ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸(ì‹œë„ {i}/{max_retry}): {e}")
            _accept_unexpected_alerts(driver, wait_timeout=0.8)
            try:
                driver.save_screenshot(f"error_filter_try{i}.png")
            except Exception:
                pass
            time.sleep(0.6)
    return False


def bigkinds_login_and_download(
    user_id: str,
    user_pw: str,
    download_dir: str,
    headless: bool = True,
) -> str:
    driver, wait = setup_driver(download_dir, headless=headless)
    try:
        driver.get("https://www.bigkinds.or.kr")
        time.sleep(2)
        membership = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "li.topMembership")))
        ActionChains(driver).move_to_element(membership).perform()
        time.sleep(0.6)
        login_link = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[text()="ë¡œê·¸ì¸"]')))
        driver.execute_script("arguments[0].click();", login_link)

        wait.until(EC.visibility_of_element_located((By.ID, "login-user-id"))).send_keys(user_id)
        driver.find_element(By.ID, "login-user-password").send_keys(user_pw)
        wait.until(EC.element_to_be_clickable((By.ID, "login-btn"))).click()
        time.sleep(3)

        # ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ ì´ë™
        driver.get("https://www.bigkinds.or.kr/v2/news/index.do")
        time.sleep(2)

        # ì–¸ë¡ ì‚¬/ì •ì¹˜
        logging.info("ğŸ“° ì–¸ë¡ ì‚¬: ì „êµ­ì¼ê°„ì§€ ì„ íƒ ì‹œë„")
        select_national_dailies(driver, wait)
        logging.info("ğŸ·ï¸ í†µí•©ë¶„ë¥˜: ì •ì¹˜ ì„ íƒ ì‹œë„")
        choose_politics_category(driver, wait)

        # ê¸°ê°„: ë‹¹ì¼(KST) 1ì¼ ë²”ìœ„ ì§ì ‘ ì…ë ¥
        kst_today = _kst_now().strftime("%Y-%m-%d")
        logging.info(f"ğŸ“… ê¸°ê°„ ì„¤ì •: {kst_today} ~ {kst_today}")
        set_date_range_robust(driver, kst_today, kst_today)
        time.sleep(0.4)

        # ê²€ìƒ‰ ì ìš©
        logging.info("ğŸ” ê²€ìƒ‰ ì ìš© í´ë¦­")
        click_search_button(driver, wait)
        time.sleep(2)

        # ë¶„ì„ê¸°ì‚¬ ì²´í¬ â†’ ì ìš©í•˜ê¸°
        logging.info("ğŸ§© ë¶„ì„ê¸°ì‚¬ ì²´í¬/ì ìš©")
        apply_analysis_article_filter(driver, wait, max_retry=3)

        # ë‹¤ìš´ë¡œë“œ
        logging.info("â¬‡ï¸ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì—´ê¸°")
        step3_btn = wait.until(EC.element_to_be_clickable((By.ID, "collapse-step-3")))
        driver.execute_script("arguments[0].click();", step3_btn)
        time.sleep(0.5)
        excel_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.news-download-btn.mobile-excel-download')))
        ActionChains(driver).move_to_element(excel_btn).click().perform()
        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°ëŠ” í˜¸ì¶œìì—ì„œ ìˆ˜í–‰
        time.sleep(2)
        return wait_for_download_complete(download_dir)
    finally:
        try:
            driver.quit()
        except Exception:
            pass


def _trigger_prewarm_after_upload(urls: List[str], concurrency: int = 3, limit: int = 0) -> int:
    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    cmd = [os.sys.executable, "-m", "fastapitest.scripts.prewarm_articles"]
    # íŒŒì¼ ëª¨ë“œë¡œ URL ì „ë‹¬(ì œëª© í”„ë¦¬ë¡œë“œ ì¤‘ë³µ ë°©ì§€)
    tmp_dir = os.path.join(repo_root, ".tmp")
    os.makedirs(tmp_dir, exist_ok=True)
    url_file = os.path.join(tmp_dir, f"urls_partition_daily.txt")
    with open(url_file, "w", encoding="utf-8") as f:
        for u in urls:
            f.write(u.strip() + "\n")
    cmd += ["--source", "file", "--file", url_file, "--concurrency", str(concurrency), "--limit", str(limit)]
    logging.info(f"prewarm ì‹œì‘: {' '.join(cmd)} (cwd={repo_root})")
    proc = subprocess.run(cmd, capture_output=True, text=True, cwd=repo_root)
    if proc.returncode != 0:
        logging.error(f"prewarm ì‹¤íŒ¨(rc={proc.returncode})\nSTDOUT:\n{proc.stdout}\nSTDERR:\n{proc.stderr}")
    else:
        logging.info(f"prewarm ì™„ë£Œ\nSTDOUT:\n{proc.stdout}")
    return proc.returncode


def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ëª¨ë“œ: ìê¸° ìì‹ ì„ ë¶„ë¦¬ëœ ì„¸ì…˜ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ê³  ì¦‰ì‹œ ë°˜í™˜
    if os.environ.get("RUN_BIGKINDS_BACKGROUND", "0") in ("1", "true", "TRUE", "yes", "YES") \
       and os.environ.get("RUN_BIGKINDS_DETACHED", "0") != "1":
        try:
            repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
            logs_dir = os.path.join(repo_root, "logs")
            os.makedirs(logs_dir, exist_ok=True)
            ts = time.strftime("%Y%m%d-%H%M%S")
            log_path = os.path.join(logs_dir, f"run_bigkinds_{ts}.log")
            cmd = [sys.executable, "-m", "fastapitest.scripts.run_bigkinds_collect_and_build"]
            env = os.environ.copy()
            env["PYTHONUNBUFFERED"] = "1"
            env["RUN_BIGKINDS_DETACHED"] = "1"
            with open(log_path, "a", buffering=1, encoding="utf-8") as f:
                p = subprocess.Popen(
                    cmd,
                    cwd=repo_root,
                    stdout=f,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,
                    env=env,
                    start_new_session=True,
                )
            logging.info(f"ğŸ§© run_bigkinds ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘ (PID={p.pid}, log={log_path})")
            return
        except Exception as e:
            logging.error(f"run_bigkinds ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ê³„ì†í•´ì„œ í¬ê·¸ë¼ìš´ë“œ ëª¨ë“œë¡œ ì§„í–‰

    # ì˜ˆì•½ ì‹¤í–‰ ì˜µì…˜: RUN_AT_KST=HH:MM, RUN_DAILY=1
    run_at_kst = os.environ.get("RUN_AT_KST") or os.environ.get("RUN_BIGKINDS_RUN_AT_KST")
    run_daily = os.environ.get("RUN_DAILY", "0") in ("1", "true", "TRUE", "yes", "YES")
    if run_at_kst:
        parsed = _parse_hhmm(run_at_kst)
        if not parsed:
            logging.warning(f"RUN_AT_KST í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤(HH:MM): {run_at_kst}. ì¦‰ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        else:
            h, mi = parsed
            _sleep_until_kst(h, mi)
            # ë§Œì•½ ë§¤ì¼ ë°˜ë³µì´ë©´, ì´ í•¨ìˆ˜ê°€ í•œ ë²ˆ ì‹¤í–‰ í›„ ë£¨í”„ ëŒë„ë¡ ì•„ë˜ì—ì„œ ì²˜ë¦¬

    def _run_once():
        # í™˜ê²½ ë³€ìˆ˜
        user_id = os.environ.get("BIGKINDS_USER_ID")
        user_pw = os.environ.get("BIGKINDS_USER_PW")
        if not user_id or not user_pw:
            raise SystemExit("í™˜ê²½ë³€ìˆ˜ BIGKINDS_USER_ID/BIGKINDS_USER_PWê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        openai_api_key = os.environ.get("OPENAI_API_KEY")
        if not openai_api_key:
            raise SystemExit("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        bucket = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
        s3_prefix = os.environ.get("S3_INDEX_PREFIX", "feature_faiss_db_openai_partition/")
        download_dir = os.environ.get("DOWNLOAD_DIR", os.path.abspath("./downloads"))
        headless = os.environ.get("HEADLESS", "1") in ("1", "true", "TRUE", "yes", "YES")
        # CSV íŒŒì´í”„ë¼ì¸ ì˜µì…˜ (ë¡œì»¬ ì €ì¥ë§Œ; S3 ì—…ë¡œë“œëŠ” ë¹„í™œì„±)
        write_csv = os.environ.get("WRITE_CSV", "1") in ("1", "true", "TRUE", "yes", "YES")
        logging.info("ğŸ§¾ ì‹¤í–‰ íŒŒë¼ë¯¸í„°: bucket=%s, prefix=%s, headless=%s", bucket, s3_prefix, headless)

        # íŒŒí‹°ì…˜ ë°©ì‹: ê¸°ë³¸ì€ ë‹¹ì›”(ì„œìš¸ì‹œê°„)
        partition_month = os.environ.get("PARTITION_MONTH") or compute_partition_month_kst()

        # 1) 1ì¼ ìˆ˜ì§‘: BigKinds ì—‘ì…€ ë‹¤ìš´ë¡œë“œ â†’ CSV(ì„ íƒ) â†’ DF ë¡œë“œ
        logging.info("ğŸ“¥ BigKinds ë‹¤ìš´ë¡œë“œ ì‹œì‘ (1ì¼, KST)")
        downloaded = bigkinds_login_and_download(user_id, user_pw, download_dir, headless=headless)
        moved = move_to_data_folder(downloaded)
        logging.info(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬: {moved}")
        # ì—‘ì…€ ë¡œë“œ ë° ì •ê·œí™”
        df = pd.read_excel(moved)
        df.columns = [str(c).strip().replace("\n", "") for c in df.columns]
        # CSV ì €ì¥(í•„ìˆ˜ ì»¬ëŸ¼ë§Œ)
        if write_csv:
            csv_path = os.path.splitext(moved)[0] + ".csv"
            try:
                required_cols = ["ì¼ì", "ì–¸ë¡ ì‚¬", "ì œëª©", "URL", "íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)"]
                out = pd.DataFrame()
                for c in required_cols:
                    if c in df.columns:
                        out[c] = df[c]
                    else:
                        out[c] = ""
                out = out.fillna("")
                out.to_csv(csv_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
                logging.info(f"ğŸ“ CSV ë¡œì»¬ ì €ì¥ ì™„ë£Œ(í•„ìˆ˜ ì»¬ëŸ¼): {csv_path}")
            except Exception as e:
                logging.warning(f"CSV ì €ì¥ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
        logging.info("ğŸ§± íƒ€ì´í‹€ ì¸ë±ìŠ¤ ë³‘í•©/ì—…ë¡œë“œ ì‹œì‘")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
        part_name, used_urls = build_and_upload_month_partition(df, embeddings, bucket, s3_prefix, partition_month)
        logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket}/{s3_prefix}{part_name}/ (pklâ†’faiss)")

        # 3) (ì˜µì…˜) ì—…ë¡œë“œ ì§í›„ ë³¸ë¬¸ í”„ë¦¬ì›Œë°: íŒŒì¼ ëª¨ë“œë¡œ ìƒˆ URLë§Œ ì²˜ë¦¬
        prewarm_enabled = os.environ.get("PREWARM_AFTER_UPLOAD", "1") in ("1", "true", "TRUE", "yes", "YES")
        if prewarm_enabled:
            logging.info(f"ğŸ”¥ ë³¸ë¬¸ í”„ë¦¬ì›Œë° ì‹œì‘ (urls={len(used_urls)})")
            _trigger_prewarm_after_upload(
                urls=used_urls,
                concurrency=int(os.environ.get("PREWARM_CONCURRENCY", "3")),
                limit=int(os.environ.get("PREWARM_LIMIT", "0")),
            )

    if run_daily and run_at_kst and _parse_hhmm(run_at_kst):
        while True:
            _run_once()
            # ë‹¤ìŒ ì‹¤í–‰ê¹Œì§€ ëŒ€ê¸°
            h, mi = _parse_hhmm(run_at_kst)
            _sleep_until_kst(h, mi)
    else:
        _run_once()

    # í™˜ê²½ ë³€ìˆ˜
    user_id = os.environ.get("BIGKINDS_USER_ID")
    user_pw = os.environ.get("BIGKINDS_USER_PW")
    if not user_id or not user_pw:
        raise SystemExit("í™˜ê²½ë³€ìˆ˜ BIGKINDS_USER_ID/BIGKINDS_USER_PWê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    openai_api_key = os.environ.get("OPENAI_API_KEY")
    if not openai_api_key:
        raise SystemExit("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    bucket = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
    s3_prefix = os.environ.get("S3_INDEX_PREFIX", "feature_faiss_db_openai_partition/")
    download_dir = os.environ.get("DOWNLOAD_DIR", os.path.abspath("./downloads"))
    headless = os.environ.get("HEADLESS", "1") in ("1", "true", "TRUE", "yes", "YES")
    # CSV íŒŒì´í”„ë¼ì¸ ì˜µì…˜ (ë¡œì»¬ ì €ì¥ë§Œ; S3 ì—…ë¡œë“œëŠ” ë¹„í™œì„±)
    write_csv = os.environ.get("WRITE_CSV", "1") in ("1", "true", "TRUE", "yes", "YES")
    logging.info("ğŸ§¾ ì‹¤í–‰ íŒŒë¼ë¯¸í„°: bucket=%s, prefix=%s, headless=%s", bucket, s3_prefix, headless)

    # íŒŒí‹°ì…˜ ë°©ì‹: ê¸°ë³¸ì€ ë‹¹ì›”(ì„œìš¸ì‹œê°„)
    partition_month = os.environ.get("PARTITION_MONTH") or compute_partition_month_kst()

    # 1) 1ì¼ ìˆ˜ì§‘: BigKinds ì—‘ì…€ ë‹¤ìš´ë¡œë“œ â†’ CSV(ì„ íƒ) â†’ DF ë¡œë“œ
    logging.info("ğŸ“¥ BigKinds ë‹¤ìš´ë¡œë“œ ì‹œì‘ (1ì¼, KST)")
    downloaded = bigkinds_login_and_download(user_id, user_pw, download_dir, headless=headless)
    moved = move_to_data_folder(downloaded)
    logging.info(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬: {moved}")
    # ì—‘ì…€ ë¡œë“œ ë° ì •ê·œí™”
    df = pd.read_excel(moved)
    df.columns = [str(c).strip().replace("\n", "") for c in df.columns]
    # CSV ì €ì¥(í•„ìˆ˜ ì»¬ëŸ¼ë§Œ)
    if write_csv:
        csv_path = os.path.splitext(moved)[0] + ".csv"
        try:
            required_cols = ["ì¼ì", "ì–¸ë¡ ì‚¬", "ì œëª©", "URL", "íŠ¹ì„±ì¶”ì¶œ(ê°€ì¤‘ì¹˜ìˆœ ìƒìœ„ 50ê°œ)"]
            out = pd.DataFrame()
            for c in required_cols:
                if c in df.columns:
                    out[c] = df[c]
                else:
                    out[c] = ""
            out = out.fillna("")
            out.to_csv(csv_path, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_MINIMAL)
            logging.info(f"ğŸ“ CSV ë¡œì»¬ ì €ì¥ ì™„ë£Œ(í•„ìˆ˜ ì»¬ëŸ¼): {csv_path}")
        except Exception as e:
            logging.warning(f"CSV ì €ì¥ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
    logging.info("ğŸ§± íƒ€ì´í‹€ ì¸ë±ìŠ¤ ë³‘í•©/ì—…ë¡œë“œ ì‹œì‘")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    part_name, used_urls = build_and_upload_month_partition(df, embeddings, bucket, s3_prefix, partition_month)
    logging.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket}/{s3_prefix}{part_name}/ (pklâ†’faiss)")

    # 3) (ì˜µì…˜) ì—…ë¡œë“œ ì§í›„ ë³¸ë¬¸ í”„ë¦¬ì›Œë°: íŒŒì¼ ëª¨ë“œë¡œ ìƒˆ URLë§Œ ì²˜ë¦¬
    prewarm_enabled = os.environ.get("PREWARM_AFTER_UPLOAD", "1") in ("1", "true", "TRUE", "yes", "YES")
    if prewarm_enabled:
        logging.info(f"ğŸ”¥ ë³¸ë¬¸ í”„ë¦¬ì›Œë° ì‹œì‘ (urls={len(used_urls)})")
        _trigger_prewarm_after_upload(
            urls=used_urls,
            concurrency=int(os.environ.get("PREWARM_CONCURRENCY", "3")),
            limit=int(os.environ.get("PREWARM_LIMIT", "0")),
        )


if __name__ == "__main__":
    main()
