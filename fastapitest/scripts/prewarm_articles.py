import os
import asyncio
import logging
import random 
import time 
from typing import List, Set

from dotenv import load_dotenv

# Reuse existing app components without modifying them
from fastapitest.services.fact_checker import ensure_article_faiss, embed_model
from fastapitest.core.preload_s3_faiss import preload_faiss_from_existing_s3, CHUNK_CACHE_DIR
from langchain_community.vectorstores import FAISS


logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')


def _find_partitions(target_partition: int | None = None) -> List[str]:
    parts: List[str] = []
    if not os.path.exists(CHUNK_CACHE_DIR):
        return parts

    def part_num(name: str) -> int:
        import re
        base = os.path.basename(name)
        m = re.search(r"(\d+)", base)
        return int(m.group(1)) if m else -1

    for item in os.listdir(CHUNK_CACHE_DIR):
        p = os.path.join(CHUNK_CACHE_DIR, item)
        if os.path.isdir(p) and item.startswith("partition_"):
            current_part_num = part_num(item)
            if target_partition is None or current_part_num == target_partition:
                parts.append(p)
    parts.sort(key=part_num, reverse=True)
    return parts


def _urls_from_partitions(parts: List[str]) -> List[str]:
    urls: List[str] = []
    seen: Set[str] = set()
    for part in parts:
        try:
            db = FAISS.load_local(part, embeddings=embed_model, allow_dangerous_deserialization=True)
            for doc in db.docstore._dict.values():
                u = (doc.metadata or {}).get("url")
                if u and u not in seen:
                    seen.add(u)
                    urls.append(u)
        except Exception as e:
            logging.error(f"íŒŒí‹°ì…˜ ë¡œë“œ ì‹¤íŒ¨: {part} -> {e}")
    return urls


async def _bounded_prewarm(urls: List[str], concurrency: int, min_delay: float, max_delay: float) -> None:
    sem = asyncio.Semaphore(concurrency)

    async def one(u: str):
        async with sem:
            # Add random delay here
            delay = random.uniform(min_delay, max_delay)
            logging.info(f"â³ {delay:.2f}ì´ˆ ëŒ€ê¸° í›„ {u} ì²˜ë¦¬ ì‹œì‘...")
            await asyncio.sleep(delay)

            try:
                db = await ensure_article_faiss(u)
                if db:
                    logging.info(f"âœ… í”„ë¦¬ì›Œë° ì™„ë£Œ: {u}")
                else:
                    logging.warning(f"âš ï¸ í”„ë¦¬ì›Œë° ì‹¤íŒ¨/ë³¸ë¬¸ë¶€ì¡±: {u}")
            except Exception as e:
                logging.error(f"âŒ í”„ë¦¬ì›Œë° ì¤‘ ì˜¤ë¥˜: {u} -> {e}")

    tasks = [asyncio.create_task(one(u)) for u in urls]
    await asyncio.gather(*tasks)


async def main_async(prefix: str, source: str, url_file: str | None, limit: int, concurrency: int, partition_number: int | None, min_delay: float, max_delay: float):
    if source == "partitions":
        logging.info(f"ğŸš€ ì œëª© FAISS S3 í”„ë¦¬ë¡œë“œ ì‹œì‘ (prefix={prefix})")
        preload_faiss_from_existing_s3(prefix)
        parts = _find_partitions(target_partition=partition_number)
        logging.info(f"ğŸ”¢ ê°ì§€ëœ íŒŒí‹°ì…˜ ìˆ˜: {len(parts)}")
        urls = _urls_from_partitions(parts)
        logging.info(f"ğŸ” íŒŒí‹°ì…˜ì—ì„œ ìˆ˜ì§‘ëœ URL ìˆ˜: {len(urls)}")
    else:
        if not url_file or not os.path.exists(url_file):
            raise SystemExit("--file ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        with open(url_file, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
        logging.info(f"ğŸ“„ íŒŒì¼ì—ì„œ URL {len(urls)}ê±´ ë¡œë“œ")

    if limit > 0:
        urls = urls[:limit]
        logging.info(f"â±ï¸ ì œí•œ ì ìš©: {limit}ê±´ ëŒ€ìƒìœ¼ë¡œ í”„ë¦¬ì›Œë° ì‹¤í–‰")

    if not urls:
        logging.warning("í”„ë¦¬ì›Œë°í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    await _bounded_prewarm(urls, concurrency=concurrency, min_delay=min_delay, max_delay=max_delay)


def main():
    load_dotenv()
    import argparse

    p = argparse.ArgumentParser(description="ê¸°ì‚¬ ë³¸ë¬¸/FAISS ìºì‹œ ì‚¬ì „ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸")
    p.add_argument("--prefix", default="feature_faiss_db_openai_partition/", help="S3 ì œëª© FAISS í”„ë¦¬í”½ìŠ¤")
    p.add_argument("--source", choices=["partitions", "file"], default="partitions", help="URL ì†ŒìŠ¤ ì„ íƒ")
    p.add_argument("--file", help="--source fileì¼ ë•Œ ì‚¬ìš©í•  URL ëª©ë¡ íŒŒì¼ ê²½ë¡œ")
    p.add_argument("--limit", type=int, default=0, help="ìµœëŒ€ ì²˜ë¦¬ URL ìˆ˜(0=ë¬´ì œí•œ)")
    p.add_argument("--concurrency", type=int, default=3, help="ë™ì‹œ í”„ë¦¬ì›Œë° ê°œìˆ˜")
    p.add_argument("--partition", type=int, default=None, help="ì²˜ë¦¬í•  íŠ¹ì • íŒŒí‹°ì…˜ ë²ˆí˜¸ (ì˜ˆ: 1)")
    p.add_argument("--min-delay", type=float, default=1.0, help="ê° URL ì²˜ë¦¬ ì „ ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)")
    p.add_argument("--max-delay", type=float, default=5.0, help="ê° URL ì²˜ë¦¬ ì „ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)")
    args = p.parse_args()

    asyncio.run(main_async(
        prefix=args.prefix,
        source=args.source,
        url_file=args.file,
        limit=args.limit,
        concurrency=args.concurrency,
        partition_number=args.partition,
        min_delay=args.min_delay,
        max_delay=args.max_delay,
    ))


if __name__ == "__main__":
    main()
