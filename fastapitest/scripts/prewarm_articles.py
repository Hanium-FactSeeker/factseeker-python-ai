import os
import sys
import asyncio
import logging
import random 
import time 
from typing import List, Set
from pathlib import Path
import boto3

from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS


def _resolve_imports():
    """í™˜ê²½ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    - ìš°ì„  fastapitest íŒ¨í‚¤ì§€ ê²½ë¡œë¡œ ì‹œë„
    - ì‹¤íŒ¨ ì‹œ fastapitest ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€í•˜ê³  íŒ¨í‚¤ì§€ ë‚´ë¶€ ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
    """
    try:
        from services.fact_checker import ensure_article_faiss as _ensure, embed_model as _embed
from core.preload_s3_faiss import preload_faiss_from_existing_s3 as _preload, CHUNK_CACHE_DIR as _cache
        return _ensure, _embed, _preload, _cache
    except ModuleNotFoundError:
        pkg_dir = Path(__file__).resolve().parents[1]  # fastapitest ë””ë ‰í† ë¦¬
        if str(pkg_dir) not in sys.path:
            sys.path.insert(0, str(pkg_dir))
        from services.fact_checker import ensure_article_faiss as _ensure, embed_model as _embed
        from core.preload_s3_faiss import preload_faiss_from_existing_s3 as _preload, CHUNK_CACHE_DIR as _cache
        return _ensure, _embed, _preload, _cache


ensure_article_faiss, embed_model, preload_faiss_from_existing_s3, CHUNK_CACHE_DIR = _resolve_imports()


logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')


def _find_partitions(target_partition: int | None = None) -> List[str]:
    parts: List[str] = []
    if not os.path.exists(CHUNK_CACHE_DIR):
        return parts

    def part_num(name: str) -> int:
        import re
        base = os.path.basename(name)
        m = re.search(r"(\d+)", base)
        return int(m.group(1)) if m else -1

    for item in os.listdir(CHUNK_CACHE_DIR):
        p = os.path.join(CHUNK_CACHE_DIR, item)
        if os.path.isdir(p) and item.startswith("partition_"):
            current_part_num = part_num(item)
            if target_partition is None or current_part_num == target_partition:
                parts.append(p)
    parts.sort(key=part_num, reverse=True)
    return parts


def _is_valid_url(value: str | None) -> bool:
    return isinstance(value, str) and value.strip().lower().startswith(("http://", "https://"))


def _urls_from_partitions(parts: List[str]) -> List[str]:
    urls: List[str] = []
    seen: Set[str] = set()
    for part in parts:
        try:
            db = FAISS.load_local(part, embeddings=embed_model, allow_dangerous_deserialization=True)
            for doc in db.docstore._dict.values():
                u = (doc.metadata or {}).get("url")
                if _is_valid_url(u) and u not in seen:
                    seen.add(u)
                    urls.append(u)
        except Exception as e:
            logging.error(f"íŒŒí‹°ì…˜ ë¡œë“œ ì‹¤íŒ¨: {part} -> {e}")
    return urls


def _barrier_path() -> str:
    return os.path.join(CHUNK_CACHE_DIR, ".preload.lock")


def _acquire_preload_barrier():
    os.makedirs(CHUNK_CACHE_DIR, exist_ok=True)
    path = _barrier_path()
    try:
        with open(path, "w", encoding="utf-8") as f:
            f.write(str(time.time()))
        logging.info(f"ğŸ”’ í”„ë¦¬ë¡œë“œ ë°°ë¦¬ì–´ ìƒì„±: {path}")
    except Exception as e:
        logging.warning(f"í”„ë¦¬ë¡œë“œ ë°°ë¦¬ì–´ ìƒì„± ì‹¤íŒ¨(ë¬´ì‹œ): {e}")


def _release_preload_barrier():
    path = _barrier_path()
    try:
        if os.path.exists(path):
            os.remove(path)
            logging.info("ğŸ”“ í”„ë¦¬ë¡œë“œ ë°°ë¦¬ì–´ í•´ì œ")
    except Exception as e:
        logging.warning(f"í”„ë¦¬ë¡œë“œ ë°°ë¦¬ì–´ í•´ì œ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")


def _expected_partitions_from_s3(prefix: str) -> Set[str]:
    """S3ì—ì„œ ê¸°ëŒ€ë˜ëŠ” íŒŒí‹°ì…˜ ë””ë ‰í„°ë¦¬ ì´ë¦„ ì§‘í•©ì„ ê³„ì‚°(.faiss ê¸°ì¤€)."""
    parts: Set[str] = set()
    try:
        s3 = boto3.client("s3")
        bucket = os.environ.get("S3_BUCKET_NAME", "factseeker-faiss-db")
        paginator = s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get('Contents', []):
                key = obj.get('Key')
                if key and key.endswith('.faiss'):
                    parts.add(os.path.basename(os.path.dirname(key)))
    except Exception as e:
        logging.warning(f"S3 íŒŒí‹°ì…˜ ì¡°íšŒ ì‹¤íŒ¨(ê±´ë„ˆëœ€): {e}")
    return parts


async def _wait_until_preload_complete(prefix: str, timeout_sec: float = 900.0, poll_interval_sec: float = 3.0) -> None:
    """
    í”„ë¦¬ë¡œë“œ ì™„ë£Œë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´, S3ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  íŒŒí‹°ì…˜(partition_*)ì˜
    index.faiss/index.pklì´ ë¡œì»¬ì— ì¡´ì¬í•  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    expected = _expected_partitions_from_s3(prefix)
    if not expected:
        logging.info("S3ì—ì„œ ê¸°ëŒ€ íŒŒí‹°ì…˜ì´ ì—†ì–´ ëŒ€ê¸° ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
        return

    logging.info(f"â³ í”„ë¦¬ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì‹œì‘ (ê¸°ëŒ€ íŒŒí‹°ì…˜ {len(expected)}ê°œ)")
    start = time.time()

    def _has_all_locally() -> tuple[int, int]:
        ok = 0
        total = len(expected)
        for part in expected:
            local_dir = os.path.join(CHUNK_CACHE_DIR, part)
            if os.path.isdir(local_dir):
                faiss_ok = os.path.exists(os.path.join(local_dir, 'index.faiss'))
                pkl_ok = os.path.exists(os.path.join(local_dir, 'index.pkl'))
                if faiss_ok and pkl_ok:
                    ok += 1
        return ok, total

    while True:
        ok, total = _has_all_locally()
        if ok >= total:
            logging.info("âœ… í”„ë¦¬ë¡œë“œëœ íŒŒí‹°ì…˜ì´ ëª¨ë‘ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        if time.time() - start > timeout_sec:
            logging.warning(f"âš ï¸ í”„ë¦¬ë¡œë“œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {ok}/{total}ê°œ í™•ì¸ë¨. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            return
        logging.info(f"... ëŒ€ê¸° ì¤‘: {ok}/{total}ê°œ í™•ì¸ë¨ (í´ë§ {poll_interval_sec:.0f}s)")
        await asyncio.sleep(poll_interval_sec)


async def _bounded_prewarm(urls: List[str], concurrency: int, min_delay: float, max_delay: float) -> None:
    sem = asyncio.Semaphore(concurrency)

    async def one(u: str):
        async with sem:
            # Add random delay here
            delay = random.uniform(min_delay, max_delay)
            logging.info(f"â³ {delay:.2f}ì´ˆ ëŒ€ê¸° í›„ {u} ì²˜ë¦¬ ì‹œì‘...")
            await asyncio.sleep(delay)

            try:
                db = await ensure_article_faiss(u)
                if db:
                    logging.info(f"âœ… í”„ë¦¬ì›Œë° ì™„ë£Œ: {u}")
                else:
                    logging.warning(f"âš ï¸ í”„ë¦¬ì›Œë° ì‹¤íŒ¨/ë³¸ë¬¸ë¶€ì¡±: {u}")
            except Exception as e:
                logging.error(f"âŒ í”„ë¦¬ì›Œë° ì¤‘ ì˜¤ë¥˜: {u} -> {e}")

    tasks = [asyncio.create_task(one(u)) for u in urls]
    await asyncio.gather(*tasks)


async def main_async(prefix: str, source: str, url_file: str | None, limit: int, concurrency: int, partition_number: int | None, min_delay: float, max_delay: float, preload_wait_timeout: float, preload_poll_interval: float):
    if source == "partitions":
        logging.info(f"ğŸš€ ì œëª© FAISS S3 í”„ë¦¬ë¡œë“œ ì‹œì‘ (prefix={prefix})")
        _acquire_preload_barrier()
        try:
            preload_faiss_from_existing_s3(prefix)
            await _wait_until_preload_complete(prefix, timeout_sec=preload_wait_timeout, poll_interval_sec=preload_poll_interval)
        finally:
            _release_preload_barrier()
        parts = _find_partitions(target_partition=partition_number)
        logging.info(f"ğŸ”¢ ê°ì§€ëœ íŒŒí‹°ì…˜ ìˆ˜: {len(parts)}")
        urls = _urls_from_partitions(parts)
        logging.info(f"ğŸ” íŒŒí‹°ì…˜ì—ì„œ ìˆ˜ì§‘ëœ URL ìˆ˜: {len(urls)}")
    else:
        if not url_file or not os.path.exists(url_file):
            raise SystemExit("--file ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        with open(url_file, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
        logging.info(f"ğŸ“„ íŒŒì¼ì—ì„œ URL {len(urls)}ê±´ ë¡œë“œ")

    if limit > 0:
        urls = urls[:limit]
        logging.info(f"â±ï¸ ì œí•œ ì ìš©: {limit}ê±´ ëŒ€ìƒìœ¼ë¡œ í”„ë¦¬ì›Œë° ì‹¤í–‰")

    if not urls:
        logging.warning("í”„ë¦¬ì›Œë°í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    await _bounded_prewarm(urls, concurrency=concurrency, min_delay=min_delay, max_delay=max_delay)


def main():
    load_dotenv()
    import argparse

    p = argparse.ArgumentParser(description="ê¸°ì‚¬ ë³¸ë¬¸/FAISS ìºì‹œ ì‚¬ì „ êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸")
    p.add_argument("--prefix", default="feature_faiss_db_openai_partition/", help="S3 ì œëª© FAISS í”„ë¦¬í”½ìŠ¤")
    p.add_argument("--source", choices=["partitions", "file"], default="partitions", help="URL ì†ŒìŠ¤ ì„ íƒ")
    p.add_argument("--file", help="--source fileì¼ ë•Œ ì‚¬ìš©í•  URL ëª©ë¡ íŒŒì¼ ê²½ë¡œ")
    p.add_argument("--limit", type=int, default=0, help="ìµœëŒ€ ì²˜ë¦¬ URL ìˆ˜(0=ë¬´ì œí•œ)")
    p.add_argument("--concurrency", type=int, default=3, help="ë™ì‹œ í”„ë¦¬ì›Œë° ê°œìˆ˜")
    p.add_argument("--partition", type=int, default=None, help="ì²˜ë¦¬í•  íŠ¹ì • íŒŒí‹°ì…˜ ë²ˆí˜¸ (ì˜ˆ: 1)")
    p.add_argument("--min-delay", type=float, default=1.0, help="ê° URL ì²˜ë¦¬ ì „ ìµœì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)")
    p.add_argument("--max-delay", type=float, default=5.0, help="ê° URL ì²˜ë¦¬ ì „ ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)")
    p.add_argument("--preload-wait-timeout", type=float, default=900.0, help="í”„ë¦¬ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ìµœëŒ€ ì‹œê°„(ì´ˆ)")
    p.add_argument("--preload-poll-interval", type=float, default=3.0, help="í”„ë¦¬ë¡œë“œ ìƒíƒœ í´ë§ ì£¼ê¸°(ì´ˆ)")
    args = p.parse_args()

    asyncio.run(main_async(
        prefix=args.prefix,
        source=args.source,
        url_file=args.file,
        limit=args.limit,
        concurrency=args.concurrency,
        partition_number=args.partition,
        min_delay=args.min_delay,
        max_delay=args.max_delay,
        preload_wait_timeout=args.preload_wait_timeout,
        preload_poll_interval=args.preload_poll_interval,
    ))


if __name__ == "__main__":
    main()
