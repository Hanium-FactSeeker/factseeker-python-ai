import re
import json
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Any

from article_checker.chains import build_article_claim_extractor
from core.lambdas import (
    get_article_text,
    calculate_fact_check_confidence,
    calculate_source_diversity_score,
)
from core.llm_chains import (
    build_claim_summarizer,
    build_reduce_similar_claims_chain,
    build_factcheck_chain,
    build_keyword_extractor_chain,
    build_three_line_summarizer_chain,
)

# Reuse evidence retrieval pipeline from the existing YouTube flow
from services.fact_checker import (
    search_and_retrieve_docs_once,
    MAX_CONCURRENT_CLAIMS,
    MAX_CONCURRENT_FACTCHECKS,
    MAX_EVIDENCES_PER_CLAIM,
)


async def _extract_claims_from_article(article_text: str) -> List[str]:
    extractor = build_article_claim_extractor()
    result = await extractor.ainvoke({"article_text": article_text})
    lines = [line.strip() for line in (result.content or "").split("\n") if line.strip()]
    return lines


async def _reduce_claims(claims: List[str]) -> List[str]:
    reducer = build_reduce_similar_claims_chain()
    claims_json = json.dumps(claims, ensure_ascii=False, indent=2)
    reduced_result = await reducer.ainvoke({"claims_json": claims_json})

    claims_to_check: List[str] = []
    try:
        json_match = re.search(r"```json\s*(\[.*?\])\s*```", reduced_result.content or "", re.DOTALL)
        if json_match:
            json_content = json_match.group(1)
            claims_to_check = json.loads(json_content)
        else:
            claims_to_check = [line.strip() for line in (reduced_result.content or "").strip().split('\n') if line.strip()]
    except json.JSONDecodeError:
        claims_to_check = [
            line.strip() for line in (reduced_result.content or "").strip().split('\n')
            if line.strip() and not line.strip().startswith(('```json', '```', '[', ']'))
        ]
    return claims_to_check


async def run_article_fact_check(article_url: str, faiss_partition_dirs: List[str]) -> Dict[str, Any]:
    """Article-first fact-check pipeline mirroring the YouTube flow.

    Returns a result dict similar to services.fact_checker.run_fact_check but
    with article-specific top-level fields and without YouTube identifiers.
    """
    logging.info(f"ê¸°ì‚¬ ë¶„ì„ ì‹œì‘: {article_url}")

    # 1) Fetch article body
    article_text = await get_article_text(article_url)
    if not article_text:
        return {"error": "Failed to load article"}

    # 5) Extract keywords and summary
    keyword_extractor = build_keyword_extractor_chain()
    summarizer = build_three_line_summarizer_chain()

    keywords_task = keyword_extractor.ainvoke({"text": article_text})
    summary_task = summarizer.ainvoke({"text": article_text})

    keywords_result, summary_result = await asyncio.gather(keywords_task, summary_task)

    extracted_keywords = keywords_result.content.strip() if keywords_result.content else ""
    three_line_summary = summary_result.content.strip() if summary_result.content else ""

    # 2) Extract claims from article body (article-specific chain)
    try:
        claims_raw = await _extract_claims_from_article(article_text)
        if not claims_raw:
            return {
                "article_url": article_url,
                "article_total_confidence_score": 0,
                "claims": [],
            }

        # 3) Reduce/normalize claims (reuse existing reducer)
        claims_to_check = await _reduce_claims(claims_raw)
        # Keep parity with service defaults (max 10)
        claims_to_check = claims_to_check[:10]
        if not claims_to_check:
            return {
                "article_url": article_url,
                "article_total_confidence_score": 0,
                "claims": [],
            }
        logging.info(f"âœ‚ï¸ ê¸°ì‚¬ ê¸°ë°˜ ìµœì¢… íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ì£¼ì¥ {len(claims_to_check)}ê°œ: {claims_to_check}")
    except Exception as e:
        logging.exception(f"ì£¼ì¥ ì¶”ì¶œ/ì •ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": f"Failed to extract claims: {e}"}

    # 4) For each claim: CSE + FAISS titles â†’ fetch evidence and fact-check
    async def process_claim_step(idx: int, claim: str) -> Dict[str, Any]:
        logging.info(f"--- ê¸°ì‚¬ íŒ©íŠ¸ì²´í¬ ì‹œì‘: ({idx + 1}/{len(claims_to_check)}) '{claim}'")
        url_set = set()
        validated_evidence: List[Dict[str, Any]] = []
        fact_checker = build_factcheck_chain()

        async def factcheck_doc(doc):
            try:
                check_result = await fact_checker.ainvoke({"claim": claim, "context": doc.page_content})
                result_content = check_result.content or ""
                relevance = re.search(r"ê´€ë ¨ì„±: (.+)", result_content)
                fact_check_result_match = re.search(r"ì‚¬ì‹¤ ì„¤ëª… ì—¬ë¶€: (.+)", result_content)
                justification = re.search(r"ê°„ë‹¨í•œ ì„¤ëª…: (.+)", result_content)
                snippet = re.search(r"í•µì‹¬ ê·¼ê±° ë¬¸ì¥: (.+)", result_content, re.DOTALL)
                url = doc.metadata.get("url")

                if (
                    relevance and fact_check_result_match and justification
                    and "ì˜ˆ" in relevance.group(1)
                    and url and url not in url_set
                ):
                    url_set.add(url)
                    return {
                        "url": url,
                        "relevance": "yes",
                        "fact_check_result": fact_check_result_match.group(1).strip(),
                        "justification": justification.group(1).strip(),
                        "snippet": snippet.group(1).strip() if snippet else "",
                    }
            except Exception as e:
                logging.error(f"    - LLM íŒ©íŠ¸ì²´í¬ ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

        factcheck_semaphore = asyncio.Semaphore(MAX_CONCURRENT_FACTCHECKS)

        async def limited_factcheck_doc(doc):
            async with factcheck_semaphore:
                return await factcheck_doc(doc)

        new_docs = await search_and_retrieve_docs_once(claim, faiss_partition_dirs, set())
        if not new_docs:
            logging.info(f"ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨: '{claim}'")
            return {
                "claim": claim,
                "result": "insufficient_evidence",
                "confidence_score": 0,
                "evidence": [],
            }

        for i in range(0, len(new_docs), MAX_CONCURRENT_FACTCHECKS):
            if len(validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                break
            batch = new_docs[i : i + MAX_CONCURRENT_FACTCHECKS]
            factcheck_results = await asyncio.gather(*[limited_factcheck_doc(doc) for doc in batch])
            for res in factcheck_results:
                if res and isinstance(res, dict):  # Noneì´ ì•„ë‹ˆê³  dict íƒ€ì…ì¸ì§€ í™•ì¸
                    validated_evidence.append(res)
                    logging.info(f"âœ… [ë„¤ì´ë²„] ì£¼ì¥ '{claim}' â†’ ì¦ê±° URL: {res.get('url', 'N/A')}")
                    if len(validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                        break

        diversity_score = calculate_source_diversity_score(validated_evidence)
        confidence_score = calculate_fact_check_confidence(
            {"source_diversity": diversity_score, "evidence_count": min(len(validated_evidence), 5)}
        )

        logging.info(
            f"--- ê¸°ì‚¬ íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: '{claim}' -> ì‹ ë¢°ë„: {confidence_score}% (ì¦ê±° {len(validated_evidence)}ê°œ)"
        )

        # ì‹ ë¢°ë„ê°€ 20% ì´í•˜ì¼ ê²½ìš° Google CSEë¡œ ì¬ì‹œë„
        naver_confidence = confidence_score
        naver_evidence = validated_evidence.copy()
        
        if confidence_score <= 20:
            logging.info(f"ì‹ ë¢°ë„ {confidence_score}%ë¡œ ì¸í•œ Google CSE ì¬ì‹œë„: '{claim}'")
            
            # Google CSEë¡œ ì¬ê²€ìƒ‰
            google_docs = await search_and_retrieve_docs_once(claim, faiss_partition_dirs, set(), use_google_cse=True)
            if google_docs:
                logging.info(f"Google CSEë¡œ {len(google_docs)}ê°œ ë¬¸ì„œ ë°œê²¬, ì¬íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰")
                
                # Google CSE ê²°ê³¼ë¡œ ì¬íŒ©íŠ¸ì²´í¬
                google_validated_evidence = []
                for i in range(0, len(google_docs), MAX_CONCURRENT_FACTCHECKS):
                    if len(google_validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                        break
                    batch = google_docs[i : i + MAX_CONCURRENT_FACTCHECKS]
                    factcheck_results = await asyncio.gather(*[limited_factcheck_doc(doc) for doc in batch])
                    for res in factcheck_results:
                        if res and isinstance(res, dict):  # Noneì´ ì•„ë‹ˆê³  dict íƒ€ì…ì¸ì§€ í™•ì¸
                            google_validated_evidence.append(res)
                            logging.info(f"âœ… [Google CSE] ì£¼ì¥ '{claim}' â†’ ì¦ê±° URL: {res.get('url', 'N/A')}")
                            if len(google_validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                                break
                
                # Google CSE ê²°ê³¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                if google_validated_evidence:
                    google_diversity_score = calculate_source_diversity_score(google_validated_evidence)
                    google_confidence = calculate_fact_check_confidence(
                        {"source_diversity": google_diversity_score, "evidence_count": min(len(google_validated_evidence), 5)}
                    )
                    logging.info(f"Google CSE ì¬íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: ì‹ ë¢°ë„ {google_confidence}% (ì¦ê±° {len(google_validated_evidence)}ê°œ)")
                    
                    # ë‘˜ ì¤‘ ë” ë†’ì€ ì‹ ë¢°ë„ ì„ íƒ
                    if google_confidence > naver_confidence:
                        confidence_score = google_confidence
                        validated_evidence = google_validated_evidence
                        logging.info(f"Google CSE ê²°ê³¼ ì„ íƒ: {google_confidence}% > ë„¤ì´ë²„ {naver_confidence}%")
                    else:
                        logging.info(f"ë„¤ì´ë²„ ê²°ê³¼ ìœ ì§€: {naver_confidence}% >= Google CSE {google_confidence}%")
                else:
                    logging.info("Google CSEë¡œë„ ê²€ì¦ëœ ì¦ê±°ë¥¼ ì°¾ì§€ ëª»í•¨")
            else:
                logging.info("Google CSEë¡œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
        else:
            logging.info(f"ì‹ ë¢°ë„ {confidence_score}%ë¡œ ì¶©ë¶„í•˜ë¯€ë¡œ Google CSE ì¬ì‹œë„ ìƒëµ")

        # ìµœì¢… ì‹ ë¢°ë„ê°€ 20% ì´í•˜ì´ë©´ íŒŒí‹°ì…˜ 9ë¡œ ì¬ì‹œë„
        final_confidence = confidence_score
        final_evidence = validated_evidence
        
        if confidence_score <= 20 and len(validated_evidence) > 0:
            logging.info(f"ìµœì¢… ì‹ ë¢°ë„ {confidence_score}%ë¡œ ë‚®ìŒ â†’ íŒŒí‹°ì…˜ 9ë¡œ ì¬ì‹œë„: '{claim}'")
            
            # íŒŒí‹°ì…˜ 9ë§Œ ì‚¬ìš©í•˜ì—¬ ì¬ê²€ìƒ‰
            partition_9_dirs = [dir for dir in faiss_partition_dirs if "9" in dir]
            logging.info(f"ğŸ” íŒŒí‹°ì…˜ 9 ê²€ìƒ‰: ì „ì²´ íŒŒí‹°ì…˜ {len(faiss_partition_dirs)}ê°œ ì¤‘ íŒŒí‹°ì…˜ 9 í¬í•¨ {len(partition_9_dirs)}ê°œ ë°œê²¬")
            if partition_9_dirs:
                logging.info(f"íŒŒí‹°ì…˜ 9 ë””ë ‰í† ë¦¬ {len(partition_9_dirs)}ê°œ ë°œê²¬, ì¬ê²€ìƒ‰ ì‹œì‘")
                for dir in partition_9_dirs:
                    logging.info(f"  ğŸ“ íŒŒí‹°ì…˜ 9 ê²½ë¡œ: {dir}")
                
                # íŒŒí‹°ì…˜ 9ë¡œ ì¬ê²€ìƒ‰
                partition_9_docs = await search_and_retrieve_docs_once(claim, partition_9_dirs, set(), use_google_cse=False)
                if partition_9_docs:
                    logging.info(f"íŒŒí‹°ì…˜ 9ì—ì„œ {len(partition_9_docs)}ê°œ ë¬¸ì„œ ë°œê²¬, ì¬íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰")
                    
                    # íŒŒí‹°ì…˜ 9 ê²°ê³¼ë¡œ ì¬íŒ©íŠ¸ì²´í¬
                    partition_9_validated_evidence = []
                    for i in range(0, len(partition_9_docs), MAX_CONCURRENT_FACTCHECKS):
                        if len(partition_9_validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                            break
                        batch = partition_9_docs[i : i + MAX_CONCURRENT_FACTCHECKS]
                        factcheck_results = await asyncio.gather(*[limited_factcheck_doc(doc) for doc in batch])
                        for res in factcheck_results:
                            if res and isinstance(res, dict):
                                partition_9_validated_evidence.append(res)
                                logging.info(f"âœ… [íŒŒí‹°ì…˜9] ì£¼ì¥ '{claim}' â†’ ì¦ê±° URL: {res.get('url', 'N/A')}")
                                if len(partition_9_validated_evidence) >= MAX_EVIDENCES_PER_CLAIM:
                                    break
                    
                    # íŒŒí‹°ì…˜ 9 ê²°ê³¼ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
                    if partition_9_validated_evidence:
                        partition_9_diversity_score = calculate_source_diversity_score(partition_9_validated_evidence)
                        partition_9_confidence = calculate_fact_check_confidence(
                            {"source_diversity": partition_9_diversity_score, "evidence_count": min(len(partition_9_validated_evidence), 5)}
                        )
                        logging.info(f"íŒŒí‹°ì…˜ 9 ì¬íŒ©íŠ¸ì²´í¬ ì™„ë£Œ: ì‹ ë¢°ë„ {partition_9_confidence}% (ì¦ê±° {len(partition_9_validated_evidence)}ê°œ)")
                        
                        # íŒŒí‹°ì…˜ 9 ê²°ê³¼ê°€ ë” ë†’ìœ¼ë©´ ì„ íƒ
                        if partition_9_confidence > final_confidence:
                            final_confidence = partition_9_confidence
                            final_evidence = partition_9_validated_evidence
                            logging.info(f"íŒŒí‹°ì…˜ 9 ê²°ê³¼ ì„ íƒ: {partition_9_confidence}% > ê¸°ì¡´ {confidence_score}%")
                        else:
                            logging.info(f"ê¸°ì¡´ ê²°ê³¼ ìœ ì§€: {confidence_score}% >= íŒŒí‹°ì…˜ 9 {partition_9_confidence}%")
                    else:
                        logging.info("íŒŒí‹°ì…˜ 9ìœ¼ë¡œë„ ê²€ì¦ëœ ì¦ê±°ë¥¼ ì°¾ì§€ ëª»í•¨")
                else:
                    logging.info("íŒŒí‹°ì…˜ 9ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨")
            else:
                logging.info("íŒŒí‹°ì…˜ 9 ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        return {
            "claim": claim,
            "result": "likely_true" if final_evidence else "insufficient_evidence",
            "confidence_score": final_confidence,
            "evidence": final_evidence[:3],
        }

    claim_semaphore = asyncio.Semaphore(MAX_CONCURRENT_CLAIMS)

    async def limited_process_claim(idx: int, claim: str):
        async with claim_semaphore:
            return await process_claim_step(idx, claim)

    gathered = await asyncio.gather(*[limited_process_claim(i, c) for i, c in enumerate(claims_to_check)], return_exceptions=True)

    outputs: List[Dict[str, Any]] = []
    for i, res in enumerate(gathered):
        if isinstance(res, Exception):
            claim_text = claims_to_check[i] if i < len(claims_to_check) else ""
            err_type = type(res).__name__
            err_msg = str(res) or repr(res) or err_type
            logging.error(f"ğŸ›‘ ì£¼ì¥ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: '{claim_text}' -> {err_type}: {err_msg}")
            outputs.append(
                {
                    "claim": claim_text,
                    "result": "error",
                    "confidence_score": 0,
                    "evidence": [],
                    "error": f"{err_type}: {err_msg}",
                    "error_type": err_type,
                    "error_stage": "process_claim_step",
                }
            )
        else:
            outputs.append(res)

    if outputs:
        avg_score = round(sum(o.get("confidence_score", 0) for o in outputs) / len(outputs))
        evidence_ratio = sum(1 for o in outputs if o.get("result") == "likely_true") / len(outputs)
        summary = (
            f"ì¦ê±° í™•ë³´ëœ ì£¼ì¥ ë¹„ìœ¨: {evidence_ratio*100:.1f}%"
            if len(outputs) >= 3
            else f"ì‹ ë¢°ë„ í‰ê°€ ë¶ˆê°€ (íŒ©íŠ¸ì²´í¬ ì£¼ì¥ ìˆ˜ ë¶€ì¡±: {len(outputs)}ê°œ)"
        )
    else:
        avg_score = 0
        summary = "ê²°ê³¼ ì—†ìŒ"

    return {
        "article_url": article_url,
        "article_total_confidence_score": avg_score,
        "claims": outputs,
        "summary": summary,
        "created_at": datetime.now().isoformat(),
        "keywords": extracted_keywords,
        "three_line_summary": three_line_summary,
    }

